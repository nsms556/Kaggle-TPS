{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Playground Series - Nov 2022\n",
    "  - Practice your ML skills on this approachable dataset!\n",
    "  - https://www.kaggle.com/competitions/tabular-playground-series-nov-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = 'datasets/'\n",
    "subs_path = ds_path + 'submission_files/'\n",
    "\n",
    "submission_df = pd.read_csv(ds_path + 'sample_submission.csv', index_col='id')\n",
    "labels_df = pd.read_csv(ds_path + 'train_labels.csv', index_col='id')\n",
    "\n",
    "sub_ids = submission_df.index\n",
    "gt_ids = labels_df.index\n",
    "\n",
    "subs = sorted(os.listdir(ds_path + 'submission_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subs(csv_file_no) :\n",
    "    return pd.read_csv(subs_path + subs[csv_file_no], index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5000\n",
    "bs = 64\n",
    "rs = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module) :\n",
    "    def __init__(self, input_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size+1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.sigmoid(self.linear(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_origin_df(input_size) :\n",
    "    df = labels_df.copy()\n",
    "\n",
    "    for i in tqdm(range(input_size)) :\n",
    "        df = df.join(read_subs(i), on='id', how='outer', rsuffix='_{}'.format(i))\n",
    "\n",
    "    return df.set_index('id', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:35:42<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "origin_df = make_origin_df(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = origin_df.loc[:19999]\n",
    "\n",
    "train_Y = train_df['label']\n",
    "train_X = train_df.drop('label', axis=1)\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000, 5001), (18000,))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(df) :\n",
    "    return torch.from_numpy(df.to_numpy()).float()\n",
    "\n",
    "def convert_train_dataset(train_X, train_Y) :\n",
    "    return TensorDataset(to_tensor(train_X), to_tensor(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_train_dataset(train_X, train_Y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, drop_last=True)\n",
    "\n",
    "val_dataset = convert_train_dataset(val_X, val_Y)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataloader) :\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(dataloader) :\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total += outputs.size(0)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, criterion, dataloader) :\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, (inputs, targets) in enumerate(dataloader) :\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "    \n",
    "            total += outputs.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, criterion, optimizer, epochs, train_loader, val_loader, log=True) :\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    save_loss = float('inf')\n",
    "\n",
    "    for i in range(epochs) :\n",
    "        loss = train(model, criterion, optimizer, train_loader)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if log :\n",
    "            print('Epoch : {}'.format(i))\n",
    "            print('Train Loss : {}'.format(loss))\n",
    "\n",
    "        loss = validate(model, criterion, val_loader)\n",
    "        val_loss.append(loss)\n",
    "\n",
    "        if log :\n",
    "            print('Epoch : {}'.format(i))\n",
    "            print('Val Loss : {}'.format(loss))\n",
    "\n",
    "        if loss < save_loss :\n",
    "            torch.save(model, 'Linear_{}.pt'.format(input_size))\n",
    "            save_loss = loss\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def loss_plot(train_loss, val_loss) :\n",
    "    pd.DataFrame({'Train' : train_loss, 'Val' : val_loss}).plot(figsize=(8,6))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Linear(input_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "Train Loss : 0.03030465092026095\n",
      "Epoch : 0\n",
      "Val Loss : 0.009290044784545898\n",
      "Epoch : 1\n",
      "Train Loss : 0.010701380133151583\n",
      "Epoch : 1\n",
      "Val Loss : 0.00889356404542923\n",
      "Epoch : 2\n",
      "Train Loss : 0.010296233349688537\n",
      "Epoch : 2\n",
      "Val Loss : 0.009498544186353684\n",
      "Epoch : 3\n",
      "Train Loss : 0.010267539433737243\n",
      "Epoch : 3\n",
      "Val Loss : 0.009642398461699486\n",
      "Epoch : 4\n",
      "Train Loss : 0.010262693044832275\n",
      "Epoch : 4\n",
      "Val Loss : 0.009517155915498733\n",
      "Epoch : 5\n",
      "Train Loss : 0.010268981295084295\n",
      "Epoch : 5\n",
      "Val Loss : 0.009380824506282807\n",
      "Epoch : 6\n",
      "Train Loss : 0.010279879592080663\n",
      "Epoch : 6\n",
      "Val Loss : 0.009267145097255706\n",
      "Epoch : 7\n",
      "Train Loss : 0.010292931902305194\n",
      "Epoch : 7\n",
      "Val Loss : 0.009169443249702454\n",
      "Epoch : 8\n",
      "Train Loss : 0.010307689253704828\n",
      "Epoch : 8\n",
      "Val Loss : 0.009080528795719146\n",
      "Epoch : 9\n",
      "Train Loss : 0.010324561083523616\n",
      "Epoch : 9\n",
      "Val Loss : 0.0089963970631361\n",
      "Epoch : 10\n",
      "Train Loss : 0.010345149321276097\n",
      "Epoch : 10\n",
      "Val Loss : 0.008915119662880897\n",
      "Epoch : 11\n",
      "Train Loss : 0.01037376366051915\n",
      "Epoch : 11\n",
      "Val Loss : 0.00883319017291069\n",
      "Epoch : 12\n",
      "Train Loss : 0.010418483016356242\n",
      "Epoch : 12\n",
      "Val Loss : 0.008741609066724778\n",
      "Epoch : 13\n",
      "Train Loss : 0.010492351701569303\n",
      "Epoch : 13\n",
      "Val Loss : 0.008633878007531166\n",
      "Epoch : 14\n",
      "Train Loss : 0.010533184815675445\n",
      "Epoch : 14\n",
      "Val Loss : 0.008585955202579498\n",
      "Epoch : 15\n",
      "Train Loss : 0.010522420597643826\n",
      "Epoch : 15\n",
      "Val Loss : 0.008592803731560707\n",
      "Epoch : 16\n",
      "Train Loss : 0.010495146459757328\n",
      "Epoch : 16\n",
      "Val Loss : 0.00860613749921322\n",
      "Epoch : 17\n",
      "Train Loss : 0.01047140825705543\n",
      "Epoch : 17\n",
      "Val Loss : 0.008613307788968086\n",
      "Epoch : 18\n",
      "Train Loss : 0.010454096838009951\n",
      "Epoch : 18\n",
      "Val Loss : 0.008615373030304909\n",
      "Epoch : 19\n",
      "Train Loss : 0.010441468385614959\n",
      "Epoch : 19\n",
      "Val Loss : 0.008614540427923202\n",
      "Epoch : 20\n",
      "Train Loss : 0.010431768760429795\n",
      "Epoch : 20\n",
      "Val Loss : 0.008612246260046958\n",
      "Epoch : 21\n",
      "Train Loss : 0.010423845492197313\n",
      "Epoch : 21\n",
      "Val Loss : 0.008609271481633186\n",
      "Epoch : 22\n",
      "Train Loss : 0.010417016465018017\n",
      "Epoch : 22\n",
      "Val Loss : 0.008606025964021683\n",
      "Epoch : 23\n",
      "Train Loss : 0.010410879586170365\n",
      "Epoch : 23\n",
      "Val Loss : 0.008602733418345452\n",
      "Epoch : 24\n",
      "Train Loss : 0.010405207419058713\n",
      "Epoch : 24\n",
      "Val Loss : 0.008599509015679359\n",
      "Epoch : 25\n",
      "Train Loss : 0.010399854294558013\n",
      "Epoch : 25\n",
      "Val Loss : 0.0085963996052742\n",
      "Epoch : 26\n",
      "Train Loss : 0.010394743893931874\n",
      "Epoch : 26\n",
      "Val Loss : 0.008593438729643821\n",
      "Epoch : 27\n",
      "Train Loss : 0.010389819762792133\n",
      "Epoch : 27\n",
      "Val Loss : 0.008590633973479271\n",
      "Epoch : 28\n",
      "Train Loss : 0.01038505110783615\n",
      "Epoch : 28\n",
      "Val Loss : 0.00858798436820507\n",
      "Epoch : 29\n",
      "Train Loss : 0.010380415163032737\n",
      "Epoch : 29\n",
      "Val Loss : 0.008585490062832832\n",
      "Epoch : 30\n",
      "Train Loss : 0.01037590059673818\n",
      "Epoch : 30\n",
      "Val Loss : 0.00858313949406147\n",
      "Epoch : 31\n",
      "Train Loss : 0.010371493173968644\n",
      "Epoch : 31\n",
      "Val Loss : 0.008580927044153213\n",
      "Epoch : 32\n",
      "Train Loss : 0.010367191212709157\n",
      "Epoch : 32\n",
      "Val Loss : 0.008578850015997886\n",
      "Epoch : 33\n",
      "Train Loss : 0.010362980474661571\n",
      "Epoch : 33\n",
      "Val Loss : 0.008576891973614692\n",
      "Epoch : 34\n",
      "Train Loss : 0.010358858840321306\n",
      "Epoch : 34\n",
      "Val Loss : 0.008575047180056573\n",
      "Epoch : 35\n",
      "Train Loss : 0.01035482487790419\n",
      "Epoch : 35\n",
      "Val Loss : 0.0085733123421669\n",
      "Epoch : 36\n",
      "Train Loss : 0.010350872600099795\n",
      "Epoch : 36\n",
      "Val Loss : 0.008571670278906823\n",
      "Epoch : 37\n",
      "Train Loss : 0.010347000354721983\n",
      "Epoch : 37\n",
      "Val Loss : 0.00857012914121151\n",
      "Epoch : 38\n",
      "Train Loss : 0.010343201160165762\n",
      "Epoch : 38\n",
      "Val Loss : 0.008568668097257615\n",
      "Epoch : 39\n",
      "Train Loss : 0.010339477046449202\n",
      "Epoch : 39\n",
      "Val Loss : 0.008567292392253876\n",
      "Epoch : 40\n",
      "Train Loss : 0.010335822168777422\n",
      "Epoch : 40\n",
      "Val Loss : 0.008565990939736366\n",
      "Epoch : 41\n",
      "Train Loss : 0.010332233569124097\n",
      "Epoch : 41\n",
      "Val Loss : 0.008564760193228722\n",
      "Epoch : 42\n",
      "Train Loss : 0.010328709277128834\n",
      "Epoch : 42\n",
      "Val Loss : 0.008563599482178687\n",
      "Epoch : 43\n",
      "Train Loss : 0.010325246333108148\n",
      "Epoch : 43\n",
      "Val Loss : 0.008562500268220901\n",
      "Epoch : 44\n",
      "Train Loss : 0.01032184165640605\n",
      "Epoch : 44\n",
      "Val Loss : 0.008561455979943275\n",
      "Epoch : 45\n",
      "Train Loss : 0.010318498486765658\n",
      "Epoch : 45\n",
      "Val Loss : 0.00856047123670578\n",
      "Epoch : 46\n",
      "Train Loss : 0.010315207630276574\n",
      "Epoch : 46\n",
      "Val Loss : 0.008559535369277\n",
      "Epoch : 47\n",
      "Train Loss : 0.01031197010443357\n",
      "Epoch : 47\n",
      "Val Loss : 0.008558650374412537\n",
      "Epoch : 48\n",
      "Train Loss : 0.010308783874247104\n",
      "Epoch : 48\n",
      "Val Loss : 0.00855781190097332\n",
      "Epoch : 49\n",
      "Train Loss : 0.010305650379787137\n",
      "Epoch : 49\n",
      "Val Loss : 0.008557018935680389\n",
      "Epoch : 50\n",
      "Train Loss : 0.010302562084300875\n",
      "Epoch : 50\n",
      "Val Loss : 0.008556268006563186\n",
      "Epoch : 51\n",
      "Train Loss : 0.01029952174198426\n",
      "Epoch : 51\n",
      "Val Loss : 0.008555561468005181\n",
      "Epoch : 52\n",
      "Train Loss : 0.010296526013664715\n",
      "Epoch : 52\n",
      "Val Loss : 0.008554888308048248\n",
      "Epoch : 53\n",
      "Train Loss : 0.010293575442889938\n",
      "Epoch : 53\n",
      "Val Loss : 0.008554253965616225\n",
      "Epoch : 54\n",
      "Train Loss : 0.010290664880871242\n",
      "Epoch : 54\n",
      "Val Loss : 0.008553658157587051\n",
      "Epoch : 55\n",
      "Train Loss : 0.010287795991394868\n",
      "Epoch : 55\n",
      "Val Loss : 0.00855309309065342\n",
      "Epoch : 56\n",
      "Train Loss : 0.010284968517601384\n",
      "Epoch : 56\n",
      "Val Loss : 0.00855256313085556\n",
      "Epoch : 57\n",
      "Train Loss : 0.010282177511218179\n",
      "Epoch : 57\n",
      "Val Loss : 0.008552062615752221\n",
      "Epoch : 58\n",
      "Train Loss : 0.010279423832310052\n",
      "Epoch : 58\n",
      "Val Loss : 0.008551598504185676\n",
      "Epoch : 59\n",
      "Train Loss : 0.010276706518068432\n",
      "Epoch : 59\n",
      "Val Loss : 0.008551160126924515\n",
      "Epoch : 60\n",
      "Train Loss : 0.010274026513073155\n",
      "Epoch : 60\n",
      "Val Loss : 0.008550751477479935\n",
      "Epoch : 61\n",
      "Train Loss : 0.01027137575525247\n",
      "Epoch : 61\n",
      "Val Loss : 0.008550369516015052\n",
      "Epoch : 62\n",
      "Train Loss : 0.010268765904367289\n",
      "Epoch : 62\n",
      "Val Loss : 0.00855001862347126\n",
      "Epoch : 63\n",
      "Train Loss : 0.010266179704435462\n",
      "Epoch : 63\n",
      "Val Loss : 0.008549690514802933\n",
      "Epoch : 64\n",
      "Train Loss : 0.010263630019971484\n",
      "Epoch : 64\n",
      "Val Loss : 0.008549391701817512\n",
      "Epoch : 65\n",
      "Train Loss : 0.010261109920742249\n",
      "Epoch : 65\n",
      "Val Loss : 0.008549116849899292\n",
      "Epoch : 66\n",
      "Train Loss : 0.010258618642798098\n",
      "Epoch : 66\n",
      "Val Loss : 0.008548865258693696\n",
      "Epoch : 67\n",
      "Train Loss : 0.010256157201976644\n",
      "Epoch : 67\n",
      "Val Loss : 0.008548638373613358\n",
      "Epoch : 68\n",
      "Train Loss : 0.01025372519558859\n",
      "Epoch : 68\n",
      "Val Loss : 0.0085484379529953\n",
      "Epoch : 69\n",
      "Train Loss : 0.010251320429557264\n",
      "Epoch : 69\n",
      "Val Loss : 0.008548261627554893\n",
      "Epoch : 70\n",
      "Train Loss : 0.010248942512793473\n",
      "Epoch : 70\n",
      "Val Loss : 0.008548109501600265\n",
      "Epoch : 71\n",
      "Train Loss : 0.010246584001345248\n",
      "Epoch : 71\n",
      "Val Loss : 0.00854797601699829\n",
      "Epoch : 72\n",
      "Train Loss : 0.01024425921802578\n",
      "Epoch : 72\n",
      "Val Loss : 0.008547863841056824\n",
      "Epoch : 73\n",
      "Train Loss : 0.010241955699352713\n",
      "Epoch : 73\n",
      "Val Loss : 0.008547778144478798\n",
      "Epoch : 74\n",
      "Train Loss : 0.010239675124026702\n",
      "Epoch : 74\n",
      "Val Loss : 0.008547713965177535\n",
      "Epoch : 75\n",
      "Train Loss : 0.010237416414895303\n",
      "Epoch : 75\n",
      "Val Loss : 0.008547669410705566\n",
      "Epoch : 76\n",
      "Train Loss : 0.010235186523734674\n",
      "Epoch : 76\n",
      "Val Loss : 0.008547645702958106\n",
      "Epoch : 77\n",
      "Train Loss : 0.010232975746229875\n",
      "Epoch : 77\n",
      "Val Loss : 0.008547642812132836\n",
      "Epoch : 78\n",
      "Train Loss : 0.010230784082380902\n",
      "Epoch : 78\n",
      "Val Loss : 0.008547658026218414\n",
      "Epoch : 79\n",
      "Train Loss : 0.010228620262094028\n",
      "Epoch : 79\n",
      "Val Loss : 0.008547696083784103\n",
      "Epoch : 80\n",
      "Train Loss : 0.010226472115203791\n",
      "Epoch : 80\n",
      "Val Loss : 0.008547753393650055\n",
      "Epoch : 81\n",
      "Train Loss : 0.010224344812041925\n",
      "Epoch : 81\n",
      "Val Loss : 0.008547833666205406\n",
      "Epoch : 82\n",
      "Train Loss : 0.010222237949919128\n",
      "Epoch : 82\n",
      "Val Loss : 0.008547926858067512\n",
      "Epoch : 83\n",
      "Train Loss : 0.01022015078311448\n",
      "Epoch : 83\n",
      "Val Loss : 0.008548045516014098\n",
      "Epoch : 84\n",
      "Train Loss : 0.010218080476231316\n",
      "Epoch : 84\n",
      "Val Loss : 0.00854818019270897\n",
      "Epoch : 85\n",
      "Train Loss : 0.010216032222801574\n",
      "Epoch : 85\n",
      "Val Loss : 0.008548338294029236\n",
      "Epoch : 86\n",
      "Train Loss : 0.01021400019791627\n",
      "Epoch : 86\n",
      "Val Loss : 0.00854851146042347\n",
      "Epoch : 87\n",
      "Train Loss : 0.010211986053761533\n",
      "Epoch : 87\n",
      "Val Loss : 0.008548702523112297\n",
      "Epoch : 88\n",
      "Train Loss : 0.010209985387269607\n",
      "Epoch : 88\n",
      "Val Loss : 0.00854891248047352\n",
      "Epoch : 89\n",
      "Train Loss : 0.010208006349998754\n",
      "Epoch : 89\n",
      "Val Loss : 0.008549138233065605\n",
      "Epoch : 90\n",
      "Train Loss : 0.010206042155888582\n",
      "Epoch : 90\n",
      "Val Loss : 0.00854938766360283\n",
      "Epoch : 91\n",
      "Train Loss : 0.01020409240059263\n",
      "Epoch : 91\n",
      "Val Loss : 0.00854965016245842\n",
      "Epoch : 92\n",
      "Train Loss : 0.010202163089650064\n",
      "Epoch : 92\n",
      "Val Loss : 0.008549933210015298\n",
      "Epoch : 93\n",
      "Train Loss : 0.010200247917576193\n",
      "Epoch : 93\n",
      "Val Loss : 0.008550231382250785\n",
      "Epoch : 94\n",
      "Train Loss : 0.01019834566304586\n",
      "Epoch : 94\n",
      "Val Loss : 0.008550549805164337\n",
      "Epoch : 95\n",
      "Train Loss : 0.010196455475937217\n",
      "Epoch : 95\n",
      "Val Loss : 0.0085508813560009\n",
      "Epoch : 96\n",
      "Train Loss : 0.010194585607237981\n",
      "Epoch : 96\n",
      "Val Loss : 0.008551231935620308\n",
      "Epoch : 97\n",
      "Train Loss : 0.010192725414682007\n",
      "Epoch : 97\n",
      "Val Loss : 0.008551597252488136\n",
      "Epoch : 98\n",
      "Train Loss : 0.010190879365966202\n",
      "Epoch : 98\n",
      "Val Loss : 0.00855198024213314\n",
      "Epoch : 99\n",
      "Train Loss : 0.010189048821616958\n",
      "Epoch : 99\n",
      "Val Loss : 0.008552380621433257\n",
      "Epoch : 100\n",
      "Train Loss : 0.010187228288156812\n",
      "Epoch : 100\n",
      "Val Loss : 0.00855279329419136\n",
      "Epoch : 101\n",
      "Train Loss : 0.010185425998344752\n",
      "Epoch : 101\n",
      "Val Loss : 0.008553227424621583\n",
      "Epoch : 102\n",
      "Train Loss : 0.010183632151750695\n",
      "Epoch : 102\n",
      "Val Loss : 0.008553676053881645\n",
      "Epoch : 103\n",
      "Train Loss : 0.010181852752256648\n",
      "Epoch : 103\n",
      "Val Loss : 0.008554138779640197\n",
      "Epoch : 104\n",
      "Train Loss : 0.010180084856750702\n",
      "Epoch : 104\n",
      "Val Loss : 0.008554618507623672\n",
      "Epoch : 105\n",
      "Train Loss : 0.010178328132144178\n",
      "Epoch : 105\n",
      "Val Loss : 0.0085551136136055\n",
      "Epoch : 106\n",
      "Train Loss : 0.010176585287889764\n",
      "Epoch : 106\n",
      "Val Loss : 0.008555620223283768\n",
      "Epoch : 107\n",
      "Train Loss : 0.010174852401495404\n",
      "Epoch : 107\n",
      "Val Loss : 0.00855614474415779\n",
      "Epoch : 108\n",
      "Train Loss : 0.010173133264537702\n",
      "Epoch : 108\n",
      "Val Loss : 0.008556686803698539\n",
      "Epoch : 109\n",
      "Train Loss : 0.01017142336126218\n",
      "Epoch : 109\n",
      "Val Loss : 0.008557239040732384\n",
      "Epoch : 110\n",
      "Train Loss : 0.010169721276456245\n",
      "Epoch : 110\n",
      "Val Loss : 0.008557806238532067\n",
      "Epoch : 111\n",
      "Train Loss : 0.010168033822694813\n",
      "Epoch : 111\n",
      "Val Loss : 0.008558389529585838\n",
      "Epoch : 112\n",
      "Train Loss : 0.010166356542223925\n",
      "Epoch : 112\n",
      "Val Loss : 0.008558982789516448\n",
      "Epoch : 113\n",
      "Train Loss : 0.010164690840313231\n",
      "Epoch : 113\n",
      "Val Loss : 0.008559601143002511\n",
      "Epoch : 114\n",
      "Train Loss : 0.010163027859455326\n",
      "Epoch : 114\n",
      "Val Loss : 0.008560220658779144\n",
      "Epoch : 115\n",
      "Train Loss : 0.010161384149683221\n",
      "Epoch : 115\n",
      "Val Loss : 0.008560863837599754\n",
      "Epoch : 116\n",
      "Train Loss : 0.010159742040725365\n",
      "Epoch : 116\n",
      "Val Loss : 0.008561513125896454\n",
      "Epoch : 117\n",
      "Train Loss : 0.010158114718584828\n",
      "Epoch : 117\n",
      "Val Loss : 0.008562179341912269\n",
      "Epoch : 118\n",
      "Train Loss : 0.01015649544028731\n",
      "Epoch : 118\n",
      "Val Loss : 0.008562856301665306\n",
      "Epoch : 119\n",
      "Train Loss : 0.010154883886001394\n",
      "Epoch : 119\n",
      "Val Loss : 0.008563546121120452\n",
      "Epoch : 120\n",
      "Train Loss : 0.010153284206906884\n",
      "Epoch : 120\n",
      "Val Loss : 0.00856425042450428\n",
      "Epoch : 121\n",
      "Train Loss : 0.010151690841582852\n",
      "Epoch : 121\n",
      "Val Loss : 0.008564966946840287\n",
      "Epoch : 122\n",
      "Train Loss : 0.010150106227708138\n",
      "Epoch : 122\n",
      "Val Loss : 0.008565693765878677\n",
      "Epoch : 123\n",
      "Train Loss : 0.010148531618093893\n",
      "Epoch : 123\n",
      "Val Loss : 0.008566435620188714\n",
      "Epoch : 124\n",
      "Train Loss : 0.010146963726596582\n",
      "Epoch : 124\n",
      "Val Loss : 0.008567189872264862\n",
      "Epoch : 125\n",
      "Train Loss : 0.010145404192145078\n",
      "Epoch : 125\n",
      "Val Loss : 0.008567955166101456\n",
      "Epoch : 126\n",
      "Train Loss : 0.01014385640859816\n",
      "Epoch : 126\n",
      "Val Loss : 0.008568731009960175\n",
      "Epoch : 127\n",
      "Train Loss : 0.0101423149669934\n",
      "Epoch : 127\n",
      "Val Loss : 0.008569520816206933\n",
      "Epoch : 128\n",
      "Train Loss : 0.010140780737338541\n",
      "Epoch : 128\n",
      "Val Loss : 0.008570324897766114\n",
      "Epoch : 129\n",
      "Train Loss : 0.01013925231767825\n",
      "Epoch : 129\n",
      "Val Loss : 0.00857113067805767\n",
      "Epoch : 130\n",
      "Train Loss : 0.010137735556121717\n",
      "Epoch : 130\n",
      "Val Loss : 0.00857195682823658\n",
      "Epoch : 131\n",
      "Train Loss : 0.01013622119247224\n",
      "Epoch : 131\n",
      "Val Loss : 0.008572793170809745\n",
      "Epoch : 132\n",
      "Train Loss : 0.010134716654110209\n",
      "Epoch : 132\n",
      "Val Loss : 0.008573632538318634\n",
      "Epoch : 133\n",
      "Train Loss : 0.010133222176352005\n",
      "Epoch : 133\n",
      "Val Loss : 0.008574492827057838\n",
      "Epoch : 134\n",
      "Train Loss : 0.010131732497722229\n",
      "Epoch : 134\n",
      "Val Loss : 0.00857536095380783\n",
      "Epoch : 135\n",
      "Train Loss : 0.01013024664878368\n",
      "Epoch : 135\n",
      "Val Loss : 0.008576237067580223\n",
      "Epoch : 136\n",
      "Train Loss : 0.010128771680741971\n",
      "Epoch : 136\n",
      "Val Loss : 0.008577122703194618\n",
      "Epoch : 137\n",
      "Train Loss : 0.010127300638506634\n",
      "Epoch : 137\n",
      "Val Loss : 0.00857801778614521\n",
      "Epoch : 138\n",
      "Train Loss : 0.010125838500179112\n",
      "Epoch : 138\n",
      "Val Loss : 0.008578924775123595\n",
      "Epoch : 139\n",
      "Train Loss : 0.010124385779478266\n",
      "Epoch : 139\n",
      "Val Loss : 0.008579836145043374\n",
      "Epoch : 140\n",
      "Train Loss : 0.010122938045164656\n",
      "Epoch : 140\n",
      "Val Loss : 0.008580765515565872\n",
      "Epoch : 141\n",
      "Train Loss : 0.01012149660142132\n",
      "Epoch : 141\n",
      "Val Loss : 0.008581704452633858\n",
      "Epoch : 142\n",
      "Train Loss : 0.01012005708495228\n",
      "Epoch : 142\n",
      "Val Loss : 0.008582646846771241\n",
      "Epoch : 143\n",
      "Train Loss : 0.010118626546963147\n",
      "Epoch : 143\n",
      "Val Loss : 0.008583596915006637\n",
      "Epoch : 144\n",
      "Train Loss : 0.010117204508535378\n",
      "Epoch : 144\n",
      "Val Loss : 0.008584562823176383\n",
      "Epoch : 145\n",
      "Train Loss : 0.010115784604526605\n",
      "Epoch : 145\n",
      "Val Loss : 0.00858553421497345\n",
      "Epoch : 146\n",
      "Train Loss : 0.010114377904749637\n",
      "Epoch : 146\n",
      "Val Loss : 0.008586516305804252\n",
      "Epoch : 147\n",
      "Train Loss : 0.010112969706902194\n",
      "Epoch : 147\n",
      "Val Loss : 0.00858750669658184\n",
      "Epoch : 148\n",
      "Train Loss : 0.010111567971969415\n",
      "Epoch : 148\n",
      "Val Loss : 0.008588507562875748\n",
      "Epoch : 149\n",
      "Train Loss : 0.010110174246079344\n",
      "Epoch : 149\n",
      "Val Loss : 0.008589508563280106\n",
      "Epoch : 150\n",
      "Train Loss : 0.010108786195954076\n",
      "Epoch : 150\n",
      "Val Loss : 0.008590527206659317\n",
      "Epoch : 151\n",
      "Train Loss : 0.010107404608743471\n",
      "Epoch : 151\n",
      "Val Loss : 0.008591555565595627\n",
      "Epoch : 152\n",
      "Train Loss : 0.01010602200072378\n",
      "Epoch : 152\n",
      "Val Loss : 0.008592581808567047\n",
      "Epoch : 153\n",
      "Train Loss : 0.010104653304542193\n",
      "Epoch : 153\n",
      "Val Loss : 0.008593623295426369\n",
      "Epoch : 154\n",
      "Train Loss : 0.010103285965572684\n",
      "Epoch : 154\n",
      "Val Loss : 0.008594665467739106\n",
      "Epoch : 155\n",
      "Train Loss : 0.010101922005547536\n",
      "Epoch : 155\n",
      "Val Loss : 0.008595720499753951\n",
      "Epoch : 156\n",
      "Train Loss : 0.010100566700856562\n",
      "Epoch : 156\n",
      "Val Loss : 0.008596787750720978\n",
      "Epoch : 157\n",
      "Train Loss : 0.010099216858157059\n",
      "Epoch : 157\n",
      "Val Loss : 0.00859785296022892\n",
      "Epoch : 158\n",
      "Train Loss : 0.010097868011409277\n",
      "Epoch : 158\n",
      "Val Loss : 0.008598927795886993\n",
      "Epoch : 159\n",
      "Train Loss : 0.010096526890330255\n",
      "Epoch : 159\n",
      "Val Loss : 0.00860000966489315\n",
      "Epoch : 160\n",
      "Train Loss : 0.010095194838874812\n",
      "Epoch : 160\n",
      "Val Loss : 0.008601098001003265\n",
      "Epoch : 161\n",
      "Train Loss : 0.010093863049250269\n",
      "Epoch : 161\n",
      "Val Loss : 0.008602189987897873\n",
      "Epoch : 162\n",
      "Train Loss : 0.010092539356497789\n",
      "Epoch : 162\n",
      "Val Loss : 0.008603300094604492\n",
      "Epoch : 163\n",
      "Train Loss : 0.010091216676268607\n",
      "Epoch : 163\n",
      "Val Loss : 0.008604414671659469\n",
      "Epoch : 164\n",
      "Train Loss : 0.010089901541078005\n",
      "Epoch : 164\n",
      "Val Loss : 0.008605531752109527\n",
      "Epoch : 165\n",
      "Train Loss : 0.010088587635498348\n",
      "Epoch : 165\n",
      "Val Loss : 0.008606649741530418\n",
      "Epoch : 166\n",
      "Train Loss : 0.010087280970040271\n",
      "Epoch : 166\n",
      "Val Loss : 0.00860777884721756\n",
      "Epoch : 167\n",
      "Train Loss : 0.010085978909823181\n",
      "Epoch : 167\n",
      "Val Loss : 0.00860890991985798\n",
      "Epoch : 168\n",
      "Train Loss : 0.010084681652048835\n",
      "Epoch : 168\n",
      "Val Loss : 0.008610056251287461\n",
      "Epoch : 169\n",
      "Train Loss : 0.010083387193213684\n",
      "Epoch : 169\n",
      "Val Loss : 0.00861120367050171\n",
      "Epoch : 170\n",
      "Train Loss : 0.010082097964367938\n",
      "Epoch : 170\n",
      "Val Loss : 0.008612353876233101\n",
      "Epoch : 171\n",
      "Train Loss : 0.010080811615662112\n",
      "Epoch : 171\n",
      "Val Loss : 0.008613513737916946\n",
      "Epoch : 172\n",
      "Train Loss : 0.01007953215244614\n",
      "Epoch : 172\n",
      "Val Loss : 0.008614675670862198\n",
      "Epoch : 173\n",
      "Train Loss : 0.010078259412318574\n",
      "Epoch : 173\n",
      "Val Loss : 0.008615853995084763\n",
      "Epoch : 174\n",
      "Train Loss : 0.010076985845269363\n",
      "Epoch : 174\n",
      "Val Loss : 0.008617024898529052\n",
      "Epoch : 175\n",
      "Train Loss : 0.010075722096878969\n",
      "Epoch : 175\n",
      "Val Loss : 0.008618213906884194\n",
      "Epoch : 176\n",
      "Train Loss : 0.010074455683779144\n",
      "Epoch : 176\n",
      "Val Loss : 0.008619401663541793\n",
      "Epoch : 177\n",
      "Train Loss : 0.010073195725308194\n",
      "Epoch : 177\n",
      "Val Loss : 0.008620596528053283\n",
      "Epoch : 178\n",
      "Train Loss : 0.010071940575908617\n",
      "Epoch : 178\n",
      "Val Loss : 0.00862178684771061\n",
      "Epoch : 179\n",
      "Train Loss : 0.010070692298741633\n",
      "Epoch : 179\n",
      "Val Loss : 0.008622984811663627\n",
      "Epoch : 180\n",
      "Train Loss : 0.010069447238117563\n",
      "Epoch : 180\n",
      "Val Loss : 0.008624197110533714\n",
      "Epoch : 181\n",
      "Train Loss : 0.01006820600055609\n",
      "Epoch : 181\n",
      "Val Loss : 0.008625406563282013\n",
      "Epoch : 182\n",
      "Train Loss : 0.010066963803500362\n",
      "Epoch : 182\n",
      "Val Loss : 0.008626620545983314\n",
      "Epoch : 183\n",
      "Train Loss : 0.010065730465609195\n",
      "Epoch : 183\n",
      "Val Loss : 0.008627844750881195\n",
      "Epoch : 184\n",
      "Train Loss : 0.010064500927580421\n",
      "Epoch : 184\n",
      "Val Loss : 0.008629077136516571\n",
      "Epoch : 185\n",
      "Train Loss : 0.010063270532801163\n",
      "Epoch : 185\n",
      "Val Loss : 0.008630309909582138\n",
      "Epoch : 186\n",
      "Train Loss : 0.010062046426935022\n",
      "Epoch : 186\n",
      "Val Loss : 0.008631543442606925\n",
      "Epoch : 187\n",
      "Train Loss : 0.010060828276893316\n",
      "Epoch : 187\n",
      "Val Loss : 0.008632780015468598\n",
      "Epoch : 188\n",
      "Train Loss : 0.01005961293076228\n",
      "Epoch : 188\n",
      "Val Loss : 0.00863401947915554\n",
      "Epoch : 189\n",
      "Train Loss : 0.010058399490362491\n",
      "Epoch : 189\n",
      "Val Loss : 0.008635257959365844\n",
      "Epoch : 190\n",
      "Train Loss : 0.010057196031022963\n",
      "Epoch : 190\n",
      "Val Loss : 0.008636510998010636\n",
      "Epoch : 191\n",
      "Train Loss : 0.010055989159595925\n",
      "Epoch : 191\n",
      "Val Loss : 0.008637771010398865\n",
      "Epoch : 192\n",
      "Train Loss : 0.010054787385585682\n",
      "Epoch : 192\n",
      "Val Loss : 0.008639019340276718\n",
      "Epoch : 193\n",
      "Train Loss : 0.010053592861639966\n",
      "Epoch : 193\n",
      "Val Loss : 0.008640294954180717\n",
      "Epoch : 194\n",
      "Train Loss : 0.010052397038482687\n",
      "Epoch : 194\n",
      "Val Loss : 0.008641562417149544\n",
      "Epoch : 195\n",
      "Train Loss : 0.010051208042814744\n",
      "Epoch : 195\n",
      "Val Loss : 0.008642838582396508\n",
      "Epoch : 196\n",
      "Train Loss : 0.010050018795258846\n",
      "Epoch : 196\n",
      "Val Loss : 0.008644101679325104\n",
      "Epoch : 197\n",
      "Train Loss : 0.010048831382176418\n",
      "Epoch : 197\n",
      "Val Loss : 0.008645370706915855\n",
      "Epoch : 198\n",
      "Train Loss : 0.010047658013504702\n",
      "Epoch : 198\n",
      "Val Loss : 0.008646663367748261\n",
      "Epoch : 199\n",
      "Train Loss : 0.01004647942147221\n",
      "Epoch : 199\n",
      "Val Loss : 0.008647946551442146\n",
      "Epoch : 200\n",
      "Train Loss : 0.01004530440061436\n",
      "Epoch : 200\n",
      "Val Loss : 0.008649227276444436\n",
      "Epoch : 201\n",
      "Train Loss : 0.010044135430038929\n",
      "Epoch : 201\n",
      "Val Loss : 0.008650519117712975\n",
      "Epoch : 202\n",
      "Train Loss : 0.010042969251774066\n",
      "Epoch : 202\n",
      "Val Loss : 0.008651819854974747\n",
      "Epoch : 203\n",
      "Train Loss : 0.010041805852562508\n",
      "Epoch : 203\n",
      "Val Loss : 0.0086531230956316\n",
      "Epoch : 204\n",
      "Train Loss : 0.010040643457588459\n",
      "Epoch : 204\n",
      "Val Loss : 0.008654416427016259\n",
      "Epoch : 205\n",
      "Train Loss : 0.01003948684940877\n",
      "Epoch : 205\n",
      "Val Loss : 0.008655721798539162\n",
      "Epoch : 206\n",
      "Train Loss : 0.010038335368474713\n",
      "Epoch : 206\n",
      "Val Loss : 0.008657034933567048\n",
      "Epoch : 207\n",
      "Train Loss : 0.01003718113168756\n",
      "Epoch : 207\n",
      "Val Loss : 0.008658333837985992\n",
      "Epoch : 208\n",
      "Train Loss : 0.010036036116982483\n",
      "Epoch : 208\n",
      "Val Loss : 0.008659648180007935\n",
      "Epoch : 209\n",
      "Train Loss : 0.010034892530747262\n",
      "Epoch : 209\n",
      "Val Loss : 0.008660965815186501\n",
      "Epoch : 210\n",
      "Train Loss : 0.010033753392323055\n",
      "Epoch : 210\n",
      "Val Loss : 0.008662282466888428\n",
      "Epoch : 211\n",
      "Train Loss : 0.010032612767428478\n",
      "Epoch : 211\n",
      "Val Loss : 0.008663598284125327\n",
      "Epoch : 212\n",
      "Train Loss : 0.010031479308083388\n",
      "Epoch : 212\n",
      "Val Loss : 0.008664920210838319\n",
      "Epoch : 213\n",
      "Train Loss : 0.010030349920374536\n",
      "Epoch : 213\n",
      "Val Loss : 0.008666253313422204\n",
      "Epoch : 214\n",
      "Train Loss : 0.010029219644429209\n",
      "Epoch : 214\n",
      "Val Loss : 0.00866758143901825\n",
      "Epoch : 215\n",
      "Train Loss : 0.01002809227679548\n",
      "Epoch : 215\n",
      "Val Loss : 0.00866890724003315\n",
      "Epoch : 216\n",
      "Train Loss : 0.010026970333038595\n",
      "Epoch : 216\n",
      "Val Loss : 0.008670241340994835\n",
      "Epoch : 217\n",
      "Train Loss : 0.010025849776322626\n",
      "Epoch : 217\n",
      "Val Loss : 0.008671571269631386\n",
      "Epoch : 218\n",
      "Train Loss : 0.010024734399881333\n",
      "Epoch : 218\n",
      "Val Loss : 0.008672906503081322\n",
      "Epoch : 219\n",
      "Train Loss : 0.010023620440309794\n",
      "Epoch : 219\n",
      "Val Loss : 0.008674250423908233\n",
      "Epoch : 220\n",
      "Train Loss : 0.010022510331972531\n",
      "Epoch : 220\n",
      "Val Loss : 0.008675595611333848\n",
      "Epoch : 221\n",
      "Train Loss : 0.010021404562073882\n",
      "Epoch : 221\n",
      "Val Loss : 0.00867694342136383\n",
      "Epoch : 222\n",
      "Train Loss : 0.01002029643403996\n",
      "Epoch : 222\n",
      "Val Loss : 0.008678281530737877\n",
      "Epoch : 223\n",
      "Train Loss : 0.010019191688264711\n",
      "Epoch : 223\n",
      "Val Loss : 0.008679629400372505\n",
      "Epoch : 224\n",
      "Train Loss : 0.010018096956692875\n",
      "Epoch : 224\n",
      "Val Loss : 0.008680984154343605\n",
      "Epoch : 225\n",
      "Train Loss : 0.010016998120341649\n",
      "Epoch : 225\n",
      "Val Loss : 0.008682336181402206\n",
      "Epoch : 226\n",
      "Train Loss : 0.010015904745981893\n",
      "Epoch : 226\n",
      "Val Loss : 0.00868368062376976\n",
      "Epoch : 227\n",
      "Train Loss : 0.010014811399793816\n",
      "Epoch : 227\n",
      "Val Loss : 0.008685036346316337\n",
      "Epoch : 228\n",
      "Train Loss : 0.010013729411763971\n",
      "Epoch : 228\n",
      "Val Loss : 0.008686394467949866\n",
      "Epoch : 229\n",
      "Train Loss : 0.010012640877961582\n",
      "Epoch : 229\n",
      "Val Loss : 0.008687759533524513\n",
      "Epoch : 230\n",
      "Train Loss : 0.010011556871513774\n",
      "Epoch : 230\n",
      "Val Loss : 0.008689112603664399\n",
      "Epoch : 231\n",
      "Train Loss : 0.010010476888644631\n",
      "Epoch : 231\n",
      "Val Loss : 0.008690476641058922\n",
      "Epoch : 232\n",
      "Train Loss : 0.01000940053992212\n",
      "Epoch : 232\n",
      "Val Loss : 0.008691848978400231\n",
      "Epoch : 233\n",
      "Train Loss : 0.010008325889786155\n",
      "Epoch : 233\n",
      "Val Loss : 0.008693209961056709\n",
      "Epoch : 234\n",
      "Train Loss : 0.010007252050000259\n",
      "Epoch : 234\n",
      "Val Loss : 0.008694575503468513\n",
      "Epoch : 235\n",
      "Train Loss : 0.010006185075818326\n",
      "Epoch : 235\n",
      "Val Loss : 0.008695944294333458\n",
      "Epoch : 236\n",
      "Train Loss : 0.010005115436926962\n",
      "Epoch : 236\n",
      "Val Loss : 0.008697307243943214\n",
      "Epoch : 237\n",
      "Train Loss : 0.010004055351549196\n",
      "Epoch : 237\n",
      "Val Loss : 0.00869867318868637\n",
      "Epoch : 238\n",
      "Train Loss : 0.01000299396530271\n",
      "Epoch : 238\n",
      "Val Loss : 0.008700054407119752\n",
      "Epoch : 239\n",
      "Train Loss : 0.01000193364792288\n",
      "Epoch : 239\n",
      "Val Loss : 0.0087014230042696\n",
      "Epoch : 240\n",
      "Train Loss : 0.01000087720000606\n",
      "Epoch : 240\n",
      "Val Loss : 0.008702803656458855\n",
      "Epoch : 241\n",
      "Train Loss : 0.009999823754858823\n",
      "Epoch : 241\n",
      "Val Loss : 0.008704178318381309\n",
      "Epoch : 242\n",
      "Train Loss : 0.009998772580017504\n",
      "Epoch : 242\n",
      "Val Loss : 0.008705542862415313\n",
      "Epoch : 243\n",
      "Train Loss : 0.009997723055705163\n",
      "Epoch : 243\n",
      "Val Loss : 0.008706924423575402\n",
      "Epoch : 244\n",
      "Train Loss : 0.00999668066877063\n",
      "Epoch : 244\n",
      "Val Loss : 0.008708309143781663\n",
      "Epoch : 245\n",
      "Train Loss : 0.009995639080586064\n",
      "Epoch : 245\n",
      "Val Loss : 0.00870969432592392\n",
      "Epoch : 246\n",
      "Train Loss : 0.00999459770451767\n",
      "Epoch : 246\n",
      "Val Loss : 0.008711067542433738\n",
      "Epoch : 247\n",
      "Train Loss : 0.009993556573708604\n",
      "Epoch : 247\n",
      "Val Loss : 0.008712450042366982\n",
      "Epoch : 248\n",
      "Train Loss : 0.0099925196504227\n",
      "Epoch : 248\n",
      "Val Loss : 0.008713833212852478\n",
      "Epoch : 249\n",
      "Train Loss : 0.00999148659328549\n",
      "Epoch : 249\n",
      "Val Loss : 0.008715215280652047\n",
      "Epoch : 250\n",
      "Train Loss : 0.009990455206563146\n",
      "Epoch : 250\n",
      "Val Loss : 0.008716601431369781\n",
      "Epoch : 251\n",
      "Train Loss : 0.009989427931248823\n",
      "Epoch : 251\n",
      "Val Loss : 0.008717997714877128\n",
      "Epoch : 252\n",
      "Train Loss : 0.009988398887747335\n",
      "Epoch : 252\n",
      "Val Loss : 0.008719364672899247\n",
      "Epoch : 253\n",
      "Train Loss : 0.009987375791784495\n",
      "Epoch : 253\n",
      "Val Loss : 0.008720757827162742\n",
      "Epoch : 254\n",
      "Train Loss : 0.009986357276205279\n",
      "Epoch : 254\n",
      "Val Loss : 0.008722146973013878\n",
      "Epoch : 255\n",
      "Train Loss : 0.009985338454051904\n",
      "Epoch : 255\n",
      "Val Loss : 0.008723530799150466\n",
      "Epoch : 256\n",
      "Train Loss : 0.009984320125731498\n",
      "Epoch : 256\n",
      "Val Loss : 0.008724920198321343\n",
      "Epoch : 257\n",
      "Train Loss : 0.00998330438589127\n",
      "Epoch : 257\n",
      "Val Loss : 0.00872629602253437\n",
      "Epoch : 258\n",
      "Train Loss : 0.009982296411491585\n",
      "Epoch : 258\n",
      "Val Loss : 0.008727695524692535\n",
      "Epoch : 259\n",
      "Train Loss : 0.009981287096451397\n",
      "Epoch : 259\n",
      "Val Loss : 0.008729088082909585\n",
      "Epoch : 260\n",
      "Train Loss : 0.009980277007518608\n",
      "Epoch : 260\n",
      "Val Loss : 0.008730479761958123\n",
      "Epoch : 261\n",
      "Train Loss : 0.009979271175823708\n",
      "Epoch : 261\n",
      "Val Loss : 0.0087318734228611\n",
      "Epoch : 262\n",
      "Train Loss : 0.009978270300687209\n",
      "Epoch : 262\n",
      "Val Loss : 0.008733273178339005\n",
      "Epoch : 263\n",
      "Train Loss : 0.00997727091864971\n",
      "Epoch : 263\n",
      "Val Loss : 0.008734663054347037\n",
      "Epoch : 264\n",
      "Train Loss : 0.009976271140551546\n",
      "Epoch : 264\n",
      "Val Loss : 0.008736049979925156\n",
      "Epoch : 265\n",
      "Train Loss : 0.009975273082582977\n",
      "Epoch : 265\n",
      "Val Loss : 0.008737433582544326\n",
      "Epoch : 266\n",
      "Train Loss : 0.009974278027383989\n",
      "Epoch : 266\n",
      "Val Loss : 0.008738821864128114\n",
      "Epoch : 267\n",
      "Train Loss : 0.00997328952127186\n",
      "Epoch : 267\n",
      "Val Loss : 0.008740228101611137\n",
      "Epoch : 268\n",
      "Train Loss : 0.009972303419695417\n",
      "Epoch : 268\n",
      "Val Loss : 0.008741621598601342\n",
      "Epoch : 269\n",
      "Train Loss : 0.009971313276311575\n",
      "Epoch : 269\n",
      "Val Loss : 0.00874301390349865\n",
      "Epoch : 270\n",
      "Train Loss : 0.009970326569872607\n",
      "Epoch : 270\n",
      "Val Loss : 0.008744407057762147\n",
      "Epoch : 271\n",
      "Train Loss : 0.009969345222681336\n",
      "Epoch : 271\n",
      "Val Loss : 0.008745803192257881\n",
      "Epoch : 272\n",
      "Train Loss : 0.009968368043241445\n",
      "Epoch : 272\n",
      "Val Loss : 0.008747196480631828\n",
      "Epoch : 273\n",
      "Train Loss : 0.009967387726802206\n",
      "Epoch : 273\n",
      "Val Loss : 0.008748580321669579\n",
      "Epoch : 274\n",
      "Train Loss : 0.009966409379066203\n",
      "Epoch : 274\n",
      "Val Loss : 0.008749985560774803\n",
      "Epoch : 275\n",
      "Train Loss : 0.009965437335157734\n",
      "Epoch : 275\n",
      "Val Loss : 0.008751384913921356\n",
      "Epoch : 276\n",
      "Train Loss : 0.009964467251666713\n",
      "Epoch : 276\n",
      "Val Loss : 0.008752780050039292\n",
      "Epoch : 277\n",
      "Train Loss : 0.00996349680857249\n",
      "Epoch : 277\n",
      "Val Loss : 0.008754173398017883\n",
      "Epoch : 278\n",
      "Train Loss : 0.00996252733988694\n",
      "Epoch : 278\n",
      "Val Loss : 0.008755574613809585\n",
      "Epoch : 279\n",
      "Train Loss : 0.009961561296546162\n",
      "Epoch : 279\n",
      "Val Loss : 0.00875695414841175\n",
      "Epoch : 280\n",
      "Train Loss : 0.0099605976096835\n",
      "Epoch : 280\n",
      "Val Loss : 0.008758357480168342\n",
      "Epoch : 281\n",
      "Train Loss : 0.00995963979910171\n",
      "Epoch : 281\n",
      "Val Loss : 0.0087597536444664\n",
      "Epoch : 282\n",
      "Train Loss : 0.009958681612345142\n",
      "Epoch : 282\n",
      "Val Loss : 0.008761145323514938\n",
      "Epoch : 283\n",
      "Train Loss : 0.009957721929175255\n",
      "Epoch : 283\n",
      "Val Loss : 0.008762529715895653\n",
      "Epoch : 284\n",
      "Train Loss : 0.009956769668414286\n",
      "Epoch : 284\n",
      "Val Loss : 0.008763922542333604\n",
      "Epoch : 285\n",
      "Train Loss : 0.009955818806294339\n",
      "Epoch : 285\n",
      "Val Loss : 0.00876532207429409\n",
      "Epoch : 286\n",
      "Train Loss : 0.009954867072509489\n",
      "Epoch : 286\n",
      "Val Loss : 0.008766721069812775\n",
      "Epoch : 287\n",
      "Train Loss : 0.009953913827396903\n",
      "Epoch : 287\n",
      "Val Loss : 0.008768100187182427\n",
      "Epoch : 288\n",
      "Train Loss : 0.009952971153292686\n",
      "Epoch : 288\n",
      "Val Loss : 0.008769505977630616\n",
      "Epoch : 289\n",
      "Train Loss : 0.009952028565360664\n",
      "Epoch : 289\n",
      "Val Loss : 0.00877089160680771\n",
      "Epoch : 290\n",
      "Train Loss : 0.009951084079983184\n",
      "Epoch : 290\n",
      "Val Loss : 0.008772281020879746\n",
      "Epoch : 291\n",
      "Train Loss : 0.009950142370344693\n",
      "Epoch : 291\n",
      "Val Loss : 0.008773674830794335\n",
      "Epoch : 292\n",
      "Train Loss : 0.009949201351740924\n",
      "Epoch : 292\n",
      "Val Loss : 0.008775046557188033\n",
      "Epoch : 293\n",
      "Train Loss : 0.009948264958264034\n",
      "Epoch : 293\n",
      "Val Loss : 0.008776456475257873\n",
      "Epoch : 294\n",
      "Train Loss : 0.009947330793664125\n",
      "Epoch : 294\n",
      "Val Loss : 0.008777842968702316\n",
      "Epoch : 295\n",
      "Train Loss : 0.00994639540773906\n",
      "Epoch : 295\n",
      "Val Loss : 0.008779224961996079\n",
      "Epoch : 296\n",
      "Train Loss : 0.009945468610861865\n",
      "Epoch : 296\n",
      "Val Loss : 0.008780611544847489\n",
      "Epoch : 297\n",
      "Train Loss : 0.009944539445906453\n",
      "Epoch : 297\n",
      "Val Loss : 0.008782008931040764\n",
      "Epoch : 298\n",
      "Train Loss : 0.00994361127855992\n",
      "Epoch : 298\n",
      "Val Loss : 0.008783383399248123\n",
      "Epoch : 299\n",
      "Train Loss : 0.00994268800977127\n",
      "Epoch : 299\n",
      "Val Loss : 0.008784782633185387\n",
      "Epoch : 300\n",
      "Train Loss : 0.009941765736934342\n",
      "Epoch : 300\n",
      "Val Loss : 0.008786177143454551\n",
      "Epoch : 301\n",
      "Train Loss : 0.009940842458202745\n",
      "Epoch : 301\n",
      "Val Loss : 0.008787553429603577\n",
      "Epoch : 302\n",
      "Train Loss : 0.009939921915438358\n",
      "Epoch : 302\n",
      "Val Loss : 0.00878892894089222\n",
      "Epoch : 303\n",
      "Train Loss : 0.009939005038306595\n",
      "Epoch : 303\n",
      "Val Loss : 0.008790309205651284\n",
      "Epoch : 304\n",
      "Train Loss : 0.009938092998417884\n",
      "Epoch : 304\n",
      "Val Loss : 0.008791702687740327\n",
      "Epoch : 305\n",
      "Train Loss : 0.009937175519737908\n",
      "Epoch : 305\n",
      "Val Loss : 0.008793081864714622\n",
      "Epoch : 306\n",
      "Train Loss : 0.00993626494643368\n",
      "Epoch : 306\n",
      "Val Loss : 0.008794465601444245\n",
      "Epoch : 307\n",
      "Train Loss : 0.009935353630722841\n",
      "Epoch : 307\n",
      "Val Loss : 0.008795841485261918\n",
      "Epoch : 308\n",
      "Train Loss : 0.009934445826528972\n",
      "Epoch : 308\n",
      "Val Loss : 0.00879722249507904\n",
      "Epoch : 309\n",
      "Train Loss : 0.009933543954959332\n",
      "Epoch : 309\n",
      "Val Loss : 0.008798601821064948\n",
      "Epoch : 310\n",
      "Train Loss : 0.009932637070487933\n",
      "Epoch : 310\n",
      "Val Loss : 0.008799973905086517\n",
      "Epoch : 311\n",
      "Train Loss : 0.0099317360390972\n",
      "Epoch : 311\n",
      "Val Loss : 0.008801351964473724\n",
      "Epoch : 312\n",
      "Train Loss : 0.009930833088717957\n",
      "Epoch : 312\n",
      "Val Loss : 0.00880273173749447\n",
      "Epoch : 313\n",
      "Train Loss : 0.009929933412882145\n",
      "Epoch : 313\n",
      "Val Loss : 0.008804096102714539\n",
      "Epoch : 314\n",
      "Train Loss : 0.00992903678455917\n",
      "Epoch : 314\n",
      "Val Loss : 0.008805474683642388\n",
      "Epoch : 315\n",
      "Train Loss : 0.009928143984270266\n",
      "Epoch : 315\n",
      "Val Loss : 0.00880684769153595\n",
      "Epoch : 316\n",
      "Train Loss : 0.00992725051946116\n",
      "Epoch : 316\n",
      "Val Loss : 0.00880821943283081\n",
      "Epoch : 317\n",
      "Train Loss : 0.00992636101857305\n",
      "Epoch : 317\n",
      "Val Loss : 0.008809603378176689\n",
      "Epoch : 318\n",
      "Train Loss : 0.009925470324531463\n",
      "Epoch : 318\n",
      "Val Loss : 0.008810980230569839\n",
      "Epoch : 319\n",
      "Train Loss : 0.009924578679281409\n",
      "Epoch : 319\n",
      "Val Loss : 0.008812330409884453\n",
      "Epoch : 320\n",
      "Train Loss : 0.00992369372563377\n",
      "Epoch : 320\n",
      "Val Loss : 0.008813689544796943\n",
      "Epoch : 321\n",
      "Train Loss : 0.009922809756337747\n",
      "Epoch : 321\n",
      "Val Loss : 0.00881506983935833\n",
      "Epoch : 322\n",
      "Train Loss : 0.009921927724258968\n",
      "Epoch : 322\n",
      "Val Loss : 0.008816430434584618\n",
      "Epoch : 323\n",
      "Train Loss : 0.009921044470855032\n",
      "Epoch : 323\n",
      "Val Loss : 0.00881779171526432\n",
      "Epoch : 324\n",
      "Train Loss : 0.009920165060399585\n",
      "Epoch : 324\n",
      "Val Loss : 0.00881915758550167\n",
      "Epoch : 325\n",
      "Train Loss : 0.009919288076022872\n",
      "Epoch : 325\n",
      "Val Loss : 0.008820517659187318\n",
      "Epoch : 326\n",
      "Train Loss : 0.009918406935494884\n",
      "Epoch : 326\n",
      "Val Loss : 0.008821866288781167\n",
      "Epoch : 327\n",
      "Train Loss : 0.00991753554568225\n",
      "Epoch : 327\n",
      "Val Loss : 0.008823233783245087\n",
      "Epoch : 328\n",
      "Train Loss : 0.009916662150708912\n",
      "Epoch : 328\n",
      "Val Loss : 0.008824586123228072\n",
      "Epoch : 329\n",
      "Train Loss : 0.009915793102459975\n",
      "Epoch : 329\n",
      "Val Loss : 0.008825938940048218\n",
      "Epoch : 330\n",
      "Train Loss : 0.00991492429449889\n",
      "Epoch : 330\n",
      "Val Loss : 0.008827307716012001\n",
      "Epoch : 331\n",
      "Train Loss : 0.009914057435355152\n",
      "Epoch : 331\n",
      "Val Loss : 0.008828649803996086\n",
      "Epoch : 332\n",
      "Train Loss : 0.00991318748395532\n",
      "Epoch : 332\n",
      "Val Loss : 0.008829999640583992\n",
      "Epoch : 333\n",
      "Train Loss : 0.009912326123260519\n",
      "Epoch : 333\n",
      "Val Loss : 0.008831369146704673\n",
      "Epoch : 334\n",
      "Train Loss : 0.009911461779682025\n",
      "Epoch : 334\n",
      "Val Loss : 0.008832701414823532\n",
      "Epoch : 335\n",
      "Train Loss : 0.009910598997145997\n",
      "Epoch : 335\n",
      "Val Loss : 0.008834050551056861\n",
      "Epoch : 336\n",
      "Train Loss : 0.009909742727239362\n",
      "Epoch : 336\n",
      "Val Loss : 0.008835402593016625\n",
      "Epoch : 337\n",
      "Train Loss : 0.009908889678847111\n",
      "Epoch : 337\n",
      "Val Loss : 0.008836754634976386\n",
      "Epoch : 338\n",
      "Train Loss : 0.009908025771101068\n",
      "Epoch : 338\n",
      "Val Loss : 0.008838088303804397\n",
      "Epoch : 339\n",
      "Train Loss : 0.00990717180132919\n",
      "Epoch : 339\n",
      "Val Loss : 0.008839429840445518\n",
      "Epoch : 340\n",
      "Train Loss : 0.00990631715377985\n",
      "Epoch : 340\n",
      "Val Loss : 0.008840768054127692\n",
      "Epoch : 341\n",
      "Train Loss : 0.00990546841068306\n",
      "Epoch : 341\n",
      "Val Loss : 0.00884210841357708\n",
      "Epoch : 342\n",
      "Train Loss : 0.009904620552508432\n",
      "Epoch : 342\n",
      "Val Loss : 0.00884344071149826\n",
      "Epoch : 343\n",
      "Train Loss : 0.009903770258312123\n",
      "Epoch : 343\n",
      "Val Loss : 0.008844773784279824\n",
      "Epoch : 344\n",
      "Train Loss : 0.009902925205705322\n",
      "Epoch : 344\n",
      "Val Loss : 0.008846108213067055\n",
      "Epoch : 345\n",
      "Train Loss : 0.009902077271301445\n",
      "Epoch : 345\n",
      "Val Loss : 0.008847444638609887\n",
      "Epoch : 346\n",
      "Train Loss : 0.00990123406311106\n",
      "Epoch : 346\n",
      "Val Loss : 0.008848780229687691\n",
      "Epoch : 347\n",
      "Train Loss : 0.009900390672633338\n",
      "Epoch : 347\n",
      "Val Loss : 0.008850096464157105\n",
      "Epoch : 348\n",
      "Train Loss : 0.00989955019378153\n",
      "Epoch : 348\n",
      "Val Loss : 0.008851417005062104\n",
      "Epoch : 349\n",
      "Train Loss : 0.009898713927424358\n",
      "Epoch : 349\n",
      "Val Loss : 0.008852748528122902\n",
      "Epoch : 350\n",
      "Train Loss : 0.009897874363323548\n",
      "Epoch : 350\n",
      "Val Loss : 0.008854078680276871\n",
      "Epoch : 351\n",
      "Train Loss : 0.009897035808431721\n",
      "Epoch : 351\n",
      "Val Loss : 0.00885539211332798\n",
      "Epoch : 352\n",
      "Train Loss : 0.0098962037147974\n",
      "Epoch : 352\n",
      "Val Loss : 0.008856708154082299\n",
      "Epoch : 353\n",
      "Train Loss : 0.00989536946685819\n",
      "Epoch : 353\n",
      "Val Loss : 0.008858025267720223\n",
      "Epoch : 354\n",
      "Train Loss : 0.009894538082487324\n",
      "Epoch : 354\n",
      "Val Loss : 0.008859325304627418\n",
      "Epoch : 355\n",
      "Train Loss : 0.009893710488035903\n",
      "Epoch : 355\n",
      "Val Loss : 0.008860648065805435\n",
      "Epoch : 356\n",
      "Train Loss : 0.00989287811268479\n",
      "Epoch : 356\n",
      "Val Loss : 0.008861947327852249\n",
      "Epoch : 357\n",
      "Train Loss : 0.009892050768464167\n",
      "Epoch : 357\n",
      "Val Loss : 0.008863248988986015\n",
      "Epoch : 358\n",
      "Train Loss : 0.009891228254857967\n",
      "Epoch : 358\n",
      "Val Loss : 0.00886456885933876\n",
      "Epoch : 359\n",
      "Train Loss : 0.009890404175237828\n",
      "Epoch : 359\n",
      "Val Loss : 0.00886586856842041\n",
      "Epoch : 360\n",
      "Train Loss : 0.009889580523164352\n",
      "Epoch : 360\n",
      "Val Loss : 0.008867168977856636\n",
      "Epoch : 361\n",
      "Train Loss : 0.009888760826726083\n",
      "Epoch : 361\n",
      "Val Loss : 0.008868477061390876\n",
      "Epoch : 362\n",
      "Train Loss : 0.00988794107725877\n",
      "Epoch : 362\n",
      "Val Loss : 0.008869780451059342\n",
      "Epoch : 363\n",
      "Train Loss : 0.009887125260226455\n",
      "Epoch : 363\n",
      "Val Loss : 0.008871087610721588\n",
      "Epoch : 364\n",
      "Train Loss : 0.00988630608579283\n",
      "Epoch : 364\n",
      "Val Loss : 0.008872370809316635\n",
      "Epoch : 365\n",
      "Train Loss : 0.009885490109673387\n",
      "Epoch : 365\n",
      "Val Loss : 0.008873675391077996\n",
      "Epoch : 366\n",
      "Train Loss : 0.00988467653643247\n",
      "Epoch : 366\n",
      "Val Loss : 0.00887495206296444\n",
      "Epoch : 367\n",
      "Train Loss : 0.009883864862294171\n",
      "Epoch : 367\n",
      "Val Loss : 0.008876251459121704\n",
      "Epoch : 368\n",
      "Train Loss : 0.009883049741268052\n",
      "Epoch : 368\n",
      "Val Loss : 0.00887753164768219\n",
      "Epoch : 369\n",
      "Train Loss : 0.009882241472588633\n",
      "Epoch : 369\n",
      "Val Loss : 0.00887881813943386\n",
      "Epoch : 370\n",
      "Train Loss : 0.009881430479542111\n",
      "Epoch : 370\n",
      "Val Loss : 0.008880102053284645\n",
      "Epoch : 371\n",
      "Train Loss : 0.009880622096518818\n",
      "Epoch : 371\n",
      "Val Loss : 0.008881381690502167\n",
      "Epoch : 372\n",
      "Train Loss : 0.009879820267553855\n",
      "Epoch : 372\n",
      "Val Loss : 0.008882669538259506\n",
      "Epoch : 373\n",
      "Train Loss : 0.009879015124273681\n",
      "Epoch : 373\n",
      "Val Loss : 0.008883953407406806\n",
      "Epoch : 374\n",
      "Train Loss : 0.00987820820120624\n",
      "Epoch : 374\n",
      "Val Loss : 0.00888518615067005\n",
      "Epoch : 375\n",
      "Train Loss : 0.009877409277238562\n",
      "Epoch : 375\n",
      "Val Loss : 0.008886480510234832\n",
      "Epoch : 376\n",
      "Train Loss : 0.009876607720047448\n",
      "Epoch : 376\n",
      "Val Loss : 0.008887740284204483\n",
      "Epoch : 377\n",
      "Train Loss : 0.009875808569049177\n",
      "Epoch : 377\n",
      "Val Loss : 0.00888900101184845\n",
      "Epoch : 378\n",
      "Train Loss : 0.009875010735491204\n",
      "Epoch : 378\n",
      "Val Loss : 0.008890281960368156\n",
      "Epoch : 379\n",
      "Train Loss : 0.009874214171315957\n",
      "Epoch : 379\n",
      "Val Loss : 0.008891538515686989\n",
      "Epoch : 380\n",
      "Train Loss : 0.009873417767885\n",
      "Epoch : 380\n",
      "Val Loss : 0.008892795085906983\n",
      "Epoch : 381\n",
      "Train Loss : 0.00987262339944358\n",
      "Epoch : 381\n",
      "Val Loss : 0.008894058048725128\n",
      "Epoch : 382\n",
      "Train Loss : 0.009871831515081413\n",
      "Epoch : 382\n",
      "Val Loss : 0.008895293682813644\n",
      "Epoch : 383\n",
      "Train Loss : 0.009871037141668522\n",
      "Epoch : 383\n",
      "Val Loss : 0.008896549478173256\n",
      "Epoch : 384\n",
      "Train Loss : 0.009870248482135056\n",
      "Epoch : 384\n",
      "Val Loss : 0.008897804096341133\n",
      "Epoch : 385\n",
      "Train Loss : 0.009869461960334901\n",
      "Epoch : 385\n",
      "Val Loss : 0.008899050980806351\n",
      "Epoch : 386\n",
      "Train Loss : 0.009868672942855393\n",
      "Epoch : 386\n",
      "Val Loss : 0.008900286510586738\n",
      "Epoch : 387\n",
      "Train Loss : 0.009867885779735245\n",
      "Epoch : 387\n",
      "Val Loss : 0.008901533767580986\n",
      "Epoch : 388\n",
      "Train Loss : 0.00986710109240856\n",
      "Epoch : 388\n",
      "Val Loss : 0.008902773454785348\n",
      "Epoch : 389\n",
      "Train Loss : 0.009866314805924786\n",
      "Epoch : 389\n",
      "Val Loss : 0.008903998300433159\n",
      "Epoch : 390\n",
      "Train Loss : 0.00986553331194081\n",
      "Epoch : 390\n",
      "Val Loss : 0.008905250281095504\n",
      "Epoch : 391\n",
      "Train Loss : 0.009864754315293345\n",
      "Epoch : 391\n",
      "Val Loss : 0.00890648365020752\n",
      "Epoch : 392\n",
      "Train Loss : 0.009863972160103483\n",
      "Epoch : 392\n",
      "Val Loss : 0.008907709315419197\n",
      "Epoch : 393\n",
      "Train Loss : 0.009863191408526113\n",
      "Epoch : 393\n",
      "Val Loss : 0.00890894065797329\n",
      "Epoch : 394\n",
      "Train Loss : 0.009862415782537218\n",
      "Epoch : 394\n",
      "Val Loss : 0.008910164132714272\n",
      "Epoch : 395\n",
      "Train Loss : 0.009861641982736006\n",
      "Epoch : 395\n",
      "Val Loss : 0.008911393597722054\n",
      "Epoch : 396\n",
      "Train Loss : 0.00986086337552058\n",
      "Epoch : 396\n",
      "Val Loss : 0.00891260975599289\n",
      "Epoch : 397\n",
      "Train Loss : 0.009860088180392662\n",
      "Epoch : 397\n",
      "Val Loss : 0.00891383494436741\n",
      "Epoch : 398\n",
      "Train Loss : 0.009859314350762613\n",
      "Epoch : 398\n",
      "Val Loss : 0.008915045827627182\n",
      "Epoch : 399\n",
      "Train Loss : 0.009858543631617391\n",
      "Epoch : 399\n",
      "Val Loss : 0.008916263327002525\n",
      "Epoch : 400\n",
      "Train Loss : 0.00985777508500579\n",
      "Epoch : 400\n",
      "Val Loss : 0.008917472079396247\n",
      "Epoch : 401\n",
      "Train Loss : 0.009857005926903032\n",
      "Epoch : 401\n",
      "Val Loss : 0.008918681561946869\n",
      "Epoch : 402\n",
      "Train Loss : 0.009856238188984344\n",
      "Epoch : 402\n",
      "Val Loss : 0.008919872924685478\n",
      "Epoch : 403\n",
      "Train Loss : 0.009855469798145557\n",
      "Epoch : 403\n",
      "Val Loss : 0.008921083599328994\n",
      "Epoch : 404\n",
      "Train Loss : 0.00985470068147174\n",
      "Epoch : 404\n",
      "Val Loss : 0.008922263070940972\n",
      "Epoch : 405\n",
      "Train Loss : 0.009853937021817919\n",
      "Epoch : 405\n",
      "Val Loss : 0.00892347428202629\n",
      "Epoch : 406\n",
      "Train Loss : 0.00985317614453172\n",
      "Epoch : 406\n",
      "Val Loss : 0.008924667194485664\n",
      "Epoch : 407\n",
      "Train Loss : 0.009852412738423012\n",
      "Epoch : 407\n",
      "Val Loss : 0.008925867557525636\n",
      "Epoch : 408\n",
      "Train Loss : 0.009851651160159146\n",
      "Epoch : 408\n",
      "Val Loss : 0.008927057147026062\n",
      "Epoch : 409\n",
      "Train Loss : 0.009850887947937878\n",
      "Epoch : 409\n",
      "Val Loss : 0.008928226098418236\n",
      "Epoch : 410\n",
      "Train Loss : 0.009850128078203502\n",
      "Epoch : 410\n",
      "Val Loss : 0.008929423660039902\n",
      "Epoch : 411\n",
      "Train Loss : 0.009849374023435166\n",
      "Epoch : 411\n",
      "Val Loss : 0.00893059928715229\n",
      "Epoch : 412\n",
      "Train Loss : 0.009848618313166382\n",
      "Epoch : 412\n",
      "Val Loss : 0.008931780323386193\n",
      "Epoch : 413\n",
      "Train Loss : 0.009847859828815766\n",
      "Epoch : 413\n",
      "Val Loss : 0.008932954773306847\n",
      "Epoch : 414\n",
      "Train Loss : 0.009847104809581703\n",
      "Epoch : 414\n",
      "Val Loss : 0.008934120491147042\n",
      "Epoch : 415\n",
      "Train Loss : 0.009846349427430125\n",
      "Epoch : 415\n",
      "Val Loss : 0.008935281977057457\n",
      "Epoch : 416\n",
      "Train Loss : 0.009845594539111513\n",
      "Epoch : 416\n",
      "Val Loss : 0.008936451733112334\n",
      "Epoch : 417\n",
      "Train Loss : 0.009844847777493802\n",
      "Epoch : 417\n",
      "Val Loss : 0.008937620893120765\n",
      "Epoch : 418\n",
      "Train Loss : 0.009844093016776325\n",
      "Epoch : 418\n",
      "Val Loss : 0.008938785776495933\n",
      "Epoch : 419\n",
      "Train Loss : 0.009843347153338037\n",
      "Epoch : 419\n",
      "Val Loss : 0.008939941972494125\n",
      "Epoch : 420\n",
      "Train Loss : 0.009842598260615645\n",
      "Epoch : 420\n",
      "Val Loss : 0.00894110830128193\n",
      "Epoch : 421\n",
      "Train Loss : 0.00984184819793998\n",
      "Epoch : 421\n",
      "Val Loss : 0.008942258059978486\n",
      "Epoch : 422\n",
      "Train Loss : 0.009841103733142712\n",
      "Epoch : 422\n",
      "Val Loss : 0.008943413391709327\n",
      "Epoch : 423\n",
      "Train Loss : 0.009840357291356419\n",
      "Epoch : 423\n",
      "Val Loss : 0.008944560244679451\n",
      "Epoch : 424\n",
      "Train Loss : 0.009839612657529074\n",
      "Epoch : 424\n",
      "Val Loss : 0.008945698648691177\n",
      "Epoch : 425\n",
      "Train Loss : 0.009838868620278466\n",
      "Epoch : 425\n",
      "Val Loss : 0.008946833729743957\n",
      "Epoch : 426\n",
      "Train Loss : 0.00983812823706038\n",
      "Epoch : 426\n",
      "Val Loss : 0.008947982907295227\n",
      "Epoch : 427\n",
      "Train Loss : 0.009837387940014491\n",
      "Epoch : 427\n",
      "Val Loss : 0.008949117481708527\n",
      "Epoch : 428\n",
      "Train Loss : 0.009836646504501325\n",
      "Epoch : 428\n",
      "Val Loss : 0.00895026434957981\n",
      "Epoch : 429\n",
      "Train Loss : 0.009835904711042117\n",
      "Epoch : 429\n",
      "Val Loss : 0.00895137770473957\n",
      "Epoch : 430\n",
      "Train Loss : 0.009835165559092132\n",
      "Epoch : 430\n",
      "Val Loss : 0.008952510878443717\n",
      "Epoch : 431\n",
      "Train Loss : 0.00983443139021507\n",
      "Epoch : 431\n",
      "Val Loss : 0.00895362651348114\n",
      "Epoch : 432\n",
      "Train Loss : 0.009833694367712609\n",
      "Epoch : 432\n",
      "Val Loss : 0.008954748928546906\n",
      "Epoch : 433\n",
      "Train Loss : 0.009832960339693943\n",
      "Epoch : 433\n",
      "Val Loss : 0.008955877780914306\n",
      "Epoch : 434\n",
      "Train Loss : 0.00983222706402483\n",
      "Epoch : 434\n",
      "Val Loss : 0.008956980854272843\n",
      "Epoch : 435\n",
      "Train Loss : 0.009831491001016622\n",
      "Epoch : 435\n",
      "Val Loss : 0.00895810402929783\n",
      "Epoch : 436\n",
      "Train Loss : 0.009830761367779928\n",
      "Epoch : 436\n",
      "Val Loss : 0.00895921479165554\n",
      "Epoch : 437\n",
      "Train Loss : 0.009830029013490444\n",
      "Epoch : 437\n",
      "Val Loss : 0.008960312694311141\n",
      "Epoch : 438\n",
      "Train Loss : 0.009829300333119372\n",
      "Epoch : 438\n",
      "Val Loss : 0.008961428716778755\n",
      "Epoch : 439\n",
      "Train Loss : 0.009828571902979099\n",
      "Epoch : 439\n",
      "Val Loss : 0.00896254923939705\n",
      "Epoch : 440\n",
      "Train Loss : 0.009827844629534834\n",
      "Epoch : 440\n",
      "Val Loss : 0.008963650599122047\n",
      "Epoch : 441\n",
      "Train Loss : 0.009827113710343838\n",
      "Epoch : 441\n",
      "Val Loss : 0.00896474313735962\n",
      "Epoch : 442\n",
      "Train Loss : 0.009826386241354977\n",
      "Epoch : 442\n",
      "Val Loss : 0.008965837091207504\n",
      "Epoch : 443\n",
      "Train Loss : 0.009825664504474274\n",
      "Epoch : 443\n",
      "Val Loss : 0.008966923862695694\n",
      "Epoch : 444\n",
      "Train Loss : 0.009824935862217828\n",
      "Epoch : 444\n",
      "Val Loss : 0.00896799249947071\n",
      "Epoch : 445\n",
      "Train Loss : 0.00982420906934927\n",
      "Epoch : 445\n",
      "Val Loss : 0.008969087734818458\n",
      "Epoch : 446\n",
      "Train Loss : 0.009823491138959589\n",
      "Epoch : 446\n",
      "Val Loss : 0.008970184952020645\n",
      "Epoch : 447\n",
      "Train Loss : 0.00982276860332892\n",
      "Epoch : 447\n",
      "Val Loss : 0.008971259370446205\n",
      "Epoch : 448\n",
      "Train Loss : 0.009822047496168109\n",
      "Epoch : 448\n",
      "Val Loss : 0.008972331389784812\n",
      "Epoch : 449\n",
      "Train Loss : 0.009821323588410943\n",
      "Epoch : 449\n",
      "Val Loss : 0.00897339877486229\n",
      "Epoch : 450\n",
      "Train Loss : 0.009820609204338538\n",
      "Epoch : 450\n",
      "Val Loss : 0.008974495857954025\n",
      "Epoch : 451\n",
      "Train Loss : 0.009819893255909354\n",
      "Epoch : 451\n",
      "Val Loss : 0.00897555473446846\n",
      "Epoch : 452\n",
      "Train Loss : 0.009819171923375109\n",
      "Epoch : 452\n",
      "Val Loss : 0.008976627990603447\n",
      "Epoch : 453\n",
      "Train Loss : 0.009818457227757075\n",
      "Epoch : 453\n",
      "Val Loss : 0.008977691903710365\n",
      "Epoch : 454\n",
      "Train Loss : 0.009817739579084188\n",
      "Epoch : 454\n",
      "Val Loss : 0.008978741645812988\n",
      "Epoch : 455\n",
      "Train Loss : 0.00981702631856464\n",
      "Epoch : 455\n",
      "Val Loss : 0.008979809686541558\n",
      "Epoch : 456\n",
      "Train Loss : 0.009816312943701219\n",
      "Epoch : 456\n",
      "Val Loss : 0.008980865851044654\n",
      "Epoch : 457\n",
      "Train Loss : 0.009815597485790686\n",
      "Epoch : 457\n",
      "Val Loss : 0.008981906965374946\n",
      "Epoch : 458\n",
      "Train Loss : 0.009814885932143472\n",
      "Epoch : 458\n",
      "Val Loss : 0.00898295161128044\n",
      "Epoch : 459\n",
      "Train Loss : 0.009814179994603387\n",
      "Epoch : 459\n",
      "Val Loss : 0.008984006971120834\n",
      "Epoch : 460\n",
      "Train Loss : 0.009813466472252938\n",
      "Epoch : 460\n",
      "Val Loss : 0.008985047146677971\n",
      "Epoch : 461\n",
      "Train Loss : 0.009812755314666393\n",
      "Epoch : 461\n",
      "Val Loss : 0.008986081764101981\n",
      "Epoch : 462\n",
      "Train Loss : 0.009812047726597332\n",
      "Epoch : 462\n",
      "Val Loss : 0.00898712196946144\n",
      "Epoch : 463\n",
      "Train Loss : 0.009811342352490832\n",
      "Epoch : 463\n",
      "Val Loss : 0.008988158360123634\n",
      "Epoch : 464\n",
      "Train Loss : 0.009810633998814957\n",
      "Epoch : 464\n",
      "Val Loss : 0.008989186868071555\n",
      "Epoch : 465\n",
      "Train Loss : 0.009809929617345864\n",
      "Epoch : 465\n",
      "Val Loss : 0.008990217387676238\n",
      "Epoch : 466\n",
      "Train Loss : 0.0098092234560895\n",
      "Epoch : 466\n",
      "Val Loss : 0.008991238847374915\n",
      "Epoch : 467\n",
      "Train Loss : 0.009808521611728703\n",
      "Epoch : 467\n",
      "Val Loss : 0.008992276594042777\n",
      "Epoch : 468\n",
      "Train Loss : 0.009807817314774647\n",
      "Epoch : 468\n",
      "Val Loss : 0.008993307158350944\n",
      "Epoch : 469\n",
      "Train Loss : 0.009807112764275478\n",
      "Epoch : 469\n",
      "Val Loss : 0.008994320496916772\n",
      "Epoch : 470\n",
      "Train Loss : 0.009806412524043242\n",
      "Epoch : 470\n",
      "Val Loss : 0.008995334759354592\n",
      "Epoch : 471\n",
      "Train Loss : 0.009805710308479139\n",
      "Epoch : 471\n",
      "Val Loss : 0.008996347770094872\n",
      "Epoch : 472\n",
      "Train Loss : 0.00980500976498706\n",
      "Epoch : 472\n",
      "Val Loss : 0.00899736326932907\n",
      "Epoch : 473\n",
      "Train Loss : 0.009804310437848665\n",
      "Epoch : 473\n",
      "Val Loss : 0.008998364686965942\n",
      "Epoch : 474\n",
      "Train Loss : 0.009803615934696062\n",
      "Epoch : 474\n",
      "Val Loss : 0.008999395996332168\n",
      "Epoch : 475\n",
      "Train Loss : 0.009802919403182548\n",
      "Epoch : 475\n",
      "Val Loss : 0.009000398606061935\n",
      "Epoch : 476\n",
      "Train Loss : 0.009802215694519443\n",
      "Epoch : 476\n",
      "Val Loss : 0.009001381576061248\n",
      "Epoch : 477\n",
      "Train Loss : 0.009801524669740654\n",
      "Epoch : 477\n",
      "Val Loss : 0.009002376735210418\n",
      "Epoch : 478\n",
      "Train Loss : 0.00980082786148182\n",
      "Epoch : 478\n",
      "Val Loss : 0.009003380864858627\n",
      "Epoch : 479\n",
      "Train Loss : 0.009800132138661218\n",
      "Epoch : 479\n",
      "Val Loss : 0.009004354760050773\n",
      "Epoch : 480\n",
      "Train Loss : 0.009799439546211335\n",
      "Epoch : 480\n",
      "Val Loss : 0.00900535710155964\n",
      "Epoch : 481\n",
      "Train Loss : 0.009798751177856188\n",
      "Epoch : 481\n",
      "Val Loss : 0.00900635552406311\n",
      "Epoch : 482\n",
      "Train Loss : 0.009798055938925607\n",
      "Epoch : 482\n",
      "Val Loss : 0.00900731885433197\n",
      "Epoch : 483\n",
      "Train Loss : 0.009797362577554595\n",
      "Epoch : 483\n",
      "Val Loss : 0.00900829853117466\n",
      "Epoch : 484\n",
      "Train Loss : 0.009796671927293424\n",
      "Epoch : 484\n",
      "Val Loss : 0.00900929556787014\n",
      "Epoch : 485\n",
      "Train Loss : 0.009795987159941757\n",
      "Epoch : 485\n",
      "Val Loss : 0.00901025828719139\n",
      "Epoch : 486\n",
      "Train Loss : 0.009795296655510457\n",
      "Epoch : 486\n",
      "Val Loss : 0.009011250331997872\n",
      "Epoch : 487\n",
      "Train Loss : 0.009794611435754762\n",
      "Epoch : 487\n",
      "Val Loss : 0.009012219876050949\n",
      "Epoch : 488\n",
      "Train Loss : 0.009793923011056256\n",
      "Epoch : 488\n",
      "Val Loss : 0.009013181880116463\n",
      "Epoch : 489\n",
      "Train Loss : 0.009793235133219762\n",
      "Epoch : 489\n",
      "Val Loss : 0.009014157921075822\n",
      "Epoch : 490\n",
      "Train Loss : 0.009792550861358218\n",
      "Epoch : 490\n",
      "Val Loss : 0.009015101805329323\n",
      "Epoch : 491\n",
      "Train Loss : 0.009791864107074581\n",
      "Epoch : 491\n",
      "Val Loss : 0.009016066417098045\n",
      "Epoch : 492\n",
      "Train Loss : 0.009791180897451061\n",
      "Epoch : 492\n",
      "Val Loss : 0.009017029479146003\n",
      "Epoch : 493\n",
      "Train Loss : 0.009790498191603457\n",
      "Epoch : 493\n",
      "Val Loss : 0.009017981097102164\n",
      "Epoch : 494\n",
      "Train Loss : 0.0097898160193606\n",
      "Epoch : 494\n",
      "Val Loss : 0.009018933072686195\n",
      "Epoch : 495\n",
      "Train Loss : 0.00978913216675993\n",
      "Epoch : 495\n",
      "Val Loss : 0.009019883617758752\n",
      "Epoch : 496\n",
      "Train Loss : 0.009788456283430188\n",
      "Epoch : 496\n",
      "Val Loss : 0.009020836591720581\n",
      "Epoch : 497\n",
      "Train Loss : 0.009787776525665964\n",
      "Epoch : 497\n",
      "Val Loss : 0.00902179491519928\n",
      "Epoch : 498\n",
      "Train Loss : 0.009787094739540828\n",
      "Epoch : 498\n",
      "Val Loss : 0.009022730827331543\n",
      "Epoch : 499\n",
      "Train Loss : 0.009786416150072716\n",
      "Epoch : 499\n",
      "Val Loss : 0.009023659035563469\n",
      "Epoch : 500\n",
      "Train Loss : 0.009785739951883029\n",
      "Epoch : 500\n",
      "Val Loss : 0.009024622440338135\n",
      "Epoch : 501\n",
      "Train Loss : 0.009785062346766535\n",
      "Epoch : 501\n",
      "Val Loss : 0.009025550425052643\n",
      "Epoch : 502\n",
      "Train Loss : 0.009784382819347216\n",
      "Epoch : 502\n",
      "Val Loss : 0.009026463270187378\n",
      "Epoch : 503\n",
      "Train Loss : 0.009783704751883749\n",
      "Epoch : 503\n",
      "Val Loss : 0.009027401700615882\n",
      "Epoch : 504\n",
      "Train Loss : 0.009783032332013403\n",
      "Epoch : 504\n",
      "Val Loss : 0.00902832804620266\n",
      "Epoch : 505\n",
      "Train Loss : 0.009782359063678366\n",
      "Epoch : 505\n",
      "Val Loss : 0.009029256999492646\n",
      "Epoch : 506\n",
      "Train Loss : 0.00978168808387798\n",
      "Epoch : 506\n",
      "Val Loss : 0.009030172988772392\n",
      "Epoch : 507\n",
      "Train Loss : 0.009781013418559078\n",
      "Epoch : 507\n",
      "Val Loss : 0.009031091690063477\n",
      "Epoch : 508\n",
      "Train Loss : 0.009780341780867122\n",
      "Epoch : 508\n",
      "Val Loss : 0.009032006233930587\n",
      "Epoch : 509\n",
      "Train Loss : 0.00977966949854086\n",
      "Epoch : 509\n",
      "Val Loss : 0.009032919511198997\n",
      "Epoch : 510\n",
      "Train Loss : 0.009779001700483077\n",
      "Epoch : 510\n",
      "Val Loss : 0.009033849954605103\n",
      "Epoch : 511\n",
      "Train Loss : 0.009778331550918652\n",
      "Epoch : 511\n",
      "Val Loss : 0.00903475683927536\n",
      "Epoch : 512\n",
      "Train Loss : 0.009777661699642596\n",
      "Epoch : 512\n",
      "Val Loss : 0.009035665854811668\n",
      "Epoch : 513\n",
      "Train Loss : 0.009776994178330134\n",
      "Epoch : 513\n",
      "Val Loss : 0.009036559537053108\n",
      "Epoch : 514\n",
      "Train Loss : 0.009776329318412788\n",
      "Epoch : 514\n",
      "Val Loss : 0.00903747221827507\n",
      "Epoch : 515\n",
      "Train Loss : 0.009775655915847982\n",
      "Epoch : 515\n",
      "Val Loss : 0.00903837163746357\n",
      "Epoch : 516\n",
      "Train Loss : 0.009774997924848913\n",
      "Epoch : 516\n",
      "Val Loss : 0.00903926196694374\n",
      "Epoch : 517\n",
      "Train Loss : 0.009774328429861744\n",
      "Epoch : 517\n",
      "Val Loss : 0.009040165796875953\n",
      "Epoch : 518\n",
      "Train Loss : 0.00977366082734856\n",
      "Epoch : 518\n",
      "Val Loss : 0.009041046530008316\n",
      "Epoch : 519\n",
      "Train Loss : 0.009772998058764111\n",
      "Epoch : 519\n",
      "Val Loss : 0.009041958525776862\n",
      "Epoch : 520\n",
      "Train Loss : 0.009772335344865865\n",
      "Epoch : 520\n",
      "Val Loss : 0.009042842075228692\n",
      "Epoch : 521\n",
      "Train Loss : 0.009771670529691774\n",
      "Epoch : 521\n",
      "Val Loss : 0.009043731465935708\n",
      "Epoch : 522\n",
      "Train Loss : 0.009771011140051685\n",
      "Epoch : 522\n",
      "Val Loss : 0.009044609278440476\n",
      "Epoch : 523\n",
      "Train Loss : 0.009770350174454713\n",
      "Epoch : 523\n",
      "Val Loss : 0.00904548628628254\n",
      "Epoch : 524\n",
      "Train Loss : 0.009769688420050722\n",
      "Epoch : 524\n",
      "Val Loss : 0.009046379685401916\n",
      "Epoch : 525\n",
      "Train Loss : 0.009769035153607986\n",
      "Epoch : 525\n",
      "Val Loss : 0.009047268569469451\n",
      "Epoch : 526\n",
      "Train Loss : 0.009768369919173021\n",
      "Epoch : 526\n",
      "Val Loss : 0.009048108667135239\n",
      "Epoch : 527\n",
      "Train Loss : 0.009767707947681383\n",
      "Epoch : 527\n",
      "Val Loss : 0.0090489941239357\n",
      "Epoch : 528\n",
      "Train Loss : 0.009767055373930527\n",
      "Epoch : 528\n",
      "Val Loss : 0.009049881055951118\n",
      "Epoch : 529\n",
      "Train Loss : 0.009766397407788823\n",
      "Epoch : 529\n",
      "Val Loss : 0.00905073843896389\n",
      "Epoch : 530\n",
      "Train Loss : 0.009765743446997053\n",
      "Epoch : 530\n",
      "Val Loss : 0.009051599651575088\n",
      "Epoch : 531\n",
      "Train Loss : 0.009765087641788866\n",
      "Epoch : 531\n",
      "Val Loss : 0.009052479222416877\n",
      "Epoch : 532\n",
      "Train Loss : 0.009764430287138318\n",
      "Epoch : 532\n",
      "Val Loss : 0.009053333267569541\n",
      "Epoch : 533\n",
      "Train Loss : 0.009763777436642142\n",
      "Epoch : 533\n",
      "Val Loss : 0.009054195582866669\n",
      "Epoch : 534\n",
      "Train Loss : 0.009763122279382579\n",
      "Epoch : 534\n",
      "Val Loss : 0.009055044010281562\n",
      "Epoch : 535\n",
      "Train Loss : 0.009762470956785717\n",
      "Epoch : 535\n",
      "Val Loss : 0.00905589261651039\n",
      "Epoch : 536\n",
      "Train Loss : 0.009761815759754372\n",
      "Epoch : 536\n",
      "Val Loss : 0.009056751310825348\n",
      "Epoch : 537\n",
      "Train Loss : 0.009761168381192613\n",
      "Epoch : 537\n",
      "Val Loss : 0.009057596325874329\n",
      "Epoch : 538\n",
      "Train Loss : 0.009760517724773108\n",
      "Epoch : 538\n",
      "Val Loss : 0.009058464139699936\n",
      "Epoch : 539\n",
      "Train Loss : 0.009759866195031544\n",
      "Epoch : 539\n",
      "Val Loss : 0.009059297189116479\n",
      "Epoch : 540\n",
      "Train Loss : 0.009759217429428868\n",
      "Epoch : 540\n",
      "Val Loss : 0.009060140073299408\n",
      "Epoch : 541\n",
      "Train Loss : 0.009758565013107985\n",
      "Epoch : 541\n",
      "Val Loss : 0.009060982525348663\n",
      "Epoch : 542\n",
      "Train Loss : 0.00975791995788119\n",
      "Epoch : 542\n",
      "Val Loss : 0.009061829313635826\n",
      "Epoch : 543\n",
      "Train Loss : 0.009757272716863512\n",
      "Epoch : 543\n",
      "Val Loss : 0.009062657445669173\n",
      "Epoch : 544\n",
      "Train Loss : 0.009756626655742048\n",
      "Epoch : 544\n",
      "Val Loss : 0.00906350041925907\n",
      "Epoch : 545\n",
      "Train Loss : 0.009755976395383212\n",
      "Epoch : 545\n",
      "Val Loss : 0.009064306795597077\n",
      "Epoch : 546\n",
      "Train Loss : 0.009755332258221729\n",
      "Epoch : 546\n",
      "Val Loss : 0.009065161004662514\n",
      "Epoch : 547\n",
      "Train Loss : 0.009754687459854363\n",
      "Epoch : 547\n",
      "Val Loss : 0.009065979182720185\n",
      "Epoch : 548\n",
      "Train Loss : 0.009754046519349902\n",
      "Epoch : 548\n",
      "Val Loss : 0.009066825479269028\n",
      "Epoch : 549\n",
      "Train Loss : 0.009753400365427614\n",
      "Epoch : 549\n",
      "Val Loss : 0.009067639753222466\n",
      "Epoch : 550\n",
      "Train Loss : 0.009752757409819504\n",
      "Epoch : 550\n",
      "Val Loss : 0.009068457514047622\n",
      "Epoch : 551\n",
      "Train Loss : 0.00975211391894949\n",
      "Epoch : 551\n",
      "Val Loss : 0.009069283336400986\n",
      "Epoch : 552\n",
      "Train Loss : 0.009751474186512923\n",
      "Epoch : 552\n",
      "Val Loss : 0.009070099472999573\n",
      "Epoch : 553\n",
      "Train Loss : 0.009750833758070163\n",
      "Epoch : 553\n",
      "Val Loss : 0.009070931866765022\n",
      "Epoch : 554\n",
      "Train Loss : 0.009750190462744746\n",
      "Epoch : 554\n",
      "Val Loss : 0.009071717858314515\n",
      "Epoch : 555\n",
      "Train Loss : 0.009749550503277587\n",
      "Epoch : 555\n",
      "Val Loss : 0.009072542533278466\n",
      "Epoch : 556\n",
      "Train Loss : 0.009748916033973579\n",
      "Epoch : 556\n",
      "Val Loss : 0.009073369920253753\n",
      "Epoch : 557\n",
      "Train Loss : 0.009748277164916127\n",
      "Epoch : 557\n",
      "Val Loss : 0.009074176326394082\n",
      "Epoch : 558\n",
      "Train Loss : 0.009747636376870166\n",
      "Epoch : 558\n",
      "Val Loss : 0.009074960514903068\n",
      "Epoch : 559\n",
      "Train Loss : 0.009747000515553769\n",
      "Epoch : 559\n",
      "Val Loss : 0.009075782626867295\n",
      "Epoch : 560\n",
      "Train Loss : 0.009746364137204198\n",
      "Epoch : 560\n",
      "Val Loss : 0.009076577082276344\n",
      "Epoch : 561\n",
      "Train Loss : 0.009745730045732124\n",
      "Epoch : 561\n",
      "Val Loss : 0.009077381491661072\n",
      "Epoch : 562\n",
      "Train Loss : 0.009745095652657366\n",
      "Epoch : 562\n",
      "Val Loss : 0.009078170076012611\n",
      "Epoch : 563\n",
      "Train Loss : 0.009744460290145408\n",
      "Epoch : 563\n",
      "Val Loss : 0.009078978031873703\n",
      "Epoch : 564\n",
      "Train Loss : 0.009743824768546318\n",
      "Epoch : 564\n",
      "Val Loss : 0.009079774364829063\n",
      "Epoch : 565\n",
      "Train Loss : 0.009743189686093997\n",
      "Epoch : 565\n",
      "Val Loss : 0.009080561265349387\n",
      "Epoch : 566\n",
      "Train Loss : 0.00974255817813063\n",
      "Epoch : 566\n",
      "Val Loss : 0.009081353858113289\n",
      "Epoch : 567\n",
      "Train Loss : 0.009741932478504673\n",
      "Epoch : 567\n",
      "Val Loss : 0.009082155182957649\n",
      "Epoch : 568\n",
      "Train Loss : 0.009741296125012466\n",
      "Epoch : 568\n",
      "Val Loss : 0.009082932025194168\n",
      "Epoch : 569\n",
      "Train Loss : 0.009740665258369094\n",
      "Epoch : 569\n",
      "Val Loss : 0.009083705052733421\n",
      "Epoch : 570\n",
      "Train Loss : 0.009740034235952906\n",
      "Epoch : 570\n",
      "Val Loss : 0.009084501221776009\n",
      "Epoch : 571\n",
      "Train Loss : 0.009739407445917244\n",
      "Epoch : 571\n",
      "Val Loss : 0.009085284292697907\n",
      "Epoch : 572\n",
      "Train Loss : 0.009738779900217714\n",
      "Epoch : 572\n",
      "Val Loss : 0.009086079746484756\n",
      "Epoch : 573\n",
      "Train Loss : 0.009738151535882327\n",
      "Epoch : 573\n",
      "Val Loss : 0.009086835697293282\n",
      "Epoch : 574\n",
      "Train Loss : 0.009737520365979111\n",
      "Epoch : 574\n",
      "Val Loss : 0.009087620913982391\n",
      "Epoch : 575\n",
      "Train Loss : 0.009736895978821979\n",
      "Epoch : 575\n",
      "Val Loss : 0.009088383868336677\n",
      "Epoch : 576\n",
      "Train Loss : 0.009736268380093404\n",
      "Epoch : 576\n",
      "Val Loss : 0.009089158073067665\n",
      "Epoch : 577\n",
      "Train Loss : 0.009735643820591881\n",
      "Epoch : 577\n",
      "Val Loss : 0.009089921340346337\n",
      "Epoch : 578\n",
      "Train Loss : 0.009735017328844587\n",
      "Epoch : 578\n",
      "Val Loss : 0.009090675964951516\n",
      "Epoch : 579\n",
      "Train Loss : 0.00973439590799957\n",
      "Epoch : 579\n",
      "Val Loss : 0.009091465011239051\n",
      "Epoch : 580\n",
      "Train Loss : 0.009733771908617317\n",
      "Epoch : 580\n",
      "Val Loss : 0.009092221975326538\n",
      "Epoch : 581\n",
      "Train Loss : 0.009733150684974054\n",
      "Epoch : 581\n",
      "Val Loss : 0.009092988580465316\n",
      "Epoch : 582\n",
      "Train Loss : 0.009732527045195\n",
      "Epoch : 582\n",
      "Val Loss : 0.009093773409724235\n",
      "Epoch : 583\n",
      "Train Loss : 0.009731903110441894\n",
      "Epoch : 583\n",
      "Val Loss : 0.009094514280557632\n",
      "Epoch : 584\n",
      "Train Loss : 0.009731283497555825\n",
      "Epoch : 584\n",
      "Val Loss : 0.009095262512564659\n",
      "Epoch : 585\n",
      "Train Loss : 0.00973066107081614\n",
      "Epoch : 585\n",
      "Val Loss : 0.009096032917499542\n",
      "Epoch : 586\n",
      "Train Loss : 0.009730043217831446\n",
      "Epoch : 586\n",
      "Val Loss : 0.009096769720315933\n",
      "Epoch : 587\n",
      "Train Loss : 0.009729421626299195\n",
      "Epoch : 587\n",
      "Val Loss : 0.009097533598542214\n",
      "Epoch : 588\n",
      "Train Loss : 0.009728801174891376\n",
      "Epoch : 588\n",
      "Val Loss : 0.009098279848694801\n",
      "Epoch : 589\n",
      "Train Loss : 0.009728185032093334\n",
      "Epoch : 589\n",
      "Val Loss : 0.00909903198480606\n",
      "Epoch : 590\n",
      "Train Loss : 0.009727567570197836\n",
      "Epoch : 590\n",
      "Val Loss : 0.009099779471755028\n",
      "Epoch : 591\n",
      "Train Loss : 0.009726948885320027\n",
      "Epoch : 591\n",
      "Val Loss : 0.009100518018007278\n",
      "Epoch : 592\n",
      "Train Loss : 0.009726332145945244\n",
      "Epoch : 592\n",
      "Val Loss : 0.00910125696659088\n",
      "Epoch : 593\n",
      "Train Loss : 0.009725716178805908\n",
      "Epoch : 593\n",
      "Val Loss : 0.009101997122168541\n",
      "Epoch : 594\n",
      "Train Loss : 0.009725103824270153\n",
      "Epoch : 594\n",
      "Val Loss : 0.009102744981646538\n",
      "Epoch : 595\n",
      "Train Loss : 0.00972449056161203\n",
      "Epoch : 595\n",
      "Val Loss : 0.009103474706411362\n",
      "Epoch : 596\n",
      "Train Loss : 0.009723874712130884\n",
      "Epoch : 596\n",
      "Val Loss : 0.0091042340695858\n",
      "Epoch : 597\n",
      "Train Loss : 0.009723262375823861\n",
      "Epoch : 597\n",
      "Val Loss : 0.009104961156845092\n",
      "Epoch : 598\n",
      "Train Loss : 0.009722650148889242\n",
      "Epoch : 598\n",
      "Val Loss : 0.009105688840150833\n",
      "Epoch : 599\n",
      "Train Loss : 0.009722037477836384\n",
      "Epoch : 599\n",
      "Val Loss : 0.009106421932578088\n",
      "Epoch : 600\n",
      "Train Loss : 0.009721423012081839\n",
      "Epoch : 600\n",
      "Val Loss : 0.009107136115431786\n",
      "Epoch : 601\n",
      "Train Loss : 0.009720811295551761\n",
      "Epoch : 601\n",
      "Val Loss : 0.00910787484049797\n",
      "Epoch : 602\n",
      "Train Loss : 0.0097202036672295\n",
      "Epoch : 602\n",
      "Val Loss : 0.009108596652746201\n",
      "Epoch : 603\n",
      "Train Loss : 0.009719590550401246\n",
      "Epoch : 603\n",
      "Val Loss : 0.009109313502907753\n",
      "Epoch : 604\n",
      "Train Loss : 0.009718983031451384\n",
      "Epoch : 604\n",
      "Val Loss : 0.00911005775630474\n",
      "Epoch : 605\n",
      "Train Loss : 0.009718371639724197\n",
      "Epoch : 605\n",
      "Val Loss : 0.009110775917768479\n",
      "Epoch : 606\n",
      "Train Loss : 0.009717769433621622\n",
      "Epoch : 606\n",
      "Val Loss : 0.009111514285206795\n",
      "Epoch : 607\n",
      "Train Loss : 0.009717159089218042\n",
      "Epoch : 607\n",
      "Val Loss : 0.00911222292482853\n",
      "Epoch : 608\n",
      "Train Loss : 0.009716552113815877\n",
      "Epoch : 608\n",
      "Val Loss : 0.00911294175684452\n",
      "Epoch : 609\n",
      "Train Loss : 0.009715943593942821\n",
      "Epoch : 609\n",
      "Val Loss : 0.009113653749227523\n",
      "Epoch : 610\n",
      "Train Loss : 0.00971533643625332\n",
      "Epoch : 610\n",
      "Val Loss : 0.009114343583583831\n",
      "Epoch : 611\n",
      "Train Loss : 0.009714737106976347\n",
      "Epoch : 611\n",
      "Val Loss : 0.009115065649151802\n",
      "Epoch : 612\n",
      "Train Loss : 0.009714130514377589\n",
      "Epoch : 612\n",
      "Val Loss : 0.009115786641836167\n",
      "Epoch : 613\n",
      "Train Loss : 0.009713526372714928\n",
      "Epoch : 613\n",
      "Val Loss : 0.009116494134068488\n",
      "Epoch : 614\n",
      "Train Loss : 0.00971292308945991\n",
      "Epoch : 614\n",
      "Val Loss : 0.009117193028330802\n",
      "Epoch : 615\n",
      "Train Loss : 0.009712325947630978\n",
      "Epoch : 615\n",
      "Val Loss : 0.009117923736572265\n",
      "Epoch : 616\n",
      "Train Loss : 0.009711718977200285\n",
      "Epoch : 616\n",
      "Val Loss : 0.009118611350655555\n",
      "Epoch : 617\n",
      "Train Loss : 0.009711115190169354\n",
      "Epoch : 617\n",
      "Val Loss : 0.009119327455759049\n",
      "Epoch : 618\n",
      "Train Loss : 0.009710514011504493\n",
      "Epoch : 618\n",
      "Val Loss : 0.009120008498430253\n",
      "Epoch : 619\n",
      "Train Loss : 0.009709915081602506\n",
      "Epoch : 619\n",
      "Val Loss : 0.009120732456445693\n",
      "Epoch : 620\n",
      "Train Loss : 0.009709315937927185\n",
      "Epoch : 620\n",
      "Val Loss : 0.009121418461203575\n",
      "Epoch : 621\n",
      "Train Loss : 0.009708715437039786\n",
      "Epoch : 621\n",
      "Val Loss : 0.009122118070721627\n",
      "Epoch : 622\n",
      "Train Loss : 0.009708115898960956\n",
      "Epoch : 622\n",
      "Val Loss : 0.009122809812426568\n",
      "Epoch : 623\n",
      "Train Loss : 0.00970751770815126\n",
      "Epoch : 623\n",
      "Val Loss : 0.009123502016067505\n",
      "Epoch : 624\n",
      "Train Loss : 0.009706920934211317\n",
      "Epoch : 624\n",
      "Val Loss : 0.009124192267656326\n",
      "Epoch : 625\n",
      "Train Loss : 0.009706326992353723\n",
      "Epoch : 625\n",
      "Val Loss : 0.009124888122081757\n",
      "Epoch : 626\n",
      "Train Loss : 0.009705729480978646\n",
      "Epoch : 626\n",
      "Val Loss : 0.009125575840473176\n",
      "Epoch : 627\n",
      "Train Loss : 0.009705130413532576\n",
      "Epoch : 627\n",
      "Val Loss : 0.00912626899778843\n",
      "Epoch : 628\n",
      "Train Loss : 0.009704532718213003\n",
      "Epoch : 628\n",
      "Val Loss : 0.009126938968896867\n",
      "Epoch : 629\n",
      "Train Loss : 0.009703941803982355\n",
      "Epoch : 629\n",
      "Val Loss : 0.009127650737762451\n",
      "Epoch : 630\n",
      "Train Loss : 0.009703348660874727\n",
      "Epoch : 630\n",
      "Val Loss : 0.00912833197414875\n",
      "Epoch : 631\n",
      "Train Loss : 0.009702754109183134\n",
      "Epoch : 631\n",
      "Val Loss : 0.009128990545868874\n",
      "Epoch : 632\n",
      "Train Loss : 0.009702161575909505\n",
      "Epoch : 632\n",
      "Val Loss : 0.00912968234717846\n",
      "Epoch : 633\n",
      "Train Loss : 0.00970156705404675\n",
      "Epoch : 633\n",
      "Val Loss : 0.00913036336004734\n",
      "Epoch : 634\n",
      "Train Loss : 0.009700975584668302\n",
      "Epoch : 634\n",
      "Val Loss : 0.009131049185991287\n",
      "Epoch : 635\n",
      "Train Loss : 0.00970038399597451\n",
      "Epoch : 635\n",
      "Val Loss : 0.009131711915135383\n",
      "Epoch : 636\n",
      "Train Loss : 0.009699799962262242\n",
      "Epoch : 636\n",
      "Val Loss : 0.009132406741380692\n",
      "Epoch : 637\n",
      "Train Loss : 0.009699199908807396\n",
      "Epoch : 637\n",
      "Val Loss : 0.009133052334189415\n",
      "Epoch : 638\n",
      "Train Loss : 0.009698610307045573\n",
      "Epoch : 638\n",
      "Val Loss : 0.009133717343211174\n",
      "Epoch : 639\n",
      "Train Loss : 0.009698025698299615\n",
      "Epoch : 639\n",
      "Val Loss : 0.009134416922926903\n",
      "Epoch : 640\n",
      "Train Loss : 0.009697435014413875\n",
      "Epoch : 640\n",
      "Val Loss : 0.009135076105594634\n",
      "Epoch : 641\n",
      "Train Loss : 0.009696848486679411\n",
      "Epoch : 641\n",
      "Val Loss : 0.009135750651359558\n",
      "Epoch : 642\n",
      "Train Loss : 0.009696258379484518\n",
      "Epoch : 642\n",
      "Val Loss : 0.009136404424905776\n",
      "Epoch : 643\n",
      "Train Loss : 0.009695673102904045\n",
      "Epoch : 643\n",
      "Val Loss : 0.009137081667780876\n",
      "Epoch : 644\n",
      "Train Loss : 0.00969508978011239\n",
      "Epoch : 644\n",
      "Val Loss : 0.009137744277715684\n",
      "Epoch : 645\n",
      "Train Loss : 0.00969450108978725\n",
      "Epoch : 645\n",
      "Val Loss : 0.009138402432203293\n",
      "Epoch : 646\n",
      "Train Loss : 0.009693918232656883\n",
      "Epoch : 646\n",
      "Val Loss : 0.00913907054066658\n",
      "Epoch : 647\n",
      "Train Loss : 0.009693329505874275\n",
      "Epoch : 647\n",
      "Val Loss : 0.0091397116035223\n",
      "Epoch : 648\n",
      "Train Loss : 0.009692744734726873\n",
      "Epoch : 648\n",
      "Val Loss : 0.00914038160443306\n",
      "Epoch : 649\n",
      "Train Loss : 0.009692162167599086\n",
      "Epoch : 649\n",
      "Val Loss : 0.009141036868095398\n",
      "Epoch : 650\n",
      "Train Loss : 0.009691579689957812\n",
      "Epoch : 650\n",
      "Val Loss : 0.009141697466373444\n",
      "Epoch : 651\n",
      "Train Loss : 0.009690993874801117\n",
      "Epoch : 651\n",
      "Val Loss : 0.009142341881990432\n",
      "Epoch : 652\n",
      "Train Loss : 0.009690413362548762\n",
      "Epoch : 652\n",
      "Val Loss : 0.00914298889040947\n",
      "Epoch : 653\n",
      "Train Loss : 0.009689837153934712\n",
      "Epoch : 653\n",
      "Val Loss : 0.009143666237592697\n",
      "Epoch : 654\n",
      "Train Loss : 0.009689250369340818\n",
      "Epoch : 654\n",
      "Val Loss : 0.00914430084824562\n",
      "Epoch : 655\n",
      "Train Loss : 0.009688672584769882\n",
      "Epoch : 655\n",
      "Val Loss : 0.009144940942525864\n",
      "Epoch : 656\n",
      "Train Loss : 0.009688092859667392\n",
      "Epoch : 656\n",
      "Val Loss : 0.009145606577396393\n",
      "Epoch : 657\n",
      "Train Loss : 0.00968751360519766\n",
      "Epoch : 657\n",
      "Val Loss : 0.009146247014403343\n",
      "Epoch : 658\n",
      "Train Loss : 0.009686932925572387\n",
      "Epoch : 658\n",
      "Val Loss : 0.009146880641579628\n",
      "Epoch : 659\n",
      "Train Loss : 0.009686356604271617\n",
      "Epoch : 659\n",
      "Val Loss : 0.00914753320813179\n",
      "Epoch : 660\n",
      "Train Loss : 0.009685781401552233\n",
      "Epoch : 660\n",
      "Val Loss : 0.009148198053240776\n",
      "Epoch : 661\n",
      "Train Loss : 0.009685204760420047\n",
      "Epoch : 661\n",
      "Val Loss : 0.009148835346102715\n",
      "Epoch : 662\n",
      "Train Loss : 0.0096846263196147\n",
      "Epoch : 662\n",
      "Val Loss : 0.009149473503232003\n",
      "Epoch : 663\n",
      "Train Loss : 0.009684047497663103\n",
      "Epoch : 663\n",
      "Val Loss : 0.009150093808770179\n",
      "Epoch : 664\n",
      "Train Loss : 0.009683474409476825\n",
      "Epoch : 664\n",
      "Val Loss : 0.00915074609220028\n",
      "Epoch : 665\n",
      "Train Loss : 0.009682903691025922\n",
      "Epoch : 665\n",
      "Val Loss : 0.009151394203305244\n",
      "Epoch : 666\n",
      "Train Loss : 0.009682327270295697\n",
      "Epoch : 666\n",
      "Val Loss : 0.009152027830481529\n",
      "Epoch : 667\n",
      "Train Loss : 0.009681752211749023\n",
      "Epoch : 667\n",
      "Val Loss : 0.009152640298008918\n",
      "Epoch : 668\n",
      "Train Loss : 0.009681181317639415\n",
      "Epoch : 668\n",
      "Val Loss : 0.00915328012406826\n",
      "Epoch : 669\n",
      "Train Loss : 0.009680603261294633\n",
      "Epoch : 669\n",
      "Val Loss : 0.009153922095894813\n",
      "Epoch : 670\n",
      "Train Loss : 0.00968003410720051\n",
      "Epoch : 670\n",
      "Val Loss : 0.00915455824136734\n",
      "Epoch : 671\n",
      "Train Loss : 0.009679464704532747\n",
      "Epoch : 671\n",
      "Val Loss : 0.00915517722070217\n",
      "Epoch : 672\n",
      "Train Loss : 0.009678891182171174\n",
      "Epoch : 672\n",
      "Val Loss : 0.009155806630849838\n",
      "Epoch : 673\n",
      "Train Loss : 0.009678319832343223\n",
      "Epoch : 673\n",
      "Val Loss : 0.009156424283981323\n",
      "Epoch : 674\n",
      "Train Loss : 0.009677752116661904\n",
      "Epoch : 674\n",
      "Val Loss : 0.009157055199146271\n",
      "Epoch : 675\n",
      "Train Loss : 0.009677182828338014\n",
      "Epoch : 675\n",
      "Val Loss : 0.009157689720392227\n",
      "Epoch : 676\n",
      "Train Loss : 0.009676611644225825\n",
      "Epoch : 676\n",
      "Val Loss : 0.00915831497311592\n",
      "Epoch : 677\n",
      "Train Loss : 0.009676044081003005\n",
      "Epoch : 677\n",
      "Val Loss : 0.009158933252096177\n",
      "Epoch : 678\n",
      "Train Loss : 0.009675479397920102\n",
      "Epoch : 678\n",
      "Val Loss : 0.009159577533602714\n",
      "Epoch : 679\n",
      "Train Loss : 0.009674906172189742\n",
      "Epoch : 679\n",
      "Val Loss : 0.0091601752191782\n",
      "Epoch : 680\n",
      "Train Loss : 0.009674338217877727\n",
      "Epoch : 680\n",
      "Val Loss : 0.009160782888531685\n",
      "Epoch : 681\n",
      "Train Loss : 0.00967377320004899\n",
      "Epoch : 681\n",
      "Val Loss : 0.00916139230132103\n",
      "Epoch : 682\n",
      "Train Loss : 0.009673207342041345\n",
      "Epoch : 682\n",
      "Val Loss : 0.009162026181817055\n",
      "Epoch : 683\n",
      "Train Loss : 0.009672643124619731\n",
      "Epoch : 683\n",
      "Val Loss : 0.009162629500031471\n",
      "Epoch : 684\n",
      "Train Loss : 0.009672080910701663\n",
      "Epoch : 684\n",
      "Val Loss : 0.009163239359855652\n",
      "Epoch : 685\n",
      "Train Loss : 0.009671515099094431\n",
      "Epoch : 685\n",
      "Val Loss : 0.00916386191546917\n",
      "Epoch : 686\n",
      "Train Loss : 0.009670950271838818\n",
      "Epoch : 686\n",
      "Val Loss : 0.009164471238851547\n",
      "Epoch : 687\n",
      "Train Loss : 0.00967038578761483\n",
      "Epoch : 687\n",
      "Val Loss : 0.009165086716413497\n",
      "Epoch : 688\n",
      "Train Loss : 0.009669825278911939\n",
      "Epoch : 688\n",
      "Val Loss : 0.00916570584475994\n",
      "Epoch : 689\n",
      "Train Loss : 0.009669260010852403\n",
      "Epoch : 689\n",
      "Val Loss : 0.009166286781430244\n",
      "Epoch : 690\n",
      "Train Loss : 0.009668703073324151\n",
      "Epoch : 690\n",
      "Val Loss : 0.009166917592287063\n",
      "Epoch : 691\n",
      "Train Loss : 0.009668139192305532\n",
      "Epoch : 691\n",
      "Val Loss : 0.009167505279183388\n",
      "Epoch : 692\n",
      "Train Loss : 0.009667583335244041\n",
      "Epoch : 692\n",
      "Val Loss : 0.009168144598603248\n",
      "Epoch : 693\n",
      "Train Loss : 0.009667015773678378\n",
      "Epoch : 693\n",
      "Val Loss : 0.009168717116117477\n",
      "Epoch : 694\n",
      "Train Loss : 0.009666459023408937\n",
      "Epoch : 694\n",
      "Val Loss : 0.009169345498085022\n",
      "Epoch : 695\n",
      "Train Loss : 0.009665898251217985\n",
      "Epoch : 695\n",
      "Val Loss : 0.009169933766126633\n",
      "Epoch : 696\n",
      "Train Loss : 0.009665341307061105\n",
      "Epoch : 696\n",
      "Val Loss : 0.009170516073703765\n",
      "Epoch : 697\n",
      "Train Loss : 0.009664785599143797\n",
      "Epoch : 697\n",
      "Val Loss : 0.009171141281723976\n",
      "Epoch : 698\n",
      "Train Loss : 0.009664224372891663\n",
      "Epoch : 698\n",
      "Val Loss : 0.0091717369556427\n",
      "Epoch : 699\n",
      "Train Loss : 0.009663669279779828\n",
      "Epoch : 699\n",
      "Val Loss : 0.00917231959104538\n",
      "Epoch : 700\n",
      "Train Loss : 0.009663113102886919\n",
      "Epoch : 700\n",
      "Val Loss : 0.00917294329404831\n",
      "Epoch : 701\n",
      "Train Loss : 0.009662557971660459\n",
      "Epoch : 701\n",
      "Val Loss : 0.009173516884446144\n",
      "Epoch : 702\n",
      "Train Loss : 0.009661999784635373\n",
      "Epoch : 702\n",
      "Val Loss : 0.009174109667539596\n",
      "Epoch : 703\n",
      "Train Loss : 0.009661444583808293\n",
      "Epoch : 703\n",
      "Val Loss : 0.00917471495270729\n",
      "Epoch : 704\n",
      "Train Loss : 0.009660887853424744\n",
      "Epoch : 704\n",
      "Val Loss : 0.009175293058156966\n",
      "Epoch : 705\n",
      "Train Loss : 0.009660340426323995\n",
      "Epoch : 705\n",
      "Val Loss : 0.009175913974642753\n",
      "Epoch : 706\n",
      "Train Loss : 0.009659784942122966\n",
      "Epoch : 706\n",
      "Val Loss : 0.009176505908370018\n",
      "Epoch : 707\n",
      "Train Loss : 0.009659233589215848\n",
      "Epoch : 707\n",
      "Val Loss : 0.009177088096737862\n",
      "Epoch : 708\n",
      "Train Loss : 0.009658680524464922\n",
      "Epoch : 708\n",
      "Val Loss : 0.009177676394581795\n",
      "Epoch : 709\n",
      "Train Loss : 0.00965812513140756\n",
      "Epoch : 709\n",
      "Val Loss : 0.009178253799676895\n",
      "Epoch : 710\n",
      "Train Loss : 0.009657579646495525\n",
      "Epoch : 710\n",
      "Val Loss : 0.009178859263658523\n",
      "Epoch : 711\n",
      "Train Loss : 0.0096570247406425\n",
      "Epoch : 711\n",
      "Val Loss : 0.00917943587899208\n",
      "Epoch : 712\n",
      "Train Loss : 0.0096564722260679\n",
      "Epoch : 712\n",
      "Val Loss : 0.00918001788854599\n",
      "Epoch : 713\n",
      "Train Loss : 0.00965592536737221\n",
      "Epoch : 713\n",
      "Val Loss : 0.00918059441447258\n",
      "Epoch : 714\n",
      "Train Loss : 0.009655373527260756\n",
      "Epoch : 714\n",
      "Val Loss : 0.009181168138980865\n",
      "Epoch : 715\n",
      "Train Loss : 0.009654824938492524\n",
      "Epoch : 715\n",
      "Val Loss : 0.009181747376918794\n",
      "Epoch : 716\n",
      "Train Loss : 0.009654275171485236\n",
      "Epoch : 716\n",
      "Val Loss : 0.009182337373495102\n",
      "Epoch : 717\n",
      "Train Loss : 0.009653729731316457\n",
      "Epoch : 717\n",
      "Val Loss : 0.009182913199067116\n",
      "Epoch : 718\n",
      "Train Loss : 0.009653182506388936\n",
      "Epoch : 718\n",
      "Val Loss : 0.009183499097824097\n",
      "Epoch : 719\n",
      "Train Loss : 0.009652636852446824\n",
      "Epoch : 719\n",
      "Val Loss : 0.009184071213006973\n",
      "Epoch : 720\n",
      "Train Loss : 0.009652088666367891\n",
      "Epoch : 720\n",
      "Val Loss : 0.009184663861989975\n",
      "Epoch : 721\n",
      "Train Loss : 0.009651544278494191\n",
      "Epoch : 721\n",
      "Val Loss : 0.009185232371091843\n",
      "Epoch : 722\n",
      "Train Loss : 0.00965099794014599\n",
      "Epoch : 722\n",
      "Val Loss : 0.009185797467827796\n",
      "Epoch : 723\n",
      "Train Loss : 0.009650453948332957\n",
      "Epoch : 723\n",
      "Val Loss : 0.009186368212103843\n",
      "Epoch : 724\n",
      "Train Loss : 0.009649911378361152\n",
      "Epoch : 724\n",
      "Val Loss : 0.0091869508177042\n",
      "Epoch : 725\n",
      "Train Loss : 0.009649367086602594\n",
      "Epoch : 725\n",
      "Val Loss : 0.009187533631920814\n",
      "Epoch : 726\n",
      "Train Loss : 0.009648823225705013\n",
      "Epoch : 726\n",
      "Val Loss : 0.009188086718320847\n",
      "Epoch : 727\n",
      "Train Loss : 0.00964827998789869\n",
      "Epoch : 727\n",
      "Val Loss : 0.009188670679926873\n",
      "Epoch : 728\n",
      "Train Loss : 0.009647735570196155\n",
      "Epoch : 728\n",
      "Val Loss : 0.009189215332269669\n",
      "Epoch : 729\n",
      "Train Loss : 0.009647195411388667\n",
      "Epoch : 729\n",
      "Val Loss : 0.009189779162406922\n",
      "Epoch : 730\n",
      "Train Loss : 0.009646656500420854\n",
      "Epoch : 730\n",
      "Val Loss : 0.009190374523401261\n",
      "Epoch : 731\n",
      "Train Loss : 0.00964611383267675\n",
      "Epoch : 731\n",
      "Val Loss : 0.009190927997231483\n",
      "Epoch : 732\n",
      "Train Loss : 0.009645573952271738\n",
      "Epoch : 732\n",
      "Val Loss : 0.009191494703292847\n",
      "Epoch : 733\n",
      "Train Loss : 0.00964502822707185\n",
      "Epoch : 733\n",
      "Val Loss : 0.009192040517926216\n",
      "Epoch : 734\n",
      "Train Loss : 0.009644494358834636\n",
      "Epoch : 734\n",
      "Val Loss : 0.009192612707614898\n",
      "Epoch : 735\n",
      "Train Loss : 0.009643952423554192\n",
      "Epoch : 735\n",
      "Val Loss : 0.009193161025643349\n",
      "Epoch : 736\n",
      "Train Loss : 0.00964341682193012\n",
      "Epoch : 736\n",
      "Val Loss : 0.009193749099969863\n",
      "Epoch : 737\n",
      "Train Loss : 0.009642876792380925\n",
      "Epoch : 737\n",
      "Val Loss : 0.009194301679730415\n",
      "Epoch : 738\n",
      "Train Loss : 0.009642336946776223\n",
      "Epoch : 738\n",
      "Val Loss : 0.00919486802816391\n",
      "Epoch : 739\n",
      "Train Loss : 0.00964180659337029\n",
      "Epoch : 739\n",
      "Val Loss : 0.009195425122976303\n",
      "Epoch : 740\n",
      "Train Loss : 0.009641268143092291\n",
      "Epoch : 740\n",
      "Val Loss : 0.009195957019925118\n",
      "Epoch : 741\n",
      "Train Loss : 0.009640731242256657\n",
      "Epoch : 741\n",
      "Val Loss : 0.00919653157889843\n",
      "Epoch : 742\n",
      "Train Loss : 0.009640194536965619\n",
      "Epoch : 742\n",
      "Val Loss : 0.009197080910205841\n",
      "Epoch : 743\n",
      "Train Loss : 0.009639661943082602\n",
      "Epoch : 743\n",
      "Val Loss : 0.009197652146220207\n",
      "Epoch : 744\n",
      "Train Loss : 0.00963912322600223\n",
      "Epoch : 744\n",
      "Val Loss : 0.009198191300034523\n",
      "Epoch : 745\n",
      "Train Loss : 0.009638588235869314\n",
      "Epoch : 745\n",
      "Val Loss : 0.009198740378022194\n",
      "Epoch : 746\n",
      "Train Loss : 0.00963805694782649\n",
      "Epoch : 746\n",
      "Val Loss : 0.009199279740452766\n",
      "Epoch : 747\n",
      "Train Loss : 0.009637526011101079\n",
      "Epoch : 747\n",
      "Val Loss : 0.009199845999479294\n",
      "Epoch : 748\n",
      "Train Loss : 0.009636987110076213\n",
      "Epoch : 748\n",
      "Val Loss : 0.009200383111834525\n",
      "Epoch : 749\n",
      "Train Loss : 0.00963645857788649\n",
      "Epoch : 749\n",
      "Val Loss : 0.009200926437973976\n",
      "Epoch : 750\n",
      "Train Loss : 0.009635927220243045\n",
      "Epoch : 750\n",
      "Val Loss : 0.009201494172215462\n",
      "Epoch : 751\n",
      "Train Loss : 0.009635396485690863\n",
      "Epoch : 751\n",
      "Val Loss : 0.009202026292681693\n",
      "Epoch : 752\n",
      "Train Loss : 0.009634867837500106\n",
      "Epoch : 752\n",
      "Val Loss : 0.009202580258250237\n",
      "Epoch : 753\n",
      "Train Loss : 0.009634339149537565\n",
      "Epoch : 753\n",
      "Val Loss : 0.00920313572883606\n",
      "Epoch : 754\n",
      "Train Loss : 0.009633804701295188\n",
      "Epoch : 754\n",
      "Val Loss : 0.009203667089343072\n",
      "Epoch : 755\n",
      "Train Loss : 0.009633279518220985\n",
      "Epoch : 755\n",
      "Val Loss : 0.009204240456223488\n",
      "Epoch : 756\n",
      "Train Loss : 0.00963274976139179\n",
      "Epoch : 756\n",
      "Val Loss : 0.009204758405685425\n",
      "Epoch : 757\n",
      "Train Loss : 0.009632216948763967\n",
      "Epoch : 757\n",
      "Val Loss : 0.009205315321683883\n",
      "Epoch : 758\n",
      "Train Loss : 0.009631693245531507\n",
      "Epoch : 758\n",
      "Val Loss : 0.009205851316452027\n",
      "Epoch : 759\n",
      "Train Loss : 0.009631162426464286\n",
      "Epoch : 759\n",
      "Val Loss : 0.009206384047865867\n",
      "Epoch : 760\n",
      "Train Loss : 0.009630633461756426\n",
      "Epoch : 760\n",
      "Val Loss : 0.0092069244235754\n",
      "Epoch : 761\n",
      "Train Loss : 0.009630111397452838\n",
      "Epoch : 761\n",
      "Val Loss : 0.00920747298002243\n",
      "Epoch : 762\n",
      "Train Loss : 0.009629585954204892\n",
      "Epoch : 762\n",
      "Val Loss : 0.009208017572760583\n",
      "Epoch : 763\n",
      "Train Loss : 0.00962906192616954\n",
      "Epoch : 763\n",
      "Val Loss : 0.009208556532859803\n",
      "Epoch : 764\n",
      "Train Loss : 0.009628533011176409\n",
      "Epoch : 764\n",
      "Val Loss : 0.009209075063467026\n",
      "Epoch : 765\n",
      "Train Loss : 0.009628011589849972\n",
      "Epoch : 765\n",
      "Val Loss : 0.009209627419710159\n",
      "Epoch : 766\n",
      "Train Loss : 0.009627489048284992\n",
      "Epoch : 766\n",
      "Val Loss : 0.00921016252040863\n",
      "Epoch : 767\n",
      "Train Loss : 0.009626961763934945\n",
      "Epoch : 767\n",
      "Val Loss : 0.009210676163434982\n",
      "Epoch : 768\n",
      "Train Loss : 0.009626440309465356\n",
      "Epoch : 768\n",
      "Val Loss : 0.009211219176650047\n",
      "Epoch : 769\n",
      "Train Loss : 0.009625917656870818\n",
      "Epoch : 769\n",
      "Val Loss : 0.009211760863661766\n",
      "Epoch : 770\n",
      "Train Loss : 0.009625395348965062\n",
      "Epoch : 770\n",
      "Val Loss : 0.00921228477358818\n",
      "Epoch : 771\n",
      "Train Loss : 0.009624874653473656\n",
      "Epoch : 771\n",
      "Val Loss : 0.009212811648845673\n",
      "Epoch : 772\n",
      "Train Loss : 0.009624349558228806\n",
      "Epoch : 772\n",
      "Val Loss : 0.009213340923190117\n",
      "Epoch : 773\n",
      "Train Loss : 0.00962382493858819\n",
      "Epoch : 773\n",
      "Val Loss : 0.009213855043053627\n",
      "Epoch : 774\n",
      "Train Loss : 0.009623308729022423\n",
      "Epoch : 774\n",
      "Val Loss : 0.009214394539594651\n",
      "Epoch : 775\n",
      "Train Loss : 0.009622789980691205\n",
      "Epoch : 775\n",
      "Val Loss : 0.009214929476380349\n",
      "Epoch : 776\n",
      "Train Loss : 0.009622269487373027\n",
      "Epoch : 776\n",
      "Val Loss : 0.0092154491096735\n",
      "Epoch : 777\n",
      "Train Loss : 0.009621754224044255\n",
      "Epoch : 777\n",
      "Val Loss : 0.00921598795056343\n",
      "Epoch : 778\n",
      "Train Loss : 0.009621230505897376\n",
      "Epoch : 778\n",
      "Val Loss : 0.009216492772102356\n",
      "Epoch : 779\n",
      "Train Loss : 0.009620714591305663\n",
      "Epoch : 779\n",
      "Val Loss : 0.009217024430632591\n",
      "Epoch : 780\n",
      "Train Loss : 0.009620198363511164\n",
      "Epoch : 780\n",
      "Val Loss : 0.009217546954751014\n",
      "Epoch : 781\n",
      "Train Loss : 0.009619680987306443\n",
      "Epoch : 781\n",
      "Val Loss : 0.009218062192201615\n",
      "Epoch : 782\n",
      "Train Loss : 0.009619164406537373\n",
      "Epoch : 782\n",
      "Val Loss : 0.00921859373152256\n",
      "Epoch : 783\n",
      "Train Loss : 0.009618649685099137\n",
      "Epoch : 783\n",
      "Val Loss : 0.0092191099524498\n",
      "Epoch : 784\n",
      "Train Loss : 0.009618132356951987\n",
      "Epoch : 784\n",
      "Val Loss : 0.009219625413417816\n",
      "Epoch : 785\n",
      "Train Loss : 0.009617620093078481\n",
      "Epoch : 785\n",
      "Val Loss : 0.009220156252384187\n",
      "Epoch : 786\n",
      "Train Loss : 0.009617100517825617\n",
      "Epoch : 786\n",
      "Val Loss : 0.00922067642211914\n",
      "Epoch : 787\n",
      "Train Loss : 0.009616589766937007\n",
      "Epoch : 787\n",
      "Val Loss : 0.009221197411417961\n",
      "Epoch : 788\n",
      "Train Loss : 0.009616077420205621\n",
      "Epoch : 788\n",
      "Val Loss : 0.009221711471676827\n",
      "Epoch : 789\n",
      "Train Loss : 0.009615562801511155\n",
      "Epoch : 789\n",
      "Val Loss : 0.009222240269184112\n",
      "Epoch : 790\n",
      "Train Loss : 0.00961505064866721\n",
      "Epoch : 790\n",
      "Val Loss : 0.009222744569182395\n",
      "Epoch : 791\n",
      "Train Loss : 0.009614539849721028\n",
      "Epoch : 791\n",
      "Val Loss : 0.009223268032073975\n",
      "Epoch : 792\n",
      "Train Loss : 0.009614022592831655\n",
      "Epoch : 792\n",
      "Val Loss : 0.009223767772316933\n",
      "Epoch : 793\n",
      "Train Loss : 0.009613511790571157\n",
      "Epoch : 793\n",
      "Val Loss : 0.009224271610379219\n",
      "Epoch : 794\n",
      "Train Loss : 0.009613003174101542\n",
      "Epoch : 794\n",
      "Val Loss : 0.0092247896194458\n",
      "Epoch : 795\n",
      "Train Loss : 0.009612495010035955\n",
      "Epoch : 795\n",
      "Val Loss : 0.009225310921669007\n",
      "Epoch : 796\n",
      "Train Loss : 0.009611980281969088\n",
      "Epoch : 796\n",
      "Val Loss : 0.0092258111089468\n",
      "Epoch : 797\n",
      "Train Loss : 0.009611475516733749\n",
      "Epoch : 797\n",
      "Val Loss : 0.009226345121860505\n",
      "Epoch : 798\n",
      "Train Loss : 0.009610964210697341\n",
      "Epoch : 798\n",
      "Val Loss : 0.009226840749382972\n",
      "Epoch : 799\n",
      "Train Loss : 0.00961045272568791\n",
      "Epoch : 799\n",
      "Val Loss : 0.009227341800928116\n",
      "Epoch : 800\n",
      "Train Loss : 0.0096099465286684\n",
      "Epoch : 800\n",
      "Val Loss : 0.009227848067879676\n",
      "Epoch : 801\n",
      "Train Loss : 0.009609443541563172\n",
      "Epoch : 801\n",
      "Val Loss : 0.009228368774056434\n",
      "Epoch : 802\n",
      "Train Loss : 0.009608929610589114\n",
      "Epoch : 802\n",
      "Val Loss : 0.009228860363364219\n",
      "Epoch : 803\n",
      "Train Loss : 0.009608426069993247\n",
      "Epoch : 803\n",
      "Val Loss : 0.009229371830821037\n",
      "Epoch : 804\n",
      "Train Loss : 0.009607916856946895\n",
      "Epoch : 804\n",
      "Val Loss : 0.009229889452457427\n",
      "Epoch : 805\n",
      "Train Loss : 0.00960741505139504\n",
      "Epoch : 805\n",
      "Val Loss : 0.009230386972427368\n",
      "Epoch : 806\n",
      "Train Loss : 0.009606908443400444\n",
      "Epoch : 806\n",
      "Val Loss : 0.009230883076786996\n",
      "Epoch : 807\n",
      "Train Loss : 0.0096064011493426\n",
      "Epoch : 807\n",
      "Val Loss : 0.00923138515651226\n",
      "Epoch : 808\n",
      "Train Loss : 0.009605894698777975\n",
      "Epoch : 808\n",
      "Val Loss : 0.009231896474957466\n",
      "Epoch : 809\n",
      "Train Loss : 0.009605394474154478\n",
      "Epoch : 809\n",
      "Val Loss : 0.009232389241456985\n",
      "Epoch : 810\n",
      "Train Loss : 0.009604894327417388\n",
      "Epoch : 810\n",
      "Val Loss : 0.00923289854824543\n",
      "Epoch : 811\n",
      "Train Loss : 0.00960438342075596\n",
      "Epoch : 811\n",
      "Val Loss : 0.009233397960662842\n",
      "Epoch : 812\n",
      "Train Loss : 0.009603883832480984\n",
      "Epoch : 812\n",
      "Val Loss : 0.009233914449810981\n",
      "Epoch : 813\n",
      "Train Loss : 0.009603380152683878\n",
      "Epoch : 813\n",
      "Val Loss : 0.009234373793005943\n",
      "Epoch : 814\n",
      "Train Loss : 0.00960287981703082\n",
      "Epoch : 814\n",
      "Val Loss : 0.009234895363450051\n",
      "Epoch : 815\n",
      "Train Loss : 0.009602380775617854\n",
      "Epoch : 815\n",
      "Val Loss : 0.009235402822494507\n",
      "Epoch : 816\n",
      "Train Loss : 0.009601875151974875\n",
      "Epoch : 816\n",
      "Val Loss : 0.009235887452960015\n",
      "Epoch : 817\n",
      "Train Loss : 0.009601378874700795\n",
      "Epoch : 817\n",
      "Val Loss : 0.009236382111907006\n",
      "Epoch : 818\n",
      "Train Loss : 0.009600877528181598\n",
      "Epoch : 818\n",
      "Val Loss : 0.009236870691180229\n",
      "Epoch : 819\n",
      "Train Loss : 0.009600379462834461\n",
      "Epoch : 819\n",
      "Val Loss : 0.009237392380833626\n",
      "Epoch : 820\n",
      "Train Loss : 0.009599876016696577\n",
      "Epoch : 820\n",
      "Val Loss : 0.00923786886036396\n",
      "Epoch : 821\n",
      "Train Loss : 0.009599377527117092\n",
      "Epoch : 821\n",
      "Val Loss : 0.009238355368375778\n",
      "Epoch : 822\n",
      "Train Loss : 0.009598878888393425\n",
      "Epoch : 822\n",
      "Val Loss : 0.009238855063915252\n",
      "Epoch : 823\n",
      "Train Loss : 0.00959838406776088\n",
      "Epoch : 823\n",
      "Val Loss : 0.009239350527524948\n",
      "Epoch : 824\n",
      "Train Loss : 0.00959788792803088\n",
      "Epoch : 824\n",
      "Val Loss : 0.009239838004112244\n",
      "Epoch : 825\n",
      "Train Loss : 0.00959738749129121\n",
      "Epoch : 825\n",
      "Val Loss : 0.009240337535738946\n",
      "Epoch : 826\n",
      "Train Loss : 0.009596893335178866\n",
      "Epoch : 826\n",
      "Val Loss : 0.009240806460380554\n",
      "Epoch : 827\n",
      "Train Loss : 0.009596398973578979\n",
      "Epoch : 827\n",
      "Val Loss : 0.009241321071982384\n",
      "Epoch : 828\n",
      "Train Loss : 0.009595900790573651\n",
      "Epoch : 828\n",
      "Val Loss : 0.00924180829524994\n",
      "Epoch : 829\n",
      "Train Loss : 0.0095954027765984\n",
      "Epoch : 829\n",
      "Val Loss : 0.009242289125919342\n",
      "Epoch : 830\n",
      "Train Loss : 0.009594913132926217\n",
      "Epoch : 830\n",
      "Val Loss : 0.009242785289883614\n",
      "Epoch : 831\n",
      "Train Loss : 0.009594414456087923\n",
      "Epoch : 831\n",
      "Val Loss : 0.009243261992931366\n",
      "Epoch : 832\n",
      "Train Loss : 0.009593918697504174\n",
      "Epoch : 832\n",
      "Val Loss : 0.009243740558624267\n",
      "Epoch : 833\n",
      "Train Loss : 0.009593429483035812\n",
      "Epoch : 833\n",
      "Val Loss : 0.009244217425584794\n",
      "Epoch : 834\n",
      "Train Loss : 0.00959293787231755\n",
      "Epoch : 834\n",
      "Val Loss : 0.009244716793298722\n",
      "Epoch : 835\n",
      "Train Loss : 0.009592440623949114\n",
      "Epoch : 835\n",
      "Val Loss : 0.00924519668519497\n",
      "Epoch : 836\n",
      "Train Loss : 0.009591946739610616\n",
      "Epoch : 836\n",
      "Val Loss : 0.009245673328638076\n",
      "Epoch : 837\n",
      "Train Loss : 0.009591461109574154\n",
      "Epoch : 837\n",
      "Val Loss : 0.009246152460575103\n",
      "Epoch : 838\n",
      "Train Loss : 0.009590969871716355\n",
      "Epoch : 838\n",
      "Val Loss : 0.009246649980545044\n",
      "Epoch : 839\n",
      "Train Loss : 0.009590472661462544\n",
      "Epoch : 839\n",
      "Val Loss : 0.009247127398848533\n",
      "Epoch : 840\n",
      "Train Loss : 0.00958998359945268\n",
      "Epoch : 840\n",
      "Val Loss : 0.009247593834996223\n",
      "Epoch : 841\n",
      "Train Loss : 0.009589499548687418\n",
      "Epoch : 841\n",
      "Val Loss : 0.009248096004128455\n",
      "Epoch : 842\n",
      "Train Loss : 0.009589007952883574\n",
      "Epoch : 842\n",
      "Val Loss : 0.009248549327254295\n",
      "Epoch : 843\n",
      "Train Loss : 0.009588519114589989\n",
      "Epoch : 843\n",
      "Val Loss : 0.009249034151434899\n",
      "Epoch : 844\n",
      "Train Loss : 0.009588032296371524\n",
      "Epoch : 844\n",
      "Val Loss : 0.009249519646167756\n",
      "Epoch : 845\n",
      "Train Loss : 0.009587544901462212\n",
      "Epoch : 845\n",
      "Val Loss : 0.00924998825788498\n",
      "Epoch : 846\n",
      "Train Loss : 0.009587052545023029\n",
      "Epoch : 846\n",
      "Val Loss : 0.009250475451350212\n",
      "Epoch : 847\n",
      "Train Loss : 0.009586569067634297\n",
      "Epoch : 847\n",
      "Val Loss : 0.00925094747543335\n",
      "Epoch : 848\n",
      "Train Loss : 0.009586081323064731\n",
      "Epoch : 848\n",
      "Val Loss : 0.00925144524872303\n",
      "Epoch : 849\n",
      "Train Loss : 0.009585592680315742\n",
      "Epoch : 849\n",
      "Val Loss : 0.009251900851726532\n",
      "Epoch : 850\n",
      "Train Loss : 0.009585110427566483\n",
      "Epoch : 850\n",
      "Val Loss : 0.009252381935715676\n",
      "Epoch : 851\n",
      "Train Loss : 0.009584622368136972\n",
      "Epoch : 851\n",
      "Val Loss : 0.009252841278910637\n",
      "Epoch : 852\n",
      "Train Loss : 0.009584135190315306\n",
      "Epoch : 852\n",
      "Val Loss : 0.009253302946686744\n",
      "Epoch : 853\n",
      "Train Loss : 0.009583651454409989\n",
      "Epoch : 853\n",
      "Val Loss : 0.00925377643108368\n",
      "Epoch : 854\n",
      "Train Loss : 0.009583168346567405\n",
      "Epoch : 854\n",
      "Val Loss : 0.009254263237118721\n",
      "Epoch : 855\n",
      "Train Loss : 0.009582684645462396\n",
      "Epoch : 855\n",
      "Val Loss : 0.009254724368453026\n",
      "Epoch : 856\n",
      "Train Loss : 0.009582197343353912\n",
      "Epoch : 856\n",
      "Val Loss : 0.009255189865827561\n",
      "Epoch : 857\n",
      "Train Loss : 0.009581718156346222\n",
      "Epoch : 857\n",
      "Val Loss : 0.009255687221884728\n",
      "Epoch : 858\n",
      "Train Loss : 0.009581237298923666\n",
      "Epoch : 858\n",
      "Val Loss : 0.009256146907806396\n",
      "Epoch : 859\n",
      "Train Loss : 0.00958075188763201\n",
      "Epoch : 859\n",
      "Val Loss : 0.009256595358252526\n",
      "Epoch : 860\n",
      "Train Loss : 0.009580272355935539\n",
      "Epoch : 860\n",
      "Val Loss : 0.009257074162364006\n",
      "Epoch : 861\n",
      "Train Loss : 0.009579787630707132\n",
      "Epoch : 861\n",
      "Val Loss : 0.00925753153860569\n",
      "Epoch : 862\n",
      "Train Loss : 0.00957931193533052\n",
      "Epoch : 862\n",
      "Val Loss : 0.009258016720414161\n",
      "Epoch : 863\n",
      "Train Loss : 0.009578830186357171\n",
      "Epoch : 863\n",
      "Val Loss : 0.00925848226249218\n",
      "Epoch : 864\n",
      "Train Loss : 0.009578350049798175\n",
      "Epoch : 864\n",
      "Val Loss : 0.009258930146694183\n",
      "Epoch : 865\n",
      "Train Loss : 0.009577870342442998\n",
      "Epoch : 865\n",
      "Val Loss : 0.009259392276406288\n",
      "Epoch : 866\n",
      "Train Loss : 0.009577393548370892\n",
      "Epoch : 866\n",
      "Val Loss : 0.00925985823571682\n",
      "Epoch : 867\n",
      "Train Loss : 0.009576910375899162\n",
      "Epoch : 867\n",
      "Val Loss : 0.009260336712002755\n",
      "Epoch : 868\n",
      "Train Loss : 0.00957644179635731\n",
      "Epoch : 868\n",
      "Val Loss : 0.009260813474655152\n",
      "Epoch : 869\n",
      "Train Loss : 0.009575957785363829\n",
      "Epoch : 869\n",
      "Val Loss : 0.009261256277561188\n",
      "Epoch : 870\n",
      "Train Loss : 0.00957547893807345\n",
      "Epoch : 870\n",
      "Val Loss : 0.009261702030897141\n",
      "Epoch : 871\n",
      "Train Loss : 0.009575003585728463\n",
      "Epoch : 871\n",
      "Val Loss : 0.009262192517518997\n",
      "Epoch : 872\n",
      "Train Loss : 0.009574528369270398\n",
      "Epoch : 872\n",
      "Val Loss : 0.009262639179825782\n",
      "Epoch : 873\n",
      "Train Loss : 0.009574048004023651\n",
      "Epoch : 873\n",
      "Val Loss : 0.009263096705079079\n",
      "Epoch : 874\n",
      "Train Loss : 0.009573574090091465\n",
      "Epoch : 874\n",
      "Val Loss : 0.00926355528831482\n",
      "Epoch : 875\n",
      "Train Loss : 0.009573103392702193\n",
      "Epoch : 875\n",
      "Val Loss : 0.009264040023088456\n",
      "Epoch : 876\n",
      "Train Loss : 0.00957262629205593\n",
      "Epoch : 876\n",
      "Val Loss : 0.009264493584632874\n",
      "Epoch : 877\n",
      "Train Loss : 0.009572154785973746\n",
      "Epoch : 877\n",
      "Val Loss : 0.009264924630522729\n",
      "Epoch : 878\n",
      "Train Loss : 0.009571677744985158\n",
      "Epoch : 878\n",
      "Val Loss : 0.009265410646796226\n",
      "Epoch : 879\n",
      "Train Loss : 0.009571201517660954\n",
      "Epoch : 879\n",
      "Val Loss : 0.009265827357769013\n",
      "Epoch : 880\n",
      "Train Loss : 0.009570730256838094\n",
      "Epoch : 880\n",
      "Val Loss : 0.009266305029392243\n",
      "Epoch : 881\n",
      "Train Loss : 0.009570260378084679\n",
      "Epoch : 881\n",
      "Val Loss : 0.00926676668226719\n",
      "Epoch : 882\n",
      "Train Loss : 0.009569785928890587\n",
      "Epoch : 882\n",
      "Val Loss : 0.009267200395464897\n",
      "Epoch : 883\n",
      "Train Loss : 0.009569313121939682\n",
      "Epoch : 883\n",
      "Val Loss : 0.009267672404646873\n",
      "Epoch : 884\n",
      "Train Loss : 0.009568846310584994\n",
      "Epoch : 884\n",
      "Val Loss : 0.009268123671412469\n",
      "Epoch : 885\n",
      "Train Loss : 0.009568372376766918\n",
      "Epoch : 885\n",
      "Val Loss : 0.009268571987748147\n",
      "Epoch : 886\n",
      "Train Loss : 0.009567904774948052\n",
      "Epoch : 886\n",
      "Val Loss : 0.009269023284316063\n",
      "Epoch : 887\n",
      "Train Loss : 0.009567435366827397\n",
      "Epoch : 887\n",
      "Val Loss : 0.009269486039876937\n",
      "Epoch : 888\n",
      "Train Loss : 0.009566965957049586\n",
      "Epoch : 888\n",
      "Val Loss : 0.009269943416118621\n",
      "Epoch : 889\n",
      "Train Loss : 0.009566491424997612\n",
      "Epoch : 889\n",
      "Val Loss : 0.009270367175340653\n",
      "Epoch : 890\n",
      "Train Loss : 0.009566025778624723\n",
      "Epoch : 890\n",
      "Val Loss : 0.009270830988883972\n",
      "Epoch : 891\n",
      "Train Loss : 0.009565556917366078\n",
      "Epoch : 891\n",
      "Val Loss : 0.009271288529038428\n",
      "Epoch : 892\n",
      "Train Loss : 0.00956508988560943\n",
      "Epoch : 892\n",
      "Val Loss : 0.009271735951304436\n",
      "Epoch : 893\n",
      "Train Loss : 0.009564619973712862\n",
      "Epoch : 893\n",
      "Val Loss : 0.009272160485386849\n",
      "Epoch : 894\n",
      "Train Loss : 0.009564152789497715\n",
      "Epoch : 894\n",
      "Val Loss : 0.009272600680589676\n",
      "Epoch : 895\n",
      "Train Loss : 0.009563688412507551\n",
      "Epoch : 895\n",
      "Val Loss : 0.00927308164536953\n",
      "Epoch : 896\n",
      "Train Loss : 0.009563222361788207\n",
      "Epoch : 896\n",
      "Val Loss : 0.009273528784513473\n",
      "Epoch : 897\n",
      "Train Loss : 0.00956275543940396\n",
      "Epoch : 897\n",
      "Val Loss : 0.009273959696292878\n",
      "Epoch : 898\n",
      "Train Loss : 0.009562283928350304\n",
      "Epoch : 898\n",
      "Val Loss : 0.009274396941065789\n",
      "Epoch : 899\n",
      "Train Loss : 0.009561827777490498\n",
      "Epoch : 899\n",
      "Val Loss : 0.009274869546294213\n",
      "Epoch : 900\n",
      "Train Loss : 0.009561361706885261\n",
      "Epoch : 900\n",
      "Val Loss : 0.009275301441550254\n",
      "Epoch : 901\n",
      "Train Loss : 0.009560900806611755\n",
      "Epoch : 901\n",
      "Val Loss : 0.009275767117738724\n",
      "Epoch : 902\n",
      "Train Loss : 0.009560433514681364\n",
      "Epoch : 902\n",
      "Val Loss : 0.00927619606256485\n",
      "Epoch : 903\n",
      "Train Loss : 0.009559970267872687\n",
      "Epoch : 903\n",
      "Val Loss : 0.009276628166437149\n",
      "Epoch : 904\n",
      "Train Loss : 0.009559506346600867\n",
      "Epoch : 904\n",
      "Val Loss : 0.009277072519063949\n",
      "Epoch : 905\n",
      "Train Loss : 0.009559037806830796\n",
      "Epoch : 905\n",
      "Val Loss : 0.009277503147721291\n",
      "Epoch : 906\n",
      "Train Loss : 0.009558586503156987\n",
      "Epoch : 906\n",
      "Val Loss : 0.009277957811951638\n",
      "Epoch : 907\n",
      "Train Loss : 0.009558118344533168\n",
      "Epoch : 907\n",
      "Val Loss : 0.009278400182723998\n",
      "Epoch : 908\n",
      "Train Loss : 0.009557655092753019\n",
      "Epoch : 908\n",
      "Val Loss : 0.009278813272714615\n",
      "Epoch : 909\n",
      "Train Loss : 0.009557193882591042\n",
      "Epoch : 909\n",
      "Val Loss : 0.009279266402125359\n",
      "Epoch : 910\n",
      "Train Loss : 0.009556739900950541\n",
      "Epoch : 910\n",
      "Val Loss : 0.009279720962047577\n",
      "Epoch : 911\n",
      "Train Loss : 0.009556281849330961\n",
      "Epoch : 911\n",
      "Val Loss : 0.009280158311128616\n",
      "Epoch : 912\n",
      "Train Loss : 0.009555825665328001\n",
      "Epoch : 912\n",
      "Val Loss : 0.009280604556202889\n",
      "Epoch : 913\n",
      "Train Loss : 0.009555355131997333\n",
      "Epoch : 913\n",
      "Val Loss : 0.009281011074781418\n",
      "Epoch : 914\n",
      "Train Loss : 0.00955490268157046\n",
      "Epoch : 914\n",
      "Val Loss : 0.009281482011079789\n",
      "Epoch : 915\n",
      "Train Loss : 0.009554447082544137\n",
      "Epoch : 915\n",
      "Val Loss : 0.009281910419464112\n",
      "Epoch : 916\n",
      "Train Loss : 0.009553986386101016\n",
      "Epoch : 916\n",
      "Val Loss : 0.009282329261302949\n",
      "Epoch : 917\n",
      "Train Loss : 0.009553528264880817\n",
      "Epoch : 917\n",
      "Val Loss : 0.009282774418592453\n",
      "Epoch : 918\n",
      "Train Loss : 0.009553074770444652\n",
      "Epoch : 918\n",
      "Val Loss : 0.009283223941922188\n",
      "Epoch : 919\n",
      "Train Loss : 0.00955261176392383\n",
      "Epoch : 919\n",
      "Val Loss : 0.00928364384174347\n",
      "Epoch : 920\n",
      "Train Loss : 0.009552159850416022\n",
      "Epoch : 920\n",
      "Val Loss : 0.009284076794981956\n",
      "Epoch : 921\n",
      "Train Loss : 0.009551703742642314\n",
      "Epoch : 921\n",
      "Val Loss : 0.009284500300884247\n",
      "Epoch : 922\n",
      "Train Loss : 0.009551244830957935\n",
      "Epoch : 922\n",
      "Val Loss : 0.009284945949912072\n",
      "Epoch : 923\n",
      "Train Loss : 0.009550793142823562\n",
      "Epoch : 923\n",
      "Val Loss : 0.009285377129912377\n",
      "Epoch : 924\n",
      "Train Loss : 0.009550336706932647\n",
      "Epoch : 924\n",
      "Val Loss : 0.00928579293191433\n",
      "Epoch : 925\n",
      "Train Loss : 0.009549882900950853\n",
      "Epoch : 925\n",
      "Val Loss : 0.009286218956112861\n",
      "Epoch : 926\n",
      "Train Loss : 0.009549429075083169\n",
      "Epoch : 926\n",
      "Val Loss : 0.009286661237478256\n",
      "Epoch : 927\n",
      "Train Loss : 0.009548980557091295\n",
      "Epoch : 927\n",
      "Val Loss : 0.009287097990512848\n",
      "Epoch : 928\n",
      "Train Loss : 0.009548521953638232\n",
      "Epoch : 928\n",
      "Val Loss : 0.009287505224347115\n",
      "Epoch : 929\n",
      "Train Loss : 0.0095480735317615\n",
      "Epoch : 929\n",
      "Val Loss : 0.009287960439920426\n",
      "Epoch : 930\n",
      "Train Loss : 0.00954762130836522\n",
      "Epoch : 930\n",
      "Val Loss : 0.009288377404212952\n",
      "Epoch : 931\n",
      "Train Loss : 0.009547167963073241\n",
      "Epoch : 931\n",
      "Val Loss : 0.009288831651210784\n",
      "Epoch : 932\n",
      "Train Loss : 0.00954671308988195\n",
      "Epoch : 932\n",
      "Val Loss : 0.009289235725998879\n",
      "Epoch : 933\n",
      "Train Loss : 0.009546266194247373\n",
      "Epoch : 933\n",
      "Val Loss : 0.009289649724960327\n",
      "Epoch : 934\n",
      "Train Loss : 0.009545814668514445\n",
      "Epoch : 934\n",
      "Val Loss : 0.009290111437439918\n",
      "Epoch : 935\n",
      "Train Loss : 0.009545365532402785\n",
      "Epoch : 935\n",
      "Val Loss : 0.009290509834885597\n",
      "Epoch : 936\n",
      "Train Loss : 0.009544914621475328\n",
      "Epoch : 936\n",
      "Val Loss : 0.009290951713919639\n",
      "Epoch : 937\n",
      "Train Loss : 0.009544469556999906\n",
      "Epoch : 937\n",
      "Val Loss : 0.009291366875171661\n",
      "Epoch : 938\n",
      "Train Loss : 0.009544029053022216\n",
      "Epoch : 938\n",
      "Val Loss : 0.009291809901595116\n",
      "Epoch : 939\n",
      "Train Loss : 0.009543570265624659\n",
      "Epoch : 939\n",
      "Val Loss : 0.009292203903198243\n",
      "Epoch : 940\n",
      "Train Loss : 0.009543123073358872\n",
      "Epoch : 940\n",
      "Val Loss : 0.009292646169662475\n",
      "Epoch : 941\n",
      "Train Loss : 0.009542674303479041\n",
      "Epoch : 941\n",
      "Val Loss : 0.009293079733848571\n",
      "Epoch : 942\n",
      "Train Loss : 0.009542231915313153\n",
      "Epoch : 942\n",
      "Val Loss : 0.009293490558862687\n",
      "Epoch : 943\n",
      "Train Loss : 0.009541782674800565\n",
      "Epoch : 943\n",
      "Val Loss : 0.00929392023384571\n",
      "Epoch : 944\n",
      "Train Loss : 0.009541336287913373\n",
      "Epoch : 944\n",
      "Val Loss : 0.009294318497180938\n",
      "Epoch : 945\n",
      "Train Loss : 0.009540888716158493\n",
      "Epoch : 945\n",
      "Val Loss : 0.009294756159186363\n",
      "Epoch : 946\n",
      "Train Loss : 0.009540440834515142\n",
      "Epoch : 946\n",
      "Val Loss : 0.009295173600316047\n",
      "Epoch : 947\n",
      "Train Loss : 0.009540004413773793\n",
      "Epoch : 947\n",
      "Val Loss : 0.009295581921935081\n",
      "Epoch : 948\n",
      "Train Loss : 0.009539557345794826\n",
      "Epoch : 948\n",
      "Val Loss : 0.009296023473143577\n",
      "Epoch : 949\n",
      "Train Loss : 0.009539112528235886\n",
      "Epoch : 949\n",
      "Val Loss : 0.009296417623758315\n",
      "Epoch : 950\n",
      "Train Loss : 0.00953866924520489\n",
      "Epoch : 950\n",
      "Val Loss : 0.00929684929549694\n",
      "Epoch : 951\n",
      "Train Loss : 0.009538226253833635\n",
      "Epoch : 951\n",
      "Val Loss : 0.009297249749302863\n",
      "Epoch : 952\n",
      "Train Loss : 0.009537786737521878\n",
      "Epoch : 952\n",
      "Val Loss : 0.009297698393464088\n",
      "Epoch : 953\n",
      "Train Loss : 0.009537341134470235\n",
      "Epoch : 953\n",
      "Val Loss : 0.009298087820410729\n",
      "Epoch : 954\n",
      "Train Loss : 0.009536890185428153\n",
      "Epoch : 954\n",
      "Val Loss : 0.009298498347401618\n",
      "Epoch : 955\n",
      "Train Loss : 0.00953645612116492\n",
      "Epoch : 955\n",
      "Val Loss : 0.009298913881182671\n",
      "Epoch : 956\n",
      "Train Loss : 0.009536014134031174\n",
      "Epoch : 956\n",
      "Val Loss : 0.009299338594079018\n",
      "Epoch : 957\n",
      "Train Loss : 0.009535568993326502\n",
      "Epoch : 957\n",
      "Val Loss : 0.009299749597907067\n",
      "Epoch : 958\n",
      "Train Loss : 0.009535132322354355\n",
      "Epoch : 958\n",
      "Val Loss : 0.009300164207816123\n",
      "Epoch : 959\n",
      "Train Loss : 0.009534692211123017\n",
      "Epoch : 959\n",
      "Val Loss : 0.009300564974546432\n",
      "Epoch : 960\n",
      "Train Loss : 0.009534249148493984\n",
      "Epoch : 960\n",
      "Val Loss : 0.009300993993878364\n",
      "Epoch : 961\n",
      "Train Loss : 0.009533815689093278\n",
      "Epoch : 961\n",
      "Val Loss : 0.009301419287919999\n",
      "Epoch : 962\n",
      "Train Loss : 0.009533374697911252\n",
      "Epoch : 962\n",
      "Val Loss : 0.009301806956529617\n",
      "Epoch : 963\n",
      "Train Loss : 0.009532935110341718\n",
      "Epoch : 963\n",
      "Val Loss : 0.009302214369177818\n",
      "Epoch : 964\n",
      "Train Loss : 0.009532499811496068\n",
      "Epoch : 964\n",
      "Val Loss : 0.009302645385265351\n",
      "Epoch : 965\n",
      "Train Loss : 0.009532064923625506\n",
      "Epoch : 965\n",
      "Val Loss : 0.009303047254681587\n",
      "Epoch : 966\n",
      "Train Loss : 0.009531629997640317\n",
      "Epoch : 966\n",
      "Val Loss : 0.009303460329771042\n",
      "Epoch : 967\n",
      "Train Loss : 0.009531186797467202\n",
      "Epoch : 967\n",
      "Val Loss : 0.009303880214691163\n",
      "Epoch : 968\n",
      "Train Loss : 0.009530751911253797\n",
      "Epoch : 968\n",
      "Val Loss : 0.009304264351725579\n",
      "Epoch : 969\n",
      "Train Loss : 0.009530317212299202\n",
      "Epoch : 969\n",
      "Val Loss : 0.009304669007658958\n",
      "Epoch : 970\n",
      "Train Loss : 0.00952988199796859\n",
      "Epoch : 970\n",
      "Val Loss : 0.00930508954823017\n",
      "Epoch : 971\n",
      "Train Loss : 0.009529447580730788\n",
      "Epoch : 971\n",
      "Val Loss : 0.00930550193786621\n",
      "Epoch : 972\n",
      "Train Loss : 0.009529016376721584\n",
      "Epoch : 972\n",
      "Val Loss : 0.009305928230285644\n",
      "Epoch : 973\n",
      "Train Loss : 0.009528573135119529\n",
      "Epoch : 973\n",
      "Val Loss : 0.009306318119168282\n",
      "Epoch : 974\n",
      "Train Loss : 0.009528141420705781\n",
      "Epoch : 974\n",
      "Val Loss : 0.00930670565366745\n",
      "Epoch : 975\n",
      "Train Loss : 0.009527709467661339\n",
      "Epoch : 975\n",
      "Val Loss : 0.009307134464383126\n",
      "Epoch : 976\n",
      "Train Loss : 0.009527282283916486\n",
      "Epoch : 976\n",
      "Val Loss : 0.009307526350021363\n",
      "Epoch : 977\n",
      "Train Loss : 0.009526843786756656\n",
      "Epoch : 977\n",
      "Val Loss : 0.009307925954461099\n",
      "Epoch : 978\n",
      "Train Loss : 0.009526411469137542\n",
      "Epoch : 978\n",
      "Val Loss : 0.009308341830968857\n",
      "Epoch : 979\n",
      "Train Loss : 0.009525977787677716\n",
      "Epoch : 979\n",
      "Val Loss : 0.009308729723095894\n",
      "Epoch : 980\n",
      "Train Loss : 0.00952554770556421\n",
      "Epoch : 980\n",
      "Val Loss : 0.009309142932295799\n",
      "Epoch : 981\n",
      "Train Loss : 0.009525123287615402\n",
      "Epoch : 981\n",
      "Val Loss : 0.009309551283717155\n",
      "Epoch : 982\n",
      "Train Loss : 0.009524692211207334\n",
      "Epoch : 982\n",
      "Val Loss : 0.009309956416487694\n",
      "Epoch : 983\n",
      "Train Loss : 0.009524257709454493\n",
      "Epoch : 983\n",
      "Val Loss : 0.009310356587171555\n",
      "Epoch : 984\n",
      "Train Loss : 0.009523828046601863\n",
      "Epoch : 984\n",
      "Val Loss : 0.009310751527547836\n",
      "Epoch : 985\n",
      "Train Loss : 0.009523394602115574\n",
      "Epoch : 985\n",
      "Val Loss : 0.009311154708266258\n",
      "Epoch : 986\n",
      "Train Loss : 0.009522973992314946\n",
      "Epoch : 986\n",
      "Val Loss : 0.009311559036374092\n",
      "Epoch : 987\n",
      "Train Loss : 0.009522545622045247\n",
      "Epoch : 987\n",
      "Val Loss : 0.009311943128705025\n",
      "Epoch : 988\n",
      "Train Loss : 0.009522113814830674\n",
      "Epoch : 988\n",
      "Val Loss : 0.00931234610080719\n",
      "Epoch : 989\n",
      "Train Loss : 0.009521682675450616\n",
      "Epoch : 989\n",
      "Val Loss : 0.00931274202466011\n",
      "Epoch : 990\n",
      "Train Loss : 0.009521262804742279\n",
      "Epoch : 990\n",
      "Val Loss : 0.009313154056668282\n",
      "Epoch : 991\n",
      "Train Loss : 0.009520834424529636\n",
      "Epoch : 991\n",
      "Val Loss : 0.009313552036881447\n",
      "Epoch : 992\n",
      "Train Loss : 0.009520409182973498\n",
      "Epoch : 992\n",
      "Val Loss : 0.00931396196782589\n",
      "Epoch : 993\n",
      "Train Loss : 0.009519978800914465\n",
      "Epoch : 993\n",
      "Val Loss : 0.009314343094825745\n",
      "Epoch : 994\n",
      "Train Loss : 0.009519560499534382\n",
      "Epoch : 994\n",
      "Val Loss : 0.009314751103520393\n",
      "Epoch : 995\n",
      "Train Loss : 0.009519133009215372\n",
      "Epoch : 995\n",
      "Val Loss : 0.009315150544047355\n",
      "Epoch : 996\n",
      "Train Loss : 0.00951870628284602\n",
      "Epoch : 996\n",
      "Val Loss : 0.009315547108650207\n",
      "Epoch : 997\n",
      "Train Loss : 0.009518277677259941\n",
      "Epoch : 997\n",
      "Val Loss : 0.00931591123342514\n",
      "Epoch : 998\n",
      "Train Loss : 0.009517849214189418\n",
      "Epoch : 998\n",
      "Val Loss : 0.009316326573491096\n",
      "Epoch : 999\n",
      "Train Loss : 0.00951743617594189\n",
      "Epoch : 999\n",
      "Val Loss : 0.009316718682646752\n",
      "Epoch : 1000\n",
      "Train Loss : 0.009517012588229044\n",
      "Epoch : 1000\n",
      "Val Loss : 0.009317128032445908\n",
      "Epoch : 1001\n",
      "Train Loss : 0.009516587792448301\n",
      "Epoch : 1001\n",
      "Val Loss : 0.009317507892847061\n",
      "Epoch : 1002\n",
      "Train Loss : 0.009516172372865296\n",
      "Epoch : 1002\n",
      "Val Loss : 0.009317904278635979\n",
      "Epoch : 1003\n",
      "Train Loss : 0.00951574163286022\n",
      "Epoch : 1003\n",
      "Val Loss : 0.009318277835845948\n",
      "Epoch : 1004\n",
      "Train Loss : 0.009515322231127487\n",
      "Epoch : 1004\n",
      "Val Loss : 0.009318686291575432\n",
      "Epoch : 1005\n",
      "Train Loss : 0.009514904228035772\n",
      "Epoch : 1005\n",
      "Val Loss : 0.009319069281220437\n",
      "Epoch : 1006\n",
      "Train Loss : 0.009514480792781424\n",
      "Epoch : 1006\n",
      "Val Loss : 0.009319458112120628\n",
      "Epoch : 1007\n",
      "Train Loss : 0.009514064857822403\n",
      "Epoch : 1007\n",
      "Val Loss : 0.009319853842258453\n",
      "Epoch : 1008\n",
      "Train Loss : 0.009513642276004222\n",
      "Epoch : 1008\n",
      "Val Loss : 0.009320253059267997\n",
      "Epoch : 1009\n",
      "Train Loss : 0.009513221243628403\n",
      "Epoch : 1009\n",
      "Val Loss : 0.009320608079433442\n",
      "Epoch : 1010\n",
      "Train Loss : 0.009512801593322028\n",
      "Epoch : 1010\n",
      "Val Loss : 0.009321022599935532\n",
      "Epoch : 1011\n",
      "Train Loss : 0.009512383827203852\n",
      "Epoch : 1011\n",
      "Val Loss : 0.009321408763527871\n",
      "Epoch : 1012\n",
      "Train Loss : 0.009511967956873976\n",
      "Epoch : 1012\n",
      "Val Loss : 0.009321816235780716\n",
      "Epoch : 1013\n",
      "Train Loss : 0.009511554782739526\n",
      "Epoch : 1013\n",
      "Val Loss : 0.009322201684117317\n",
      "Epoch : 1014\n",
      "Train Loss : 0.009511132479323824\n",
      "Epoch : 1014\n",
      "Val Loss : 0.009322580993175506\n",
      "Epoch : 1015\n",
      "Train Loss : 0.00951071111717364\n",
      "Epoch : 1015\n",
      "Val Loss : 0.009322947040200234\n",
      "Epoch : 1016\n",
      "Train Loss : 0.009510293448827763\n",
      "Epoch : 1016\n",
      "Val Loss : 0.009323330774903297\n",
      "Epoch : 1017\n",
      "Train Loss : 0.009509880521609795\n",
      "Epoch : 1017\n",
      "Val Loss : 0.009323723211884499\n",
      "Epoch : 1018\n",
      "Train Loss : 0.00950946375144334\n",
      "Epoch : 1018\n",
      "Val Loss : 0.009324096113443375\n",
      "Epoch : 1019\n",
      "Train Loss : 0.009509045040745328\n",
      "Epoch : 1019\n",
      "Val Loss : 0.009324472099542618\n",
      "Epoch : 1020\n",
      "Train Loss : 0.009508638950959644\n",
      "Epoch : 1020\n",
      "Val Loss : 0.00932488127052784\n",
      "Epoch : 1021\n",
      "Train Loss : 0.009508218941050068\n",
      "Epoch : 1021\n",
      "Val Loss : 0.009325250804424286\n",
      "Epoch : 1022\n",
      "Train Loss : 0.009507806626980414\n",
      "Epoch : 1022\n",
      "Val Loss : 0.009325641721487045\n",
      "Epoch : 1023\n",
      "Train Loss : 0.009507391964718434\n",
      "Epoch : 1023\n",
      "Val Loss : 0.009326023131608963\n",
      "Epoch : 1024\n",
      "Train Loss : 0.00950698239987325\n",
      "Epoch : 1024\n",
      "Val Loss : 0.009326409727334976\n",
      "Epoch : 1025\n",
      "Train Loss : 0.009506566614058413\n",
      "Epoch : 1025\n",
      "Val Loss : 0.009326788768172264\n",
      "Epoch : 1026\n",
      "Train Loss : 0.00950615465296333\n",
      "Epoch : 1026\n",
      "Val Loss : 0.009327184721827506\n",
      "Epoch : 1027\n",
      "Train Loss : 0.009505743253644675\n",
      "Epoch : 1027\n",
      "Val Loss : 0.009327546194195747\n",
      "Epoch : 1028\n",
      "Train Loss : 0.009505327822461565\n",
      "Epoch : 1028\n",
      "Val Loss : 0.009327919512987138\n",
      "Epoch : 1029\n",
      "Train Loss : 0.009504915819937542\n",
      "Epoch : 1029\n",
      "Val Loss : 0.009328295111656188\n",
      "Epoch : 1030\n",
      "Train Loss : 0.009504507121785785\n",
      "Epoch : 1030\n",
      "Val Loss : 0.009328655108809472\n",
      "Epoch : 1031\n",
      "Train Loss : 0.009504094971774736\n",
      "Epoch : 1031\n",
      "Val Loss : 0.009329049438238144\n",
      "Epoch : 1032\n",
      "Train Loss : 0.009503686837056565\n",
      "Epoch : 1032\n",
      "Val Loss : 0.009329419866204261\n",
      "Epoch : 1033\n",
      "Train Loss : 0.00950328261654466\n",
      "Epoch : 1033\n",
      "Val Loss : 0.009329810664057732\n",
      "Epoch : 1034\n",
      "Train Loss : 0.009502873197529346\n",
      "Epoch : 1034\n",
      "Val Loss : 0.009330205917358399\n",
      "Epoch : 1035\n",
      "Train Loss : 0.009502459420189527\n",
      "Epoch : 1035\n",
      "Val Loss : 0.00933058224618435\n",
      "Epoch : 1036\n",
      "Train Loss : 0.009502061468704845\n",
      "Epoch : 1036\n",
      "Val Loss : 0.009330960392951965\n",
      "Epoch : 1037\n",
      "Train Loss : 0.00950163931277617\n",
      "Epoch : 1037\n",
      "Val Loss : 0.009331307649612427\n",
      "Epoch : 1038\n",
      "Train Loss : 0.009501239803563339\n",
      "Epoch : 1038\n",
      "Val Loss : 0.009331683054566383\n",
      "Epoch : 1039\n",
      "Train Loss : 0.009500832900113271\n",
      "Epoch : 1039\n",
      "Val Loss : 0.009332073718309403\n",
      "Epoch : 1040\n",
      "Train Loss : 0.009500422783434603\n",
      "Epoch : 1040\n",
      "Val Loss : 0.009332427486777305\n",
      "Epoch : 1041\n",
      "Train Loss : 0.009500013856595097\n",
      "Epoch : 1041\n",
      "Val Loss : 0.00933280549943447\n",
      "Epoch : 1042\n",
      "Train Loss : 0.00949961197764689\n",
      "Epoch : 1042\n",
      "Val Loss : 0.009333177804946899\n",
      "Epoch : 1043\n",
      "Train Loss : 0.009499204528991969\n",
      "Epoch : 1043\n",
      "Val Loss : 0.009333549976348877\n",
      "Epoch : 1044\n",
      "Train Loss : 0.00949880476789118\n",
      "Epoch : 1044\n",
      "Val Loss : 0.00933393009006977\n",
      "Epoch : 1045\n",
      "Train Loss : 0.009498397009347787\n",
      "Epoch : 1045\n",
      "Val Loss : 0.009334307625889778\n",
      "Epoch : 1046\n",
      "Train Loss : 0.009497994401250234\n",
      "Epoch : 1046\n",
      "Val Loss : 0.009334688395261765\n",
      "Epoch : 1047\n",
      "Train Loss : 0.009497591421949376\n",
      "Epoch : 1047\n",
      "Val Loss : 0.009335045874118805\n",
      "Epoch : 1048\n",
      "Train Loss : 0.009497184286497241\n",
      "Epoch : 1048\n",
      "Val Loss : 0.009335427761077882\n",
      "Epoch : 1049\n",
      "Train Loss : 0.009496787794968412\n",
      "Epoch : 1049\n",
      "Val Loss : 0.009335802048444747\n",
      "Epoch : 1050\n",
      "Train Loss : 0.009496382691191503\n",
      "Epoch : 1050\n",
      "Val Loss : 0.00933617551624775\n",
      "Epoch : 1051\n",
      "Train Loss : 0.009495983448781047\n",
      "Epoch : 1051\n",
      "Val Loss : 0.009336518615484238\n",
      "Epoch : 1052\n",
      "Train Loss : 0.009495583386077575\n",
      "Epoch : 1052\n",
      "Val Loss : 0.009336908161640168\n",
      "Epoch : 1053\n",
      "Train Loss : 0.009495173058939892\n",
      "Epoch : 1053\n",
      "Val Loss : 0.00933726567029953\n",
      "Epoch : 1054\n",
      "Train Loss : 0.00949477696347173\n",
      "Epoch : 1054\n",
      "Val Loss : 0.009337622985243797\n",
      "Epoch : 1055\n",
      "Train Loss : 0.009494377860262512\n",
      "Epoch : 1055\n",
      "Val Loss : 0.009337988704442978\n",
      "Epoch : 1056\n",
      "Train Loss : 0.009493977769387362\n",
      "Epoch : 1056\n",
      "Val Loss : 0.009338374868035317\n",
      "Epoch : 1057\n",
      "Train Loss : 0.00949358107899962\n",
      "Epoch : 1057\n",
      "Val Loss : 0.009338742911815643\n",
      "Epoch : 1058\n",
      "Train Loss : 0.009493182989970856\n",
      "Epoch : 1058\n",
      "Val Loss : 0.00933911393582821\n",
      "Epoch : 1059\n",
      "Train Loss : 0.009492780577417901\n",
      "Epoch : 1059\n",
      "Val Loss : 0.009339463531970977\n",
      "Epoch : 1060\n",
      "Train Loss : 0.009492379961223789\n",
      "Epoch : 1060\n",
      "Val Loss : 0.009339826360344887\n",
      "Epoch : 1061\n",
      "Train Loss : 0.009491988565454598\n",
      "Epoch : 1061\n",
      "Val Loss : 0.00934020359814167\n",
      "Epoch : 1062\n",
      "Train Loss : 0.009491590464825733\n",
      "Epoch : 1062\n",
      "Val Loss : 0.009340554893016816\n",
      "Epoch : 1063\n",
      "Train Loss : 0.009491194018040158\n",
      "Epoch : 1063\n",
      "Val Loss : 0.009340920880436898\n",
      "Epoch : 1064\n",
      "Train Loss : 0.009490792634582074\n",
      "Epoch : 1064\n",
      "Val Loss : 0.009341271877288818\n",
      "Epoch : 1065\n",
      "Train Loss : 0.009490401049896915\n",
      "Epoch : 1065\n",
      "Val Loss : 0.009341641888022422\n",
      "Epoch : 1066\n",
      "Train Loss : 0.009490001300396232\n",
      "Epoch : 1066\n",
      "Val Loss : 0.009342010378837586\n",
      "Epoch : 1067\n",
      "Train Loss : 0.00948960608985023\n",
      "Epoch : 1067\n",
      "Val Loss : 0.009342372000217437\n",
      "Epoch : 1068\n",
      "Train Loss : 0.00948921040204284\n",
      "Epoch : 1068\n",
      "Val Loss : 0.009342730134725571\n",
      "Epoch : 1069\n",
      "Train Loss : 0.009488816426079256\n",
      "Epoch : 1069\n",
      "Val Loss : 0.009343084901571274\n",
      "Epoch : 1070\n",
      "Train Loss : 0.009488424031044029\n",
      "Epoch : 1070\n",
      "Val Loss : 0.009343443617224693\n",
      "Epoch : 1071\n",
      "Train Loss : 0.009488029735249027\n",
      "Epoch : 1071\n",
      "Val Loss : 0.009343805238604546\n",
      "Epoch : 1072\n",
      "Train Loss : 0.009487642841977052\n",
      "Epoch : 1072\n",
      "Val Loss : 0.009344172030687332\n",
      "Epoch : 1073\n",
      "Train Loss : 0.009487240721083832\n",
      "Epoch : 1073\n",
      "Val Loss : 0.009344503179192543\n",
      "Epoch : 1074\n",
      "Train Loss : 0.009486849375029369\n",
      "Epoch : 1074\n",
      "Val Loss : 0.009344867467880248\n",
      "Epoch : 1075\n",
      "Train Loss : 0.00948645327127542\n",
      "Epoch : 1075\n",
      "Val Loss : 0.009345223680138588\n",
      "Epoch : 1076\n",
      "Train Loss : 0.00948606737064085\n",
      "Epoch : 1076\n",
      "Val Loss : 0.009345562547445297\n",
      "Epoch : 1077\n",
      "Train Loss : 0.009485679396902116\n",
      "Epoch : 1077\n",
      "Val Loss : 0.009345936074852944\n",
      "Epoch : 1078\n",
      "Train Loss : 0.009485286769864822\n",
      "Epoch : 1078\n",
      "Val Loss : 0.009346292540431022\n",
      "Epoch : 1079\n",
      "Train Loss : 0.009484895050949897\n",
      "Epoch : 1079\n",
      "Val Loss : 0.00934663699567318\n",
      "Epoch : 1080\n",
      "Train Loss : 0.009484509702148812\n",
      "Epoch : 1080\n",
      "Val Loss : 0.0093469889909029\n",
      "Epoch : 1081\n",
      "Train Loss : 0.009484121204748272\n",
      "Epoch : 1081\n",
      "Val Loss : 0.009347347706556321\n",
      "Epoch : 1082\n",
      "Train Loss : 0.009483731252363356\n",
      "Epoch : 1082\n",
      "Val Loss : 0.009347710326313973\n",
      "Epoch : 1083\n",
      "Train Loss : 0.009483339210302697\n",
      "Epoch : 1083\n",
      "Val Loss : 0.009348062172532081\n",
      "Epoch : 1084\n",
      "Train Loss : 0.009482954378534785\n",
      "Epoch : 1084\n",
      "Val Loss : 0.009348404929041862\n",
      "Epoch : 1085\n",
      "Train Loss : 0.009482559466277154\n",
      "Epoch : 1085\n",
      "Val Loss : 0.009348734617233276\n",
      "Epoch : 1086\n",
      "Train Loss : 0.009482175247657553\n",
      "Epoch : 1086\n",
      "Val Loss : 0.00934910735487938\n",
      "Epoch : 1087\n",
      "Train Loss : 0.009481786120537124\n",
      "Epoch : 1087\n",
      "Val Loss : 0.00934945148229599\n",
      "Epoch : 1088\n",
      "Train Loss : 0.009481406026582807\n",
      "Epoch : 1088\n",
      "Val Loss : 0.009349815666675568\n",
      "Epoch : 1089\n",
      "Train Loss : 0.009481021759905638\n",
      "Epoch : 1089\n",
      "Val Loss : 0.00935016368329525\n",
      "Epoch : 1090\n",
      "Train Loss : 0.009480634977663221\n",
      "Epoch : 1090\n",
      "Val Loss : 0.00935052977502346\n",
      "Epoch : 1091\n",
      "Train Loss : 0.009480246624435393\n",
      "Epoch : 1091\n",
      "Val Loss : 0.009350841104984284\n",
      "Epoch : 1092\n",
      "Train Loss : 0.009479866976256473\n",
      "Epoch : 1092\n",
      "Val Loss : 0.009351207554340362\n",
      "Epoch : 1093\n",
      "Train Loss : 0.009479480263614675\n",
      "Epoch : 1093\n",
      "Val Loss : 0.00935155214369297\n",
      "Epoch : 1094\n",
      "Train Loss : 0.009479106486745152\n",
      "Epoch : 1094\n",
      "Val Loss : 0.00935190324485302\n",
      "Epoch : 1095\n",
      "Train Loss : 0.009478717899858102\n",
      "Epoch : 1095\n",
      "Val Loss : 0.0093522639721632\n",
      "Epoch : 1096\n",
      "Train Loss : 0.00947833242345588\n",
      "Epoch : 1096\n",
      "Val Loss : 0.009352598950266838\n",
      "Epoch : 1097\n",
      "Train Loss : 0.009477952142242754\n",
      "Epoch : 1097\n",
      "Val Loss : 0.009352956965565682\n",
      "Epoch : 1098\n",
      "Train Loss : 0.009477563404554362\n",
      "Epoch : 1098\n",
      "Val Loss : 0.009353271707892418\n",
      "Epoch : 1099\n",
      "Train Loss : 0.00947718566044953\n",
      "Epoch : 1099\n",
      "Val Loss : 0.009353623986244201\n",
      "Epoch : 1100\n",
      "Train Loss : 0.009476804094939258\n",
      "Epoch : 1100\n",
      "Val Loss : 0.009353967979550362\n",
      "Epoch : 1101\n",
      "Train Loss : 0.009476428632740449\n",
      "Epoch : 1101\n",
      "Val Loss : 0.00935431595146656\n",
      "Epoch : 1102\n",
      "Train Loss : 0.009476039623278209\n",
      "Epoch : 1102\n",
      "Val Loss : 0.009354633748531342\n",
      "Epoch : 1103\n",
      "Train Loss : 0.00947566317175631\n",
      "Epoch : 1103\n",
      "Val Loss : 0.009354977235198021\n",
      "Epoch : 1104\n",
      "Train Loss : 0.00947528859613682\n",
      "Epoch : 1104\n",
      "Val Loss : 0.009355344533920288\n",
      "Epoch : 1105\n",
      "Train Loss : 0.009474906195419115\n",
      "Epoch : 1105\n",
      "Val Loss : 0.00935567158460617\n",
      "Epoch : 1106\n",
      "Train Loss : 0.009474526464382313\n",
      "Epoch : 1106\n",
      "Val Loss : 0.00935600484907627\n",
      "Epoch : 1107\n",
      "Train Loss : 0.009474148395474589\n",
      "Epoch : 1107\n",
      "Val Loss : 0.009356359839439393\n",
      "Epoch : 1108\n",
      "Train Loss : 0.009473770265252035\n",
      "Epoch : 1108\n",
      "Val Loss : 0.009356684461236\n",
      "Epoch : 1109\n",
      "Train Loss : 0.009473397096559352\n",
      "Epoch : 1109\n",
      "Val Loss : 0.0093570327013731\n",
      "Epoch : 1110\n",
      "Train Loss : 0.009473020931725718\n",
      "Epoch : 1110\n",
      "Val Loss : 0.009357370808720589\n",
      "Epoch : 1111\n",
      "Train Loss : 0.009472639964449341\n",
      "Epoch : 1111\n",
      "Val Loss : 0.00935769209265709\n",
      "Epoch : 1112\n",
      "Train Loss : 0.009472258611055244\n",
      "Epoch : 1112\n",
      "Val Loss : 0.009358025670051574\n",
      "Epoch : 1113\n",
      "Train Loss : 0.009471888784849452\n",
      "Epoch : 1113\n",
      "Val Loss : 0.009358359053730964\n",
      "Epoch : 1114\n",
      "Train Loss : 0.009471519689450167\n",
      "Epoch : 1114\n",
      "Val Loss : 0.009358706206083298\n",
      "Epoch : 1115\n",
      "Train Loss : 0.009471150285819565\n",
      "Epoch : 1115\n",
      "Val Loss : 0.009359050676226615\n",
      "Epoch : 1116\n",
      "Train Loss : 0.009470773431608367\n",
      "Epoch : 1116\n",
      "Val Loss : 0.009359386920928955\n",
      "Epoch : 1117\n",
      "Train Loss : 0.009470403273971055\n",
      "Epoch : 1117\n",
      "Val Loss : 0.009359751403331756\n",
      "Epoch : 1118\n",
      "Train Loss : 0.009470024253854866\n",
      "Epoch : 1118\n",
      "Val Loss : 0.009360050842165946\n",
      "Epoch : 1119\n",
      "Train Loss : 0.00946965067252994\n",
      "Epoch : 1119\n",
      "Val Loss : 0.009360379695892334\n",
      "Epoch : 1120\n",
      "Train Loss : 0.00946927580690787\n",
      "Epoch : 1120\n",
      "Val Loss : 0.009360721945762634\n",
      "Epoch : 1121\n",
      "Train Loss : 0.009468903337535696\n",
      "Epoch : 1121\n",
      "Val Loss : 0.009361052930355072\n",
      "Epoch : 1122\n",
      "Train Loss : 0.00946853261480764\n",
      "Epoch : 1122\n",
      "Val Loss : 0.00936136668920517\n",
      "Epoch : 1123\n",
      "Train Loss : 0.009468166711093904\n",
      "Epoch : 1123\n",
      "Val Loss : 0.009361711695790292\n",
      "Epoch : 1124\n",
      "Train Loss : 0.00946779040871619\n",
      "Epoch : 1124\n",
      "Val Loss : 0.009362035393714906\n",
      "Epoch : 1125\n",
      "Train Loss : 0.009467418557463804\n",
      "Epoch : 1125\n",
      "Val Loss : 0.009362355902791024\n",
      "Epoch : 1126\n",
      "Train Loss : 0.009467052778036887\n",
      "Epoch : 1126\n",
      "Val Loss : 0.009362685352563857\n",
      "Epoch : 1127\n",
      "Train Loss : 0.009466687369813273\n",
      "Epoch : 1127\n",
      "Val Loss : 0.009363018110394477\n",
      "Epoch : 1128\n",
      "Train Loss : 0.009466321154553908\n",
      "Epoch : 1128\n",
      "Val Loss : 0.009363352552056312\n",
      "Epoch : 1129\n",
      "Train Loss : 0.009465946703221238\n",
      "Epoch : 1129\n",
      "Val Loss : 0.009363680750131607\n",
      "Epoch : 1130\n",
      "Train Loss : 0.009465587477852653\n",
      "Epoch : 1130\n",
      "Val Loss : 0.00936401829123497\n",
      "Epoch : 1131\n",
      "Train Loss : 0.009465218442111041\n",
      "Epoch : 1131\n",
      "Val Loss : 0.0093643389493227\n",
      "Epoch : 1132\n",
      "Train Loss : 0.009464851487759381\n",
      "Epoch : 1132\n",
      "Val Loss : 0.00936464673280716\n",
      "Epoch : 1133\n",
      "Train Loss : 0.009464478814556823\n",
      "Epoch : 1133\n",
      "Val Loss : 0.009364968791604042\n",
      "Epoch : 1134\n",
      "Train Loss : 0.00946412057851133\n",
      "Epoch : 1134\n",
      "Val Loss : 0.00936532923579216\n",
      "Epoch : 1135\n",
      "Train Loss : 0.009463755876236856\n",
      "Epoch : 1135\n",
      "Val Loss : 0.009365652322769164\n",
      "Epoch : 1136\n",
      "Train Loss : 0.0094633893361746\n",
      "Epoch : 1136\n",
      "Val Loss : 0.009365970358252526\n",
      "Epoch : 1137\n",
      "Train Loss : 0.009463025967911999\n",
      "Epoch : 1137\n",
      "Val Loss : 0.009366288870573045\n",
      "Epoch : 1138\n",
      "Train Loss : 0.009462663975090213\n",
      "Epoch : 1138\n",
      "Val Loss : 0.00936660259962082\n",
      "Epoch : 1139\n",
      "Train Loss : 0.009462294075969487\n",
      "Epoch : 1139\n",
      "Val Loss : 0.009366917505860328\n",
      "Epoch : 1140\n",
      "Train Loss : 0.009461934774371654\n",
      "Epoch : 1140\n",
      "Val Loss : 0.009367232903838158\n",
      "Epoch : 1141\n",
      "Train Loss : 0.009461573490813321\n",
      "Epoch : 1141\n",
      "Val Loss : 0.00936757142841816\n",
      "Epoch : 1142\n",
      "Train Loss : 0.00946121381966934\n",
      "Epoch : 1142\n",
      "Val Loss : 0.009367881670594215\n",
      "Epoch : 1143\n",
      "Train Loss : 0.009460841242581924\n",
      "Epoch : 1143\n",
      "Val Loss : 0.009368187978863716\n",
      "Epoch : 1144\n",
      "Train Loss : 0.009460483212023973\n",
      "Epoch : 1144\n",
      "Val Loss : 0.00936850269138813\n",
      "Epoch : 1145\n",
      "Train Loss : 0.009460118861066914\n",
      "Epoch : 1145\n",
      "Val Loss : 0.009368819341063499\n",
      "Epoch : 1146\n",
      "Train Loss : 0.00945976187120394\n",
      "Epoch : 1146\n",
      "Val Loss : 0.00936913488805294\n",
      "Epoch : 1147\n",
      "Train Loss : 0.009459412920212512\n",
      "Epoch : 1147\n",
      "Val Loss : 0.009369471594691276\n",
      "Epoch : 1148\n",
      "Train Loss : 0.009459051175964366\n",
      "Epoch : 1148\n",
      "Val Loss : 0.009369785636663438\n",
      "Epoch : 1149\n",
      "Train Loss : 0.009458690445896676\n",
      "Epoch : 1149\n",
      "Val Loss : 0.00937010082602501\n",
      "Epoch : 1150\n",
      "Train Loss : 0.009458334604443923\n",
      "Epoch : 1150\n",
      "Val Loss : 0.009370410025119782\n",
      "Epoch : 1151\n",
      "Train Loss : 0.009457972604993505\n",
      "Epoch : 1151\n",
      "Val Loss : 0.009370729297399521\n",
      "Epoch : 1152\n",
      "Train Loss : 0.009457620591574822\n",
      "Epoch : 1152\n",
      "Val Loss : 0.009371022805571555\n",
      "Epoch : 1153\n",
      "Train Loss : 0.009457259082643056\n",
      "Epoch : 1153\n",
      "Val Loss : 0.009371353715658188\n",
      "Epoch : 1154\n",
      "Train Loss : 0.009456906870365461\n",
      "Epoch : 1154\n",
      "Val Loss : 0.009371660381555557\n",
      "Epoch : 1155\n",
      "Train Loss : 0.009456551640403864\n",
      "Epoch : 1155\n",
      "Val Loss : 0.009371954873204231\n",
      "Epoch : 1156\n",
      "Train Loss : 0.009456192315605823\n",
      "Epoch : 1156\n",
      "Val Loss : 0.009372278586030007\n",
      "Epoch : 1157\n",
      "Train Loss : 0.009455840209386314\n",
      "Epoch : 1157\n",
      "Val Loss : 0.009372594267129898\n",
      "Epoch : 1158\n",
      "Train Loss : 0.009455488742829641\n",
      "Epoch : 1158\n",
      "Val Loss : 0.009372912853956222\n",
      "Epoch : 1159\n",
      "Train Loss : 0.009455136399636596\n",
      "Epoch : 1159\n",
      "Val Loss : 0.009373227208852768\n",
      "Epoch : 1160\n",
      "Train Loss : 0.009454786949840728\n",
      "Epoch : 1160\n",
      "Val Loss : 0.009373552277684211\n",
      "Epoch : 1161\n",
      "Train Loss : 0.009454424395242514\n",
      "Epoch : 1161\n",
      "Val Loss : 0.00937381061911583\n",
      "Epoch : 1162\n",
      "Train Loss : 0.009454071027926067\n",
      "Epoch : 1162\n",
      "Val Loss : 0.00937411731481552\n",
      "Epoch : 1163\n",
      "Train Loss : 0.009453718769248057\n",
      "Epoch : 1163\n",
      "Val Loss : 0.009374437779188156\n",
      "Epoch : 1164\n",
      "Train Loss : 0.009453367299377069\n",
      "Epoch : 1164\n",
      "Val Loss : 0.00937472254037857\n",
      "Epoch : 1165\n",
      "Train Loss : 0.009453015950478587\n",
      "Epoch : 1165\n",
      "Val Loss : 0.009375034600496292\n",
      "Epoch : 1166\n",
      "Train Loss : 0.009452672580793978\n",
      "Epoch : 1166\n",
      "Val Loss : 0.009375339329242706\n",
      "Epoch : 1167\n",
      "Train Loss : 0.009452324392095049\n",
      "Epoch : 1167\n",
      "Val Loss : 0.009375651642680168\n",
      "Epoch : 1168\n",
      "Train Loss : 0.009451974337436657\n",
      "Epoch : 1168\n",
      "Val Loss : 0.00937596409022808\n",
      "Epoch : 1169\n",
      "Train Loss : 0.009451627080060303\n",
      "Epoch : 1169\n",
      "Val Loss : 0.009376279652118683\n",
      "Epoch : 1170\n",
      "Train Loss : 0.009451276070879129\n",
      "Epoch : 1170\n",
      "Val Loss : 0.009376558125019073\n",
      "Epoch : 1171\n",
      "Train Loss : 0.00945092685142817\n",
      "Epoch : 1171\n",
      "Val Loss : 0.009376856461167336\n",
      "Epoch : 1172\n",
      "Train Loss : 0.009450583748545936\n",
      "Epoch : 1172\n",
      "Val Loss : 0.009377177715301513\n",
      "Epoch : 1173\n",
      "Train Loss : 0.009450238352157574\n",
      "Epoch : 1173\n",
      "Val Loss : 0.009377446234226226\n",
      "Epoch : 1174\n",
      "Train Loss : 0.009449881350694496\n",
      "Epoch : 1174\n",
      "Val Loss : 0.00937773311138153\n",
      "Epoch : 1175\n",
      "Train Loss : 0.0094495360885359\n",
      "Epoch : 1175\n",
      "Val Loss : 0.009378035828471184\n",
      "Epoch : 1176\n",
      "Train Loss : 0.009449191857129335\n",
      "Epoch : 1176\n",
      "Val Loss : 0.009378334254026413\n",
      "Epoch : 1177\n",
      "Train Loss : 0.00944885054729163\n",
      "Epoch : 1177\n",
      "Val Loss : 0.00937862753868103\n",
      "Epoch : 1178\n",
      "Train Loss : 0.009448512389367692\n",
      "Epoch : 1178\n",
      "Val Loss : 0.009378913804888726\n",
      "Epoch : 1179\n",
      "Train Loss : 0.00944816499941873\n",
      "Epoch : 1179\n",
      "Val Loss : 0.009379214927554131\n",
      "Epoch : 1180\n",
      "Train Loss : 0.009447818027073484\n",
      "Epoch : 1180\n",
      "Val Loss : 0.009379510462284088\n",
      "Epoch : 1181\n",
      "Train Loss : 0.00944746961465828\n",
      "Epoch : 1181\n",
      "Val Loss : 0.009379778295755386\n",
      "Epoch : 1182\n",
      "Train Loss : 0.009447129842662832\n",
      "Epoch : 1182\n",
      "Val Loss : 0.009380077853798865\n",
      "Epoch : 1183\n",
      "Train Loss : 0.009446790281126402\n",
      "Epoch : 1183\n",
      "Val Loss : 0.00938037732243538\n",
      "Epoch : 1184\n",
      "Train Loss : 0.00944644902431774\n",
      "Epoch : 1184\n",
      "Val Loss : 0.009380660831928253\n",
      "Epoch : 1185\n",
      "Train Loss : 0.009446107146074975\n",
      "Epoch : 1185\n",
      "Val Loss : 0.009380941957235336\n",
      "Epoch : 1186\n",
      "Train Loss : 0.009445770807710089\n",
      "Epoch : 1186\n",
      "Val Loss : 0.009381236091256141\n",
      "Epoch : 1187\n",
      "Train Loss : 0.00944543346676485\n",
      "Epoch : 1187\n",
      "Val Loss : 0.009381534427404404\n",
      "Epoch : 1188\n",
      "Train Loss : 0.009445089886621225\n",
      "Epoch : 1188\n",
      "Val Loss : 0.009381811484694481\n",
      "Epoch : 1189\n",
      "Train Loss : 0.009444752234130355\n",
      "Epoch : 1189\n",
      "Val Loss : 0.00938211826980114\n",
      "Epoch : 1190\n",
      "Train Loss : 0.009444416064795545\n",
      "Epoch : 1190\n",
      "Val Loss : 0.009382397264242172\n",
      "Epoch : 1191\n",
      "Train Loss : 0.009444080119177009\n",
      "Epoch : 1191\n",
      "Val Loss : 0.009382686972618104\n",
      "Epoch : 1192\n",
      "Train Loss : 0.009443743706240031\n",
      "Epoch : 1192\n",
      "Val Loss : 0.009382984191179276\n",
      "Epoch : 1193\n",
      "Train Loss : 0.009443407616448784\n",
      "Epoch : 1193\n",
      "Val Loss : 0.009383253693580628\n",
      "Epoch : 1194\n",
      "Train Loss : 0.009443077595168691\n",
      "Epoch : 1194\n",
      "Val Loss : 0.009383529350161552\n",
      "Epoch : 1195\n",
      "Train Loss : 0.009442739380901394\n",
      "Epoch : 1195\n",
      "Val Loss : 0.00938381265103817\n",
      "Epoch : 1196\n",
      "Train Loss : 0.009442408845902443\n",
      "Epoch : 1196\n",
      "Val Loss : 0.009384093463420867\n",
      "Epoch : 1197\n",
      "Train Loss : 0.009442073351030778\n",
      "Epoch : 1197\n",
      "Val Loss : 0.009384373933076859\n",
      "Epoch : 1198\n",
      "Train Loss : 0.00944173547648079\n",
      "Epoch : 1198\n",
      "Val Loss : 0.009384637385606766\n",
      "Epoch : 1199\n",
      "Train Loss : 0.009441404813880705\n",
      "Epoch : 1199\n",
      "Val Loss : 0.0093849338889122\n",
      "Epoch : 1200\n",
      "Train Loss : 0.00944107259189531\n",
      "Epoch : 1200\n",
      "Val Loss : 0.00938520248234272\n",
      "Epoch : 1201\n",
      "Train Loss : 0.009440734225169514\n",
      "Epoch : 1201\n",
      "Val Loss : 0.009385463923215867\n",
      "Epoch : 1202\n",
      "Train Loss : 0.009440402341244271\n",
      "Epoch : 1202\n",
      "Val Loss : 0.00938574317097664\n",
      "Epoch : 1203\n",
      "Train Loss : 0.009440077374294878\n",
      "Epoch : 1203\n",
      "Val Loss : 0.009386019796133042\n",
      "Epoch : 1204\n",
      "Train Loss : 0.009439745934487874\n",
      "Epoch : 1204\n",
      "Val Loss : 0.009386311292648315\n",
      "Epoch : 1205\n",
      "Train Loss : 0.009439412338718825\n",
      "Epoch : 1205\n",
      "Val Loss : 0.009386548772454262\n",
      "Epoch : 1206\n",
      "Train Loss : 0.009439080756396268\n",
      "Epoch : 1206\n",
      "Val Loss : 0.009386829122900963\n",
      "Epoch : 1207\n",
      "Train Loss : 0.009438759842854378\n",
      "Epoch : 1207\n",
      "Val Loss : 0.009387096986174584\n",
      "Epoch : 1208\n",
      "Train Loss : 0.009438425741652259\n",
      "Epoch : 1208\n",
      "Val Loss : 0.009387369349598884\n",
      "Epoch : 1209\n",
      "Train Loss : 0.009438102796435144\n",
      "Epoch : 1209\n",
      "Val Loss : 0.009387644216418266\n",
      "Epoch : 1210\n",
      "Train Loss : 0.009437774679229141\n",
      "Epoch : 1210\n",
      "Val Loss : 0.009387905359268188\n",
      "Epoch : 1211\n",
      "Train Loss : 0.009437443323937176\n",
      "Epoch : 1211\n",
      "Val Loss : 0.009388162091374398\n",
      "Epoch : 1212\n",
      "Train Loss : 0.009437116429713486\n",
      "Epoch : 1212\n",
      "Val Loss : 0.009388449028134346\n",
      "Epoch : 1213\n",
      "Train Loss : 0.009436800302042761\n",
      "Epoch : 1213\n",
      "Val Loss : 0.009388708457350731\n",
      "Epoch : 1214\n",
      "Train Loss : 0.009436475108062775\n",
      "Epoch : 1214\n",
      "Val Loss : 0.009388974964618683\n",
      "Epoch : 1215\n",
      "Train Loss : 0.009436154390065484\n",
      "Epoch : 1215\n",
      "Val Loss : 0.009389227166771888\n",
      "Epoch : 1216\n",
      "Train Loss : 0.009435821143956689\n",
      "Epoch : 1216\n",
      "Val Loss : 0.009389481142163276\n",
      "Epoch : 1217\n",
      "Train Loss : 0.009435501723513803\n",
      "Epoch : 1217\n",
      "Val Loss : 0.009389743462204933\n",
      "Epoch : 1218\n",
      "Train Loss : 0.009435185194810939\n",
      "Epoch : 1218\n",
      "Val Loss : 0.009390019923448562\n",
      "Epoch : 1219\n",
      "Train Loss : 0.009434863384746785\n",
      "Epoch : 1219\n",
      "Val Loss : 0.009390267550945281\n",
      "Epoch : 1220\n",
      "Train Loss : 0.009434541662511982\n",
      "Epoch : 1220\n",
      "Val Loss : 0.009390528604388238\n",
      "Epoch : 1221\n",
      "Train Loss : 0.009434221957037988\n",
      "Epoch : 1221\n",
      "Val Loss : 0.009390806674957276\n",
      "Epoch : 1222\n",
      "Train Loss : 0.009433905542678998\n",
      "Epoch : 1222\n",
      "Val Loss : 0.009391046777367592\n",
      "Epoch : 1223\n",
      "Train Loss : 0.00943358491913969\n",
      "Epoch : 1223\n",
      "Val Loss : 0.009391275629401208\n",
      "Epoch : 1224\n",
      "Train Loss : 0.009433257227823191\n",
      "Epoch : 1224\n",
      "Val Loss : 0.009391537904739379\n",
      "Epoch : 1225\n",
      "Train Loss : 0.009432943247828725\n",
      "Epoch : 1225\n",
      "Val Loss : 0.009391812935471535\n",
      "Epoch : 1226\n",
      "Train Loss : 0.009432626533524209\n",
      "Epoch : 1226\n",
      "Val Loss : 0.009392063304781914\n",
      "Epoch : 1227\n",
      "Train Loss : 0.009432312720902662\n",
      "Epoch : 1227\n",
      "Val Loss : 0.009392313525080681\n",
      "Epoch : 1228\n",
      "Train Loss : 0.009431999075654032\n",
      "Epoch : 1228\n",
      "Val Loss : 0.009392552942037582\n",
      "Epoch : 1229\n",
      "Train Loss : 0.009431681759801306\n",
      "Epoch : 1229\n",
      "Val Loss : 0.009392803058028221\n",
      "Epoch : 1230\n",
      "Train Loss : 0.009431359565276586\n",
      "Epoch : 1230\n",
      "Val Loss : 0.00939304706454277\n",
      "Epoch : 1231\n",
      "Train Loss : 0.009431053526381369\n",
      "Epoch : 1231\n",
      "Val Loss : 0.009393291145563126\n",
      "Epoch : 1232\n",
      "Train Loss : 0.00943074611038818\n",
      "Epoch : 1232\n",
      "Val Loss : 0.009393547713756561\n",
      "Epoch : 1233\n",
      "Train Loss : 0.009430430607465875\n",
      "Epoch : 1233\n",
      "Val Loss : 0.009393783539533615\n",
      "Epoch : 1234\n",
      "Train Loss : 0.009430122888212844\n",
      "Epoch : 1234\n",
      "Val Loss : 0.009394035533070564\n",
      "Epoch : 1235\n",
      "Train Loss : 0.009429804979097695\n",
      "Epoch : 1235\n",
      "Val Loss : 0.009394273355603219\n",
      "Epoch : 1236\n",
      "Train Loss : 0.009429495584458325\n",
      "Epoch : 1236\n",
      "Val Loss : 0.00939450342953205\n",
      "Epoch : 1237\n",
      "Train Loss : 0.00942918966156414\n",
      "Epoch : 1237\n",
      "Val Loss : 0.009394757509231567\n",
      "Epoch : 1238\n",
      "Train Loss : 0.009428884615306329\n",
      "Epoch : 1238\n",
      "Val Loss : 0.009395000472664833\n",
      "Epoch : 1239\n",
      "Train Loss : 0.009428570341994966\n",
      "Epoch : 1239\n",
      "Val Loss : 0.009395243793725968\n",
      "Epoch : 1240\n",
      "Train Loss : 0.009428261174386188\n",
      "Epoch : 1240\n",
      "Val Loss : 0.009395472645759583\n",
      "Epoch : 1241\n",
      "Train Loss : 0.009427952986157555\n",
      "Epoch : 1241\n",
      "Val Loss : 0.009395695060491562\n",
      "Epoch : 1242\n",
      "Train Loss : 0.009427648799964541\n",
      "Epoch : 1242\n",
      "Val Loss : 0.009395934790372849\n",
      "Epoch : 1243\n",
      "Train Loss : 0.009427348610835674\n",
      "Epoch : 1243\n",
      "Val Loss : 0.009396177858114243\n",
      "Epoch : 1244\n",
      "Train Loss : 0.00942704145501623\n",
      "Epoch : 1244\n",
      "Val Loss : 0.00939640873670578\n",
      "Epoch : 1245\n",
      "Train Loss : 0.009426732754725897\n",
      "Epoch : 1245\n",
      "Val Loss : 0.009396632328629494\n",
      "Epoch : 1246\n",
      "Train Loss : 0.009426428296759035\n",
      "Epoch : 1246\n",
      "Val Loss : 0.009396861538290977\n",
      "Epoch : 1247\n",
      "Train Loss : 0.009426123137814506\n",
      "Epoch : 1247\n",
      "Val Loss : 0.009397082597017287\n",
      "Epoch : 1248\n",
      "Train Loss : 0.009425821984219911\n",
      "Epoch : 1248\n",
      "Val Loss : 0.009397314116358756\n",
      "Epoch : 1249\n",
      "Train Loss : 0.00942551954798533\n",
      "Epoch : 1249\n",
      "Val Loss : 0.009397546112537384\n",
      "Epoch : 1250\n",
      "Train Loss : 0.009425214526584883\n",
      "Epoch : 1250\n",
      "Val Loss : 0.009397746458649636\n",
      "Epoch : 1251\n",
      "Train Loss : 0.009424918066060628\n",
      "Epoch : 1251\n",
      "Val Loss : 0.009397983908653259\n",
      "Epoch : 1252\n",
      "Train Loss : 0.009424615909885681\n",
      "Epoch : 1252\n",
      "Val Loss : 0.009398203521966934\n",
      "Epoch : 1253\n",
      "Train Loss : 0.00942432211904233\n",
      "Epoch : 1253\n",
      "Val Loss : 0.009398433953523636\n",
      "Epoch : 1254\n",
      "Train Loss : 0.009424028220483735\n",
      "Epoch : 1254\n",
      "Val Loss : 0.009398636430501938\n",
      "Epoch : 1255\n",
      "Train Loss : 0.009423734560555834\n",
      "Epoch : 1255\n",
      "Val Loss : 0.009398865535855293\n",
      "Epoch : 1256\n",
      "Train Loss : 0.009423438963410692\n",
      "Epoch : 1256\n",
      "Val Loss : 0.009399090737104416\n",
      "Epoch : 1257\n",
      "Train Loss : 0.00942314093024387\n",
      "Epoch : 1257\n",
      "Val Loss : 0.009399297907948493\n",
      "Epoch : 1258\n",
      "Train Loss : 0.009422847223915558\n",
      "Epoch : 1258\n",
      "Val Loss : 0.009399517118930816\n",
      "Epoch : 1259\n",
      "Train Loss : 0.009422559378953697\n",
      "Epoch : 1259\n",
      "Val Loss : 0.009399721100926399\n",
      "Epoch : 1260\n",
      "Train Loss : 0.00942226242791079\n",
      "Epoch : 1260\n",
      "Val Loss : 0.00939993505179882\n",
      "Epoch : 1261\n",
      "Train Loss : 0.00942196237964032\n",
      "Epoch : 1261\n",
      "Val Loss : 0.009400131702423096\n",
      "Epoch : 1262\n",
      "Train Loss : 0.00942166879594167\n",
      "Epoch : 1262\n",
      "Val Loss : 0.009400318175554275\n",
      "Epoch : 1263\n",
      "Train Loss : 0.009421378475186346\n",
      "Epoch : 1263\n",
      "Val Loss : 0.009400549411773682\n",
      "Epoch : 1264\n",
      "Train Loss : 0.009421084775486662\n",
      "Epoch : 1264\n",
      "Val Loss : 0.009400739923119544\n",
      "Epoch : 1265\n",
      "Train Loss : 0.00942079543576864\n",
      "Epoch : 1265\n",
      "Val Loss : 0.009400941997766494\n",
      "Epoch : 1266\n",
      "Train Loss : 0.009420509755054615\n",
      "Epoch : 1266\n",
      "Val Loss : 0.009401136517524718\n",
      "Epoch : 1267\n",
      "Train Loss : 0.009420221933292961\n",
      "Epoch : 1267\n",
      "Val Loss : 0.009401344761252404\n",
      "Epoch : 1268\n",
      "Train Loss : 0.009419931567794382\n",
      "Epoch : 1268\n",
      "Val Loss : 0.009401529550552368\n",
      "Epoch : 1269\n",
      "Train Loss : 0.00941965174678965\n",
      "Epoch : 1269\n",
      "Val Loss : 0.009401743710041046\n",
      "Epoch : 1270\n",
      "Train Loss : 0.00941936455143357\n",
      "Epoch : 1270\n",
      "Val Loss : 0.009401922971010208\n",
      "Epoch : 1271\n",
      "Train Loss : 0.009419076766129384\n",
      "Epoch : 1271\n",
      "Val Loss : 0.009402123734354973\n",
      "Epoch : 1272\n",
      "Train Loss : 0.009418794661561472\n",
      "Epoch : 1272\n",
      "Val Loss : 0.009402328804135322\n",
      "Epoch : 1273\n",
      "Train Loss : 0.009418509506166406\n",
      "Epoch : 1273\n",
      "Val Loss : 0.009402503669261932\n",
      "Epoch : 1274\n",
      "Train Loss : 0.009418231350605068\n",
      "Epoch : 1274\n",
      "Val Loss : 0.009402703642845154\n",
      "Epoch : 1275\n",
      "Train Loss : 0.009417951605829585\n",
      "Epoch : 1275\n",
      "Val Loss : 0.00940287646651268\n",
      "Epoch : 1276\n",
      "Train Loss : 0.009417677632934046\n",
      "Epoch : 1276\n",
      "Val Loss : 0.009403077363967895\n",
      "Epoch : 1277\n",
      "Train Loss : 0.009417398657079693\n",
      "Epoch : 1277\n",
      "Val Loss : 0.009403244376182555\n",
      "Epoch : 1278\n",
      "Train Loss : 0.009417115796847913\n",
      "Epoch : 1278\n",
      "Val Loss : 0.009403449699282647\n",
      "Epoch : 1279\n",
      "Train Loss : 0.009416851402323335\n",
      "Epoch : 1279\n",
      "Val Loss : 0.00940361773967743\n",
      "Epoch : 1280\n",
      "Train Loss : 0.009416571871321184\n",
      "Epoch : 1280\n",
      "Val Loss : 0.009403809666633607\n",
      "Epoch : 1281\n",
      "Train Loss : 0.009416301560743953\n",
      "Epoch : 1281\n",
      "Val Loss : 0.009403981924057007\n",
      "Epoch : 1282\n",
      "Train Loss : 0.00941601816856458\n",
      "Epoch : 1282\n",
      "Val Loss : 0.00940413522720337\n",
      "Epoch : 1283\n",
      "Train Loss : 0.009415743184802901\n",
      "Epoch : 1283\n",
      "Val Loss : 0.009404316559433938\n",
      "Epoch : 1284\n",
      "Train Loss : 0.009415474475039919\n",
      "Epoch : 1284\n",
      "Val Loss : 0.009404491037130355\n",
      "Epoch : 1285\n",
      "Train Loss : 0.009415207140717748\n",
      "Epoch : 1285\n",
      "Val Loss : 0.009404663279652595\n",
      "Epoch : 1286\n",
      "Train Loss : 0.009414940946520012\n",
      "Epoch : 1286\n",
      "Val Loss : 0.009404831156134606\n",
      "Epoch : 1287\n",
      "Train Loss : 0.009414671686580703\n",
      "Epoch : 1287\n",
      "Val Loss : 0.009404995441436768\n",
      "Epoch : 1288\n",
      "Train Loss : 0.0094144046389468\n",
      "Epoch : 1288\n",
      "Val Loss : 0.009405165880918503\n",
      "Epoch : 1289\n",
      "Train Loss : 0.009414144205028901\n",
      "Epoch : 1289\n",
      "Val Loss : 0.009405330374836922\n",
      "Epoch : 1290\n",
      "Train Loss : 0.009413869158295231\n",
      "Epoch : 1290\n",
      "Val Loss : 0.00940548425912857\n",
      "Epoch : 1291\n",
      "Train Loss : 0.009413609798215462\n",
      "Epoch : 1291\n",
      "Val Loss : 0.009405641496181488\n",
      "Epoch : 1292\n",
      "Train Loss : 0.009413348423032044\n",
      "Epoch : 1292\n",
      "Val Loss : 0.009405799210071564\n",
      "Epoch : 1293\n",
      "Train Loss : 0.009413080700934993\n",
      "Epoch : 1293\n",
      "Val Loss : 0.009405946910381317\n",
      "Epoch : 1294\n",
      "Train Loss : 0.009412831751119306\n",
      "Epoch : 1294\n",
      "Val Loss : 0.009406113177537917\n",
      "Epoch : 1295\n",
      "Train Loss : 0.009412573327333584\n",
      "Epoch : 1295\n",
      "Val Loss : 0.009406256422400474\n",
      "Epoch : 1296\n",
      "Train Loss : 0.009412310720882064\n",
      "Epoch : 1296\n",
      "Val Loss : 0.009406395986676216\n",
      "Epoch : 1297\n",
      "Train Loss : 0.009412048986095444\n",
      "Epoch : 1297\n",
      "Val Loss : 0.009406544893980026\n",
      "Epoch : 1298\n",
      "Train Loss : 0.00941179603258698\n",
      "Epoch : 1298\n",
      "Val Loss : 0.00940668761730194\n",
      "Epoch : 1299\n",
      "Train Loss : 0.00941153738508498\n",
      "Epoch : 1299\n",
      "Val Loss : 0.009406823143362998\n",
      "Epoch : 1300\n",
      "Train Loss : 0.00941128652622373\n",
      "Epoch : 1300\n",
      "Val Loss : 0.009406979858875274\n",
      "Epoch : 1301\n",
      "Train Loss : 0.009411030152341968\n",
      "Epoch : 1301\n",
      "Val Loss : 0.009407087966799735\n",
      "Epoch : 1302\n",
      "Train Loss : 0.009410778130156077\n",
      "Epoch : 1302\n",
      "Val Loss : 0.00940721932053566\n",
      "Epoch : 1303\n",
      "Train Loss : 0.009410530998242283\n",
      "Epoch : 1303\n",
      "Val Loss : 0.009407352030277252\n",
      "Epoch : 1304\n",
      "Train Loss : 0.009410278926341666\n",
      "Epoch : 1304\n",
      "Val Loss : 0.009407503709197044\n",
      "Epoch : 1305\n",
      "Train Loss : 0.009410039687469549\n",
      "Epoch : 1305\n",
      "Val Loss : 0.009407613560557365\n",
      "Epoch : 1306\n",
      "Train Loss : 0.00940978587886776\n",
      "Epoch : 1306\n",
      "Val Loss : 0.00940773294866085\n",
      "Epoch : 1307\n",
      "Train Loss : 0.00940954564570108\n",
      "Epoch : 1307\n",
      "Val Loss : 0.009407873705029487\n",
      "Epoch : 1308\n",
      "Train Loss : 0.009409307464095516\n",
      "Epoch : 1308\n",
      "Val Loss : 0.00940799579024315\n",
      "Epoch : 1309\n",
      "Train Loss : 0.009409065171081931\n",
      "Epoch : 1309\n",
      "Val Loss : 0.009408105790615082\n",
      "Epoch : 1310\n",
      "Train Loss : 0.00940881567606139\n",
      "Epoch : 1310\n",
      "Val Loss : 0.009408207342028617\n",
      "Epoch : 1311\n",
      "Train Loss : 0.009408574309398907\n",
      "Epoch : 1311\n",
      "Val Loss : 0.009408326983451844\n",
      "Epoch : 1312\n",
      "Train Loss : 0.009408342693451779\n",
      "Epoch : 1312\n",
      "Val Loss : 0.00940842118859291\n",
      "Epoch : 1313\n",
      "Train Loss : 0.009408103444636717\n",
      "Epoch : 1313\n",
      "Val Loss : 0.009408529624342918\n",
      "Epoch : 1314\n",
      "Train Loss : 0.009407880308365058\n",
      "Epoch : 1314\n",
      "Val Loss : 0.009408652186393737\n",
      "Epoch : 1315\n",
      "Train Loss : 0.00940764518255812\n",
      "Epoch : 1315\n",
      "Val Loss : 0.009408725082874297\n",
      "Epoch : 1316\n",
      "Train Loss : 0.009407420405700431\n",
      "Epoch : 1316\n",
      "Val Loss : 0.009408829152584076\n",
      "Epoch : 1317\n",
      "Train Loss : 0.009407192819960601\n",
      "Epoch : 1317\n",
      "Val Loss : 0.009408936649560929\n",
      "Epoch : 1318\n",
      "Train Loss : 0.009406962494939247\n",
      "Epoch : 1318\n",
      "Val Loss : 0.009409005358815193\n",
      "Epoch : 1319\n",
      "Train Loss : 0.00940674558792303\n",
      "Epoch : 1319\n",
      "Val Loss : 0.009409095332026482\n",
      "Epoch : 1320\n",
      "Train Loss : 0.00940652223787804\n",
      "Epoch : 1320\n",
      "Val Loss : 0.009409183129668236\n",
      "Epoch : 1321\n",
      "Train Loss : 0.009406308028714406\n",
      "Epoch : 1321\n",
      "Val Loss : 0.00940927417576313\n",
      "Epoch : 1322\n",
      "Train Loss : 0.009406089504312366\n",
      "Epoch : 1322\n",
      "Val Loss : 0.009409353986382485\n",
      "Epoch : 1323\n",
      "Train Loss : 0.009405876662303756\n",
      "Epoch : 1323\n",
      "Val Loss : 0.009409409016370773\n",
      "Epoch : 1324\n",
      "Train Loss : 0.00940566941154491\n",
      "Epoch : 1324\n",
      "Val Loss : 0.009409491017460823\n",
      "Epoch : 1325\n",
      "Train Loss : 0.009405455245467374\n",
      "Epoch : 1325\n",
      "Val Loss : 0.00940956163406372\n",
      "Epoch : 1326\n",
      "Train Loss : 0.00940524556365832\n",
      "Epoch : 1326\n",
      "Val Loss : 0.009409635350108147\n",
      "Epoch : 1327\n",
      "Train Loss : 0.009405044389696414\n",
      "Epoch : 1327\n",
      "Val Loss : 0.009409677833318711\n",
      "Epoch : 1328\n",
      "Train Loss : 0.009404840814513139\n",
      "Epoch : 1328\n",
      "Val Loss : 0.0094097461104393\n",
      "Epoch : 1329\n",
      "Train Loss : 0.009404639249462037\n",
      "Epoch : 1329\n",
      "Val Loss : 0.009409801095724106\n",
      "Epoch : 1330\n",
      "Train Loss : 0.00940444317954556\n",
      "Epoch : 1330\n",
      "Val Loss : 0.009409828707575799\n",
      "Epoch : 1331\n",
      "Train Loss : 0.009404246332422164\n",
      "Epoch : 1331\n",
      "Val Loss : 0.00940990450978279\n",
      "Epoch : 1332\n",
      "Train Loss : 0.009404062132725618\n",
      "Epoch : 1332\n",
      "Val Loss : 0.009409914821386338\n",
      "Epoch : 1333\n",
      "Train Loss : 0.009403864024505285\n",
      "Epoch : 1333\n",
      "Val Loss : 0.009409971088171005\n",
      "Epoch : 1334\n",
      "Train Loss : 0.009403691244281802\n",
      "Epoch : 1334\n",
      "Val Loss : 0.009409998580813407\n",
      "Epoch : 1335\n",
      "Train Loss : 0.00940351487134063\n",
      "Epoch : 1335\n",
      "Val Loss : 0.00941003718972206\n",
      "Epoch : 1336\n",
      "Train Loss : 0.009403339696524406\n",
      "Epoch : 1336\n",
      "Val Loss : 0.009410048872232436\n",
      "Epoch : 1337\n",
      "Train Loss : 0.009403161351565683\n",
      "Epoch : 1337\n",
      "Val Loss : 0.00941004940867424\n",
      "Epoch : 1338\n",
      "Train Loss : 0.009402991998344129\n",
      "Epoch : 1338\n",
      "Val Loss : 0.009410084366798402\n",
      "Epoch : 1339\n",
      "Train Loss : 0.009402836507245951\n",
      "Epoch : 1339\n",
      "Val Loss : 0.009410093769431114\n",
      "Epoch : 1340\n",
      "Train Loss : 0.009402684726523653\n",
      "Epoch : 1340\n",
      "Val Loss : 0.009410094767808914\n",
      "Epoch : 1341\n",
      "Train Loss : 0.00940253639600349\n",
      "Epoch : 1341\n",
      "Val Loss : 0.00941009134054184\n",
      "Epoch : 1342\n",
      "Train Loss : 0.00940239541663447\n",
      "Epoch : 1342\n",
      "Val Loss : 0.009410082191228866\n",
      "Epoch : 1343\n",
      "Train Loss : 0.009402251850442423\n",
      "Epoch : 1343\n",
      "Val Loss : 0.009410041123628617\n",
      "Epoch : 1344\n",
      "Train Loss : 0.00940211406441611\n",
      "Epoch : 1344\n",
      "Val Loss : 0.009410035133361817\n",
      "Epoch : 1345\n",
      "Train Loss : 0.009401991751270362\n",
      "Epoch : 1345\n",
      "Val Loss : 0.009410011917352676\n",
      "Epoch : 1346\n",
      "Train Loss : 0.009401875950754007\n",
      "Epoch : 1346\n",
      "Val Loss : 0.009409964114427566\n",
      "Epoch : 1347\n",
      "Train Loss : 0.009401765683486898\n",
      "Epoch : 1347\n",
      "Val Loss : 0.009409928649663925\n",
      "Epoch : 1348\n",
      "Train Loss : 0.009401667016323032\n",
      "Epoch : 1348\n",
      "Val Loss : 0.009409864410758018\n",
      "Epoch : 1349\n",
      "Train Loss : 0.009401572361137731\n",
      "Epoch : 1349\n",
      "Val Loss : 0.009409801334142685\n",
      "Epoch : 1350\n",
      "Train Loss : 0.009401500849815557\n",
      "Epoch : 1350\n",
      "Val Loss : 0.009409738391637803\n",
      "Epoch : 1351\n",
      "Train Loss : 0.009401427813908384\n",
      "Epoch : 1351\n",
      "Val Loss : 0.009409657627344132\n",
      "Epoch : 1352\n",
      "Train Loss : 0.009401370143166535\n",
      "Epoch : 1352\n",
      "Val Loss : 0.009409562438726425\n",
      "Epoch : 1353\n",
      "Train Loss : 0.009401326902953118\n",
      "Epoch : 1353\n",
      "Val Loss : 0.009409456819295883\n",
      "Epoch : 1354\n",
      "Train Loss : 0.009401316149657413\n",
      "Epoch : 1354\n",
      "Val Loss : 0.009409338027238846\n",
      "Epoch : 1355\n",
      "Train Loss : 0.009401305953166662\n",
      "Epoch : 1355\n",
      "Val Loss : 0.009409216493368148\n",
      "Epoch : 1356\n",
      "Train Loss : 0.009401315816568735\n",
      "Epoch : 1356\n",
      "Val Loss : 0.009409056544303894\n",
      "Epoch : 1357\n",
      "Train Loss : 0.009401353627933833\n",
      "Epoch : 1357\n",
      "Val Loss : 0.009408886417746544\n",
      "Epoch : 1358\n",
      "Train Loss : 0.009401408203105274\n",
      "Epoch : 1358\n",
      "Val Loss : 0.009408700570464135\n",
      "Epoch : 1359\n",
      "Train Loss : 0.009401492797686747\n",
      "Epoch : 1359\n",
      "Val Loss : 0.009408480778336526\n",
      "Epoch : 1360\n",
      "Train Loss : 0.009401604012848007\n",
      "Epoch : 1360\n",
      "Val Loss : 0.009408238932490348\n",
      "Epoch : 1361\n",
      "Train Loss : 0.009401756935351896\n",
      "Epoch : 1361\n",
      "Val Loss : 0.009407950669527054\n",
      "Epoch : 1362\n",
      "Train Loss : 0.009401940187154291\n",
      "Epoch : 1362\n",
      "Val Loss : 0.009407650172710419\n",
      "Epoch : 1363\n",
      "Train Loss : 0.009402163596856954\n",
      "Epoch : 1363\n",
      "Val Loss : 0.009407292798161507\n",
      "Epoch : 1364\n",
      "Train Loss : 0.009402438426502978\n",
      "Epoch : 1364\n",
      "Val Loss : 0.009406890362501145\n",
      "Epoch : 1365\n",
      "Train Loss : 0.00940276720160055\n",
      "Epoch : 1365\n",
      "Val Loss : 0.009406438052654266\n",
      "Epoch : 1366\n",
      "Train Loss : 0.009403170509018614\n",
      "Epoch : 1366\n",
      "Val Loss : 0.00940590737760067\n",
      "Epoch : 1367\n",
      "Train Loss : 0.009403630595627734\n",
      "Epoch : 1367\n",
      "Val Loss : 0.009405290856957436\n",
      "Epoch : 1368\n",
      "Train Loss : 0.00940418808333231\n",
      "Epoch : 1368\n",
      "Val Loss : 0.009404553785920143\n",
      "Epoch : 1369\n",
      "Train Loss : 0.009404824764941724\n",
      "Epoch : 1369\n",
      "Val Loss : 0.00940370650589466\n",
      "Epoch : 1370\n",
      "Train Loss : 0.0094055808546996\n",
      "Epoch : 1370\n",
      "Val Loss : 0.009402725100517272\n",
      "Epoch : 1371\n",
      "Train Loss : 0.009406470990279072\n",
      "Epoch : 1371\n",
      "Val Loss : 0.009401526242494584\n",
      "Epoch : 1372\n",
      "Train Loss : 0.009407505978004894\n",
      "Epoch : 1372\n",
      "Val Loss : 0.00940010778605938\n",
      "Epoch : 1373\n",
      "Train Loss : 0.009408718636226294\n",
      "Epoch : 1373\n",
      "Val Loss : 0.009398392468690873\n",
      "Epoch : 1374\n",
      "Train Loss : 0.009410133285388297\n",
      "Epoch : 1374\n",
      "Val Loss : 0.009396294981241227\n",
      "Epoch : 1375\n",
      "Train Loss : 0.0094117815921156\n",
      "Epoch : 1375\n",
      "Val Loss : 0.009393708243966103\n",
      "Epoch : 1376\n",
      "Train Loss : 0.009413695130232071\n",
      "Epoch : 1376\n",
      "Val Loss : 0.009390542268753051\n",
      "Epoch : 1377\n",
      "Train Loss : 0.009415929692919992\n",
      "Epoch : 1377\n",
      "Val Loss : 0.00938664412498474\n",
      "Epoch : 1378\n",
      "Train Loss : 0.009418519379511422\n",
      "Epoch : 1378\n",
      "Val Loss : 0.009381750389933587\n",
      "Epoch : 1379\n",
      "Train Loss : 0.009421475428849246\n",
      "Epoch : 1379\n",
      "Val Loss : 0.0093756785094738\n",
      "Epoch : 1380\n",
      "Train Loss : 0.009424840342054588\n",
      "Epoch : 1380\n",
      "Val Loss : 0.009368159011006355\n",
      "Epoch : 1381\n",
      "Train Loss : 0.009428629943325426\n",
      "Epoch : 1381\n",
      "Val Loss : 0.009358893141150475\n",
      "Epoch : 1382\n",
      "Train Loss : 0.009432779141168152\n",
      "Epoch : 1382\n",
      "Val Loss : 0.009347628265619278\n",
      "Epoch : 1383\n",
      "Train Loss : 0.009437216245287572\n",
      "Epoch : 1383\n",
      "Val Loss : 0.009334241598844528\n",
      "Epoch : 1384\n",
      "Train Loss : 0.009441780098763649\n",
      "Epoch : 1384\n",
      "Val Loss : 0.009318746060132981\n",
      "Epoch : 1385\n",
      "Train Loss : 0.009446231374894915\n",
      "Epoch : 1385\n",
      "Val Loss : 0.009301536306738854\n",
      "Epoch : 1386\n",
      "Train Loss : 0.009450336073084447\n",
      "Epoch : 1386\n",
      "Val Loss : 0.009283283859491348\n",
      "Epoch : 1387\n",
      "Train Loss : 0.009453854507027572\n",
      "Epoch : 1387\n",
      "Val Loss : 0.009264876037836075\n",
      "Epoch : 1388\n",
      "Train Loss : 0.00945662163542683\n",
      "Epoch : 1388\n",
      "Val Loss : 0.009247316598892212\n",
      "Epoch : 1389\n",
      "Train Loss : 0.009458569546306366\n",
      "Epoch : 1389\n",
      "Val Loss : 0.009231432303786279\n",
      "Epoch : 1390\n",
      "Train Loss : 0.009459725284478313\n",
      "Epoch : 1390\n",
      "Val Loss : 0.009217781215906143\n",
      "Epoch : 1391\n",
      "Train Loss : 0.009460204882461087\n",
      "Epoch : 1391\n",
      "Val Loss : 0.009206596955657006\n",
      "Epoch : 1392\n",
      "Train Loss : 0.009460099808725916\n",
      "Epoch : 1392\n",
      "Val Loss : 0.009197902664542198\n",
      "Epoch : 1393\n",
      "Train Loss : 0.009459570353499406\n",
      "Epoch : 1393\n",
      "Val Loss : 0.009191530466079711\n",
      "Epoch : 1394\n",
      "Train Loss : 0.00945868997857823\n",
      "Epoch : 1394\n",
      "Val Loss : 0.009187239646911621\n",
      "Epoch : 1395\n",
      "Train Loss : 0.009457549022252132\n",
      "Epoch : 1395\n",
      "Val Loss : 0.00918477039039135\n",
      "Epoch : 1396\n",
      "Train Loss : 0.009456207856429854\n",
      "Epoch : 1396\n",
      "Val Loss : 0.009183850541710854\n",
      "Epoch : 1397\n",
      "Train Loss : 0.009454702303887367\n",
      "Epoch : 1397\n",
      "Val Loss : 0.00918426588177681\n",
      "Epoch : 1398\n",
      "Train Loss : 0.00945307512094807\n",
      "Epoch : 1398\n",
      "Val Loss : 0.00918579412996769\n",
      "Epoch : 1399\n",
      "Train Loss : 0.009451336351644208\n",
      "Epoch : 1399\n",
      "Val Loss : 0.00918826712667942\n",
      "Epoch : 1400\n",
      "Train Loss : 0.00944949574503398\n",
      "Epoch : 1400\n",
      "Val Loss : 0.00919159610569477\n",
      "Epoch : 1401\n",
      "Train Loss : 0.009447563396521523\n",
      "Epoch : 1401\n",
      "Val Loss : 0.009195632860064506\n",
      "Epoch : 1402\n",
      "Train Loss : 0.009445548625961211\n",
      "Epoch : 1402\n",
      "Val Loss : 0.009200327232480049\n",
      "Epoch : 1403\n",
      "Train Loss : 0.009443440191195846\n",
      "Epoch : 1403\n",
      "Val Loss : 0.009205577179789544\n",
      "Epoch : 1404\n",
      "Train Loss : 0.009441243083584138\n",
      "Epoch : 1404\n",
      "Val Loss : 0.009211374431848525\n",
      "Epoch : 1405\n",
      "Train Loss : 0.00943893879101847\n",
      "Epoch : 1405\n",
      "Val Loss : 0.009217682257294656\n",
      "Epoch : 1406\n",
      "Train Loss : 0.009436508890877732\n",
      "Epoch : 1406\n",
      "Val Loss : 0.009224484562873841\n",
      "Epoch : 1407\n",
      "Train Loss : 0.00943395459951561\n",
      "Epoch : 1407\n",
      "Val Loss : 0.009231791764497756\n",
      "Epoch : 1408\n",
      "Train Loss : 0.009431255943211479\n",
      "Epoch : 1408\n",
      "Val Loss : 0.009239612862467765\n",
      "Epoch : 1409\n",
      "Train Loss : 0.00942837863206015\n",
      "Epoch : 1409\n",
      "Val Loss : 0.009247958794236183\n",
      "Epoch : 1410\n",
      "Train Loss : 0.00942529087515011\n",
      "Epoch : 1410\n",
      "Val Loss : 0.009256883323192596\n",
      "Epoch : 1411\n",
      "Train Loss : 0.009421968047119332\n",
      "Epoch : 1411\n",
      "Val Loss : 0.00926637428998947\n",
      "Epoch : 1412\n",
      "Train Loss : 0.00941832328473758\n",
      "Epoch : 1412\n",
      "Val Loss : 0.00927652807533741\n",
      "Epoch : 1413\n",
      "Train Loss : 0.009414317454227985\n",
      "Epoch : 1413\n",
      "Val Loss : 0.009287364989519119\n",
      "Epoch : 1414\n",
      "Train Loss : 0.009409827022776804\n",
      "Epoch : 1414\n",
      "Val Loss : 0.009298930764198304\n",
      "Epoch : 1415\n",
      "Train Loss : 0.009404773265454799\n",
      "Epoch : 1415\n",
      "Val Loss : 0.009311221435666085\n",
      "Epoch : 1416\n",
      "Train Loss : 0.009398980548413092\n",
      "Epoch : 1416\n",
      "Val Loss : 0.009324115216732026\n",
      "Epoch : 1417\n",
      "Train Loss : 0.009392287737931857\n",
      "Epoch : 1417\n",
      "Val Loss : 0.009337377741932868\n",
      "Epoch : 1418\n",
      "Train Loss : 0.009384483036011776\n",
      "Epoch : 1418\n",
      "Val Loss : 0.009350470453500747\n",
      "Epoch : 1419\n",
      "Train Loss : 0.009375364421883406\n",
      "Epoch : 1419\n",
      "Val Loss : 0.009362319827079774\n",
      "Epoch : 1420\n",
      "Train Loss : 0.009364817669330118\n",
      "Epoch : 1420\n",
      "Val Loss : 0.009371219038963317\n",
      "Epoch : 1421\n",
      "Train Loss : 0.009352961115976563\n",
      "Epoch : 1421\n",
      "Val Loss : 0.009374655157327652\n",
      "Epoch : 1422\n",
      "Train Loss : 0.009340248997008355\n",
      "Epoch : 1422\n",
      "Val Loss : 0.00936990800499916\n",
      "Epoch : 1423\n",
      "Train Loss : 0.009327386959962998\n",
      "Epoch : 1423\n",
      "Val Loss : 0.00935498046875\n",
      "Epoch : 1424\n",
      "Train Loss : 0.009314807372174334\n",
      "Epoch : 1424\n",
      "Val Loss : 0.009329328685998917\n",
      "Epoch : 1425\n",
      "Train Loss : 0.009301954326866148\n",
      "Epoch : 1425\n",
      "Val Loss : 0.009292751267552376\n",
      "Epoch : 1426\n",
      "Train Loss : 0.009287132879924626\n",
      "Epoch : 1426\n",
      "Val Loss : 0.009243325009942054\n",
      "Epoch : 1427\n",
      "Train Loss : 0.009267745426859295\n",
      "Epoch : 1427\n",
      "Val Loss : 0.009176458150148392\n",
      "Epoch : 1428\n",
      "Train Loss : 0.009240881233307073\n",
      "Epoch : 1428\n",
      "Val Loss : 0.009087903529405594\n",
      "Epoch : 1429\n",
      "Train Loss : 0.009206024716107765\n",
      "Epoch : 1429\n",
      "Val Loss : 0.008986358568072318\n",
      "Epoch : 1430\n",
      "Train Loss : 0.009170786079907544\n",
      "Epoch : 1430\n",
      "Val Loss : 0.008904483065009117\n",
      "Epoch : 1431\n",
      "Train Loss : 0.009148493937790075\n",
      "Epoch : 1431\n",
      "Val Loss : 0.00886552830040455\n",
      "Epoch : 1432\n",
      "Train Loss : 0.009143327420835923\n",
      "Epoch : 1432\n",
      "Val Loss : 0.008854798600077629\n",
      "Epoch : 1433\n",
      "Train Loss : 0.009147962068084933\n",
      "Epoch : 1433\n",
      "Val Loss : 0.00885310012102127\n",
      "Epoch : 1434\n",
      "Train Loss : 0.009154776084730634\n",
      "Epoch : 1434\n",
      "Val Loss : 0.008853249713778495\n",
      "Epoch : 1435\n",
      "Train Loss : 0.009160537160005009\n",
      "Epoch : 1435\n",
      "Val Loss : 0.008853684365749358\n",
      "Epoch : 1436\n",
      "Train Loss : 0.009164680315851105\n",
      "Epoch : 1436\n",
      "Val Loss : 0.008854152277112008\n",
      "Epoch : 1437\n",
      "Train Loss : 0.009167477082514149\n",
      "Epoch : 1437\n",
      "Val Loss : 0.00885461688041687\n",
      "Epoch : 1438\n",
      "Train Loss : 0.009169317052775855\n",
      "Epoch : 1438\n",
      "Val Loss : 0.008855080500245095\n",
      "Epoch : 1439\n",
      "Train Loss : 0.009170496947334223\n",
      "Epoch : 1439\n",
      "Val Loss : 0.008855535432696342\n",
      "Epoch : 1440\n",
      "Train Loss : 0.009171252463715569\n",
      "Epoch : 1440\n",
      "Val Loss : 0.008855982184410095\n",
      "Epoch : 1441\n",
      "Train Loss : 0.009171741205894014\n",
      "Epoch : 1441\n",
      "Val Loss : 0.008856416836380958\n",
      "Epoch : 1442\n",
      "Train Loss : 0.009172038499968544\n",
      "Epoch : 1442\n",
      "Val Loss : 0.00885685183107853\n",
      "Epoch : 1443\n",
      "Train Loss : 0.009172218924660063\n",
      "Epoch : 1443\n",
      "Val Loss : 0.00885727533698082\n",
      "Epoch : 1444\n",
      "Train Loss : 0.009172328719637255\n",
      "Epoch : 1444\n",
      "Val Loss : 0.008857691958546639\n",
      "Epoch : 1445\n",
      "Train Loss : 0.00917239587594924\n",
      "Epoch : 1445\n",
      "Val Loss : 0.008858112603425979\n",
      "Epoch : 1446\n",
      "Train Loss : 0.009172422450128611\n",
      "Epoch : 1446\n",
      "Val Loss : 0.008858527421951295\n",
      "Epoch : 1447\n",
      "Train Loss : 0.00917243477275257\n",
      "Epoch : 1447\n",
      "Val Loss : 0.008858930364251137\n",
      "Epoch : 1448\n",
      "Train Loss : 0.009172428260123178\n",
      "Epoch : 1448\n",
      "Val Loss : 0.008859339132905007\n",
      "Epoch : 1449\n",
      "Train Loss : 0.00917243270296272\n",
      "Epoch : 1449\n",
      "Val Loss : 0.00885974432528019\n",
      "Epoch : 1450\n",
      "Train Loss : 0.009172420547711679\n",
      "Epoch : 1450\n",
      "Val Loss : 0.008860143482685089\n",
      "Epoch : 1451\n",
      "Train Loss : 0.009172411106884798\n",
      "Epoch : 1451\n",
      "Val Loss : 0.008860546618700027\n",
      "Epoch : 1452\n",
      "Train Loss : 0.009172403797162597\n",
      "Epoch : 1452\n",
      "Val Loss : 0.008860943138599395\n",
      "Epoch : 1453\n",
      "Train Loss : 0.009172396223952337\n",
      "Epoch : 1453\n",
      "Val Loss : 0.008861337721347808\n",
      "Epoch : 1454\n",
      "Train Loss : 0.009172384050472564\n",
      "Epoch : 1454\n",
      "Val Loss : 0.008861732408404351\n",
      "Epoch : 1455\n",
      "Train Loss : 0.00917237869453918\n",
      "Epoch : 1455\n",
      "Val Loss : 0.00886212058365345\n",
      "Epoch : 1456\n",
      "Train Loss : 0.00917236878142238\n",
      "Epoch : 1456\n",
      "Val Loss : 0.008862513363361359\n",
      "Epoch : 1457\n",
      "Train Loss : 0.009172365032931876\n",
      "Epoch : 1457\n",
      "Val Loss : 0.008862902507185936\n",
      "Epoch : 1458\n",
      "Train Loss : 0.009172367538554176\n",
      "Epoch : 1458\n",
      "Val Loss : 0.0088632802516222\n",
      "Epoch : 1459\n",
      "Train Loss : 0.009172366507802145\n",
      "Epoch : 1459\n",
      "Val Loss : 0.008863666087388993\n",
      "Epoch : 1460\n",
      "Train Loss : 0.009172370725268253\n",
      "Epoch : 1460\n",
      "Val Loss : 0.008864042088389397\n",
      "Epoch : 1461\n",
      "Train Loss : 0.009172369570229402\n",
      "Epoch : 1461\n",
      "Val Loss : 0.008864422366023063\n",
      "Epoch : 1462\n",
      "Train Loss : 0.009172382487772942\n",
      "Epoch : 1462\n",
      "Val Loss : 0.008864793747663498\n",
      "Epoch : 1463\n",
      "Train Loss : 0.0091723974684777\n",
      "Epoch : 1463\n",
      "Val Loss : 0.008865163847804069\n",
      "Epoch : 1464\n",
      "Train Loss : 0.009172397324304988\n",
      "Epoch : 1464\n",
      "Val Loss : 0.008865529417991638\n",
      "Epoch : 1465\n",
      "Train Loss : 0.00917242908870198\n",
      "Epoch : 1465\n",
      "Val Loss : 0.008865897253155708\n",
      "Epoch : 1466\n",
      "Train Loss : 0.009172434472807044\n",
      "Epoch : 1466\n",
      "Val Loss : 0.008866256952285767\n",
      "Epoch : 1467\n",
      "Train Loss : 0.009172461946822995\n",
      "Epoch : 1467\n",
      "Val Loss : 0.008866616144776344\n",
      "Epoch : 1468\n",
      "Train Loss : 0.009172482251975141\n",
      "Epoch : 1468\n",
      "Val Loss : 0.008866977468132973\n",
      "Epoch : 1469\n",
      "Train Loss : 0.009172501145229009\n",
      "Epoch : 1469\n",
      "Val Loss : 0.008867331728339195\n",
      "Epoch : 1470\n",
      "Train Loss : 0.009172535227989493\n",
      "Epoch : 1470\n",
      "Val Loss : 0.008867679059505463\n",
      "Epoch : 1471\n",
      "Train Loss : 0.00917256154862375\n",
      "Epoch : 1471\n",
      "Val Loss : 0.008868027240037919\n",
      "Epoch : 1472\n",
      "Train Loss : 0.009172595677784647\n",
      "Epoch : 1472\n",
      "Val Loss : 0.008868372917175293\n",
      "Epoch : 1473\n",
      "Train Loss : 0.00917263672557855\n",
      "Epoch : 1473\n",
      "Val Loss : 0.008868714258074761\n",
      "Epoch : 1474\n",
      "Train Loss : 0.009172670016217698\n",
      "Epoch : 1474\n",
      "Val Loss : 0.008869056388735771\n",
      "Epoch : 1475\n",
      "Train Loss : 0.009172708381073437\n",
      "Epoch : 1475\n",
      "Val Loss : 0.008869389832019806\n",
      "Epoch : 1476\n",
      "Train Loss : 0.009172750595506294\n",
      "Epoch : 1476\n",
      "Val Loss : 0.008869728013873101\n",
      "Epoch : 1477\n",
      "Train Loss : 0.009172793096627416\n",
      "Epoch : 1477\n",
      "Val Loss : 0.008870058089494705\n",
      "Epoch : 1478\n",
      "Train Loss : 0.009172848555063862\n",
      "Epoch : 1478\n",
      "Val Loss : 0.008870387271046639\n",
      "Epoch : 1479\n",
      "Train Loss : 0.009172903889213487\n",
      "Epoch : 1479\n",
      "Val Loss : 0.008870717018842697\n",
      "Epoch : 1480\n",
      "Train Loss : 0.009172952420731137\n",
      "Epoch : 1480\n",
      "Val Loss : 0.008871037811040879\n",
      "Epoch : 1481\n",
      "Train Loss : 0.009173007166589812\n",
      "Epoch : 1481\n",
      "Val Loss : 0.00887135899066925\n",
      "Epoch : 1482\n",
      "Train Loss : 0.009173066164714904\n",
      "Epoch : 1482\n",
      "Val Loss : 0.008871680304408073\n",
      "Epoch : 1483\n",
      "Train Loss : 0.009173133685601563\n",
      "Epoch : 1483\n",
      "Val Loss : 0.008871995255351067\n",
      "Epoch : 1484\n",
      "Train Loss : 0.009173199433329584\n",
      "Epoch : 1484\n",
      "Val Loss : 0.00887231020629406\n",
      "Epoch : 1485\n",
      "Train Loss : 0.009173261898228388\n",
      "Epoch : 1485\n",
      "Val Loss : 0.008872620448470116\n",
      "Epoch : 1486\n",
      "Train Loss : 0.009173339231145234\n",
      "Epoch : 1486\n",
      "Val Loss : 0.00887293216586113\n",
      "Epoch : 1487\n",
      "Train Loss : 0.009173411383787402\n",
      "Epoch : 1487\n",
      "Val Loss : 0.00887324158847332\n",
      "Epoch : 1488\n",
      "Train Loss : 0.009173490171688625\n",
      "Epoch : 1488\n",
      "Val Loss : 0.008873549669981003\n",
      "Epoch : 1489\n",
      "Train Loss : 0.00917357336762908\n",
      "Epoch : 1489\n",
      "Val Loss : 0.008873850643634796\n",
      "Epoch : 1490\n",
      "Train Loss : 0.009173651563925038\n",
      "Epoch : 1490\n",
      "Val Loss : 0.0088741517663002\n",
      "Epoch : 1491\n",
      "Train Loss : 0.009173749800227927\n",
      "Epoch : 1491\n",
      "Val Loss : 0.008874454289674759\n",
      "Epoch : 1492\n",
      "Train Loss : 0.009173827955094945\n",
      "Epoch : 1492\n",
      "Val Loss : 0.008874763205647469\n",
      "Epoch : 1493\n",
      "Train Loss : 0.009173929929945392\n",
      "Epoch : 1493\n",
      "Val Loss : 0.008875058382749558\n",
      "Epoch : 1494\n",
      "Train Loss : 0.009174031362905302\n",
      "Epoch : 1494\n",
      "Val Loss : 0.008875356078147888\n",
      "Epoch : 1495\n",
      "Train Loss : 0.009174140785022033\n",
      "Epoch : 1495\n",
      "Val Loss : 0.008875653281807899\n",
      "Epoch : 1496\n",
      "Train Loss : 0.00917424132643115\n",
      "Epoch : 1496\n",
      "Val Loss : 0.008875952526926995\n",
      "Epoch : 1497\n",
      "Train Loss : 0.009174354316408206\n",
      "Epoch : 1497\n",
      "Val Loss : 0.008876242846250534\n",
      "Epoch : 1498\n",
      "Train Loss : 0.009174461968680611\n",
      "Epoch : 1498\n",
      "Val Loss : 0.008876543611288071\n",
      "Epoch : 1499\n",
      "Train Loss : 0.009174581487858635\n",
      "Epoch : 1499\n",
      "Val Loss : 0.008876835748553276\n",
      "Epoch : 1500\n",
      "Train Loss : 0.009174705458161988\n",
      "Epoch : 1500\n",
      "Val Loss : 0.008877129435539246\n",
      "Epoch : 1501\n",
      "Train Loss : 0.009174832688094351\n",
      "Epoch : 1501\n",
      "Val Loss : 0.008877427309751511\n",
      "Epoch : 1502\n",
      "Train Loss : 0.009174970578521596\n",
      "Epoch : 1502\n",
      "Val Loss : 0.008877725318074226\n",
      "Epoch : 1503\n",
      "Train Loss : 0.009175107718256444\n",
      "Epoch : 1503\n",
      "Val Loss : 0.008878020375967025\n",
      "Epoch : 1504\n",
      "Train Loss : 0.009175252752690129\n",
      "Epoch : 1504\n",
      "Val Loss : 0.008878321826457976\n",
      "Epoch : 1505\n",
      "Train Loss : 0.009175401524014214\n",
      "Epoch : 1505\n",
      "Val Loss : 0.008878616616129875\n",
      "Epoch : 1506\n",
      "Train Loss : 0.009175553697482864\n",
      "Epoch : 1506\n",
      "Val Loss : 0.008878916040062905\n",
      "Epoch : 1507\n",
      "Train Loss : 0.009175711003168621\n",
      "Epoch : 1507\n",
      "Val Loss : 0.008879220336675644\n",
      "Epoch : 1508\n",
      "Train Loss : 0.009175883385674278\n",
      "Epoch : 1508\n",
      "Val Loss : 0.008879524365067481\n",
      "Epoch : 1509\n",
      "Train Loss : 0.009176057587738987\n",
      "Epoch : 1509\n",
      "Val Loss : 0.008879834413528442\n",
      "Epoch : 1510\n",
      "Train Loss : 0.009176238231175311\n",
      "Epoch : 1510\n",
      "Val Loss : 0.008880145072937011\n",
      "Epoch : 1511\n",
      "Train Loss : 0.00917642604181828\n",
      "Epoch : 1511\n",
      "Val Loss : 0.008880456283688545\n",
      "Epoch : 1512\n",
      "Train Loss : 0.0091766193973106\n",
      "Epoch : 1512\n",
      "Val Loss : 0.008880774959921837\n",
      "Epoch : 1513\n",
      "Train Loss : 0.009176819588578045\n",
      "Epoch : 1513\n",
      "Val Loss : 0.008881100907921791\n",
      "Epoch : 1514\n",
      "Train Loss : 0.00917703716011446\n",
      "Epoch : 1514\n",
      "Val Loss : 0.008881429731845856\n",
      "Epoch : 1515\n",
      "Train Loss : 0.009177252461344957\n",
      "Epoch : 1515\n",
      "Val Loss : 0.00888176427781582\n",
      "Epoch : 1516\n",
      "Train Loss : 0.009177469956652126\n",
      "Epoch : 1516\n",
      "Val Loss : 0.00888210116326809\n",
      "Epoch : 1517\n",
      "Train Loss : 0.00917772601898826\n",
      "Epoch : 1517\n",
      "Val Loss : 0.008882454425096512\n",
      "Epoch : 1518\n",
      "Train Loss : 0.009177972436667126\n",
      "Epoch : 1518\n",
      "Val Loss : 0.008882812470197678\n",
      "Epoch : 1519\n",
      "Train Loss : 0.009178221906830302\n",
      "Epoch : 1519\n",
      "Val Loss : 0.008883175939321518\n",
      "Epoch : 1520\n",
      "Train Loss : 0.009178497785457089\n",
      "Epoch : 1520\n",
      "Val Loss : 0.008883552446961403\n",
      "Epoch : 1521\n",
      "Train Loss : 0.009178782382390041\n",
      "Epoch : 1521\n",
      "Val Loss : 0.008883937194943427\n",
      "Epoch : 1522\n",
      "Train Loss : 0.009179078765027888\n",
      "Epoch : 1522\n",
      "Val Loss : 0.008884337544441224\n",
      "Epoch : 1523\n",
      "Train Loss : 0.00917938846955573\n",
      "Epoch : 1523\n",
      "Val Loss : 0.00888475289940834\n",
      "Epoch : 1524\n",
      "Train Loss : 0.009179721145602308\n",
      "Epoch : 1524\n",
      "Val Loss : 0.008885175377130508\n",
      "Epoch : 1525\n",
      "Train Loss : 0.009180051130424933\n",
      "Epoch : 1525\n",
      "Val Loss : 0.008885623201727868\n",
      "Epoch : 1526\n",
      "Train Loss : 0.009180418475865787\n",
      "Epoch : 1526\n",
      "Val Loss : 0.008886084631085396\n",
      "Epoch : 1527\n",
      "Train Loss : 0.009180794176695291\n",
      "Epoch : 1527\n",
      "Val Loss : 0.008886570304632187\n",
      "Epoch : 1528\n",
      "Train Loss : 0.00918119669199202\n",
      "Epoch : 1528\n",
      "Val Loss : 0.00888707634806633\n",
      "Epoch : 1529\n",
      "Train Loss : 0.009181621976634255\n",
      "Epoch : 1529\n",
      "Val Loss : 0.00888760931789875\n",
      "Epoch : 1530\n",
      "Train Loss : 0.009182063814623819\n",
      "Epoch : 1530\n",
      "Val Loss : 0.008888174548745156\n",
      "Epoch : 1531\n",
      "Train Loss : 0.009182550059465751\n",
      "Epoch : 1531\n",
      "Val Loss : 0.008888772010803223\n",
      "Epoch : 1532\n",
      "Train Loss : 0.009183064843875976\n",
      "Epoch : 1532\n",
      "Val Loss : 0.008889407336711884\n",
      "Epoch : 1533\n",
      "Train Loss : 0.009183589508259848\n",
      "Epoch : 1533\n",
      "Val Loss : 0.008890082105994224\n",
      "Epoch : 1534\n",
      "Train Loss : 0.009184166333336529\n",
      "Epoch : 1534\n",
      "Val Loss : 0.008890805691480637\n",
      "Epoch : 1535\n",
      "Train Loss : 0.009184790733750923\n",
      "Epoch : 1535\n",
      "Val Loss : 0.008891585752367974\n",
      "Epoch : 1536\n",
      "Train Loss : 0.009185454349142906\n",
      "Epoch : 1536\n",
      "Val Loss : 0.00889242173731327\n",
      "Epoch : 1537\n",
      "Train Loss : 0.009186177970211807\n",
      "Epoch : 1537\n",
      "Val Loss : 0.008893343195319175\n",
      "Epoch : 1538\n",
      "Train Loss : 0.009186947734834248\n",
      "Epoch : 1538\n",
      "Val Loss : 0.008894344478845597\n",
      "Epoch : 1539\n",
      "Train Loss : 0.009187799886704234\n",
      "Epoch : 1539\n",
      "Val Loss : 0.008895441725850104\n",
      "Epoch : 1540\n",
      "Train Loss : 0.009188731578824995\n",
      "Epoch : 1540\n",
      "Val Loss : 0.008896655932068825\n",
      "Epoch : 1541\n",
      "Train Loss : 0.009189754243926858\n",
      "Epoch : 1541\n",
      "Val Loss : 0.008898020580410957\n",
      "Epoch : 1542\n",
      "Train Loss : 0.009190895629456778\n",
      "Epoch : 1542\n",
      "Val Loss : 0.008899549588561057\n",
      "Epoch : 1543\n",
      "Train Loss : 0.009192151625663891\n",
      "Epoch : 1543\n",
      "Val Loss : 0.008901285603642464\n",
      "Epoch : 1544\n",
      "Train Loss : 0.009193586303232722\n",
      "Epoch : 1544\n",
      "Val Loss : 0.008903297409415246\n",
      "Epoch : 1545\n",
      "Train Loss : 0.00919522576570988\n",
      "Epoch : 1545\n",
      "Val Loss : 0.008905631303787232\n",
      "Epoch : 1546\n",
      "Train Loss : 0.009197091967119335\n",
      "Epoch : 1546\n",
      "Val Loss : 0.008908401772379875\n",
      "Epoch : 1547\n",
      "Train Loss : 0.009199285115781521\n",
      "Epoch : 1547\n",
      "Val Loss : 0.008911743074655533\n",
      "Epoch : 1548\n",
      "Train Loss : 0.00920190618893801\n",
      "Epoch : 1548\n",
      "Val Loss : 0.008915857836604119\n",
      "Epoch : 1549\n",
      "Train Loss : 0.009205086633987274\n",
      "Epoch : 1549\n",
      "Val Loss : 0.00892106106877327\n",
      "Epoch : 1550\n",
      "Train Loss : 0.009209061220447365\n",
      "Epoch : 1550\n",
      "Val Loss : 0.00892788065969944\n",
      "Epoch : 1551\n",
      "Train Loss : 0.00921418259477117\n",
      "Epoch : 1551\n",
      "Val Loss : 0.00893716849386692\n",
      "Epoch : 1552\n",
      "Train Loss : 0.009221061179248676\n",
      "Epoch : 1552\n",
      "Val Loss : 0.008950492948293686\n",
      "Epoch : 1553\n",
      "Train Loss : 0.009230813911363642\n",
      "Epoch : 1553\n",
      "Val Loss : 0.00897098045051098\n",
      "Epoch : 1554\n",
      "Train Loss : 0.00924561476646475\n",
      "Epoch : 1554\n",
      "Val Loss : 0.009005374431610107\n",
      "Epoch : 1555\n",
      "Train Loss : 0.009270168565797\n",
      "Epoch : 1555\n",
      "Val Loss : 0.009069969803094864\n",
      "Epoch : 1556\n",
      "Train Loss : 0.00931526745546914\n",
      "Epoch : 1556\n",
      "Val Loss : 0.009205311104655266\n",
      "Epoch : 1557\n",
      "Train Loss : 0.009397239684462017\n",
      "Epoch : 1557\n",
      "Val Loss : 0.009447126463055611\n",
      "Epoch : 1558\n",
      "Train Loss : 0.009449727551554234\n",
      "Epoch : 1558\n",
      "Val Loss : 0.00951043526828289\n",
      "Epoch : 1559\n",
      "Train Loss : 0.009532023976918217\n",
      "Epoch : 1559\n",
      "Val Loss : 0.00931151770055294\n",
      "Epoch : 1560\n",
      "Train Loss : 0.009606832578725237\n",
      "Epoch : 1560\n",
      "Val Loss : 0.009127678647637368\n",
      "Epoch : 1561\n",
      "Train Loss : 0.009650172717087104\n",
      "Epoch : 1561\n",
      "Val Loss : 0.008983360275626183\n",
      "Epoch : 1562\n",
      "Train Loss : 0.009835681523398572\n",
      "Epoch : 1562\n",
      "Val Loss : 0.008904044851660728\n",
      "Epoch : 1563\n",
      "Train Loss : 0.010079630118632337\n",
      "Epoch : 1563\n",
      "Val Loss : 0.008898997247219086\n",
      "Epoch : 1564\n",
      "Train Loss : 0.01008177949347636\n",
      "Epoch : 1564\n",
      "Val Loss : 0.009087406769394875\n",
      "Epoch : 1565\n",
      "Train Loss : 0.010078605396997887\n",
      "Epoch : 1565\n",
      "Val Loss : 0.00942297388613224\n",
      "Epoch : 1566\n",
      "Train Loss : 0.010121184919904453\n",
      "Epoch : 1566\n",
      "Val Loss : 0.00983201563358307\n",
      "Epoch : 1567\n",
      "Train Loss : 0.010102868184540089\n",
      "Epoch : 1567\n",
      "Val Loss : 0.010113078519701958\n",
      "Epoch : 1568\n",
      "Train Loss : 0.010071866323647762\n",
      "Epoch : 1568\n",
      "Val Loss : 0.010260498240590096\n",
      "Epoch : 1569\n",
      "Train Loss : 0.010050340076689724\n",
      "Epoch : 1569\n",
      "Val Loss : 0.010330981999635697\n",
      "Epoch : 1570\n",
      "Train Loss : 0.01003683432837239\n",
      "Epoch : 1570\n",
      "Val Loss : 0.01036327876150608\n",
      "Epoch : 1571\n",
      "Train Loss : 0.010028153717570883\n",
      "Epoch : 1571\n",
      "Val Loss : 0.01037703551352024\n",
      "Epoch : 1572\n",
      "Train Loss : 0.01002222097236862\n",
      "Epoch : 1572\n",
      "Val Loss : 0.010381428927183151\n",
      "Epoch : 1573\n",
      "Train Loss : 0.01001789219001328\n",
      "Epoch : 1573\n",
      "Val Loss : 0.010380831241607666\n",
      "Epoch : 1574\n",
      "Train Loss : 0.010014533413262553\n",
      "Epoch : 1574\n",
      "Val Loss : 0.010377451732754707\n",
      "Epoch : 1575\n",
      "Train Loss : 0.010011747800927357\n",
      "Epoch : 1575\n",
      "Val Loss : 0.010372143805027007\n",
      "Epoch : 1576\n",
      "Train Loss : 0.010009329951636732\n",
      "Epoch : 1576\n",
      "Val Loss : 0.010365499585866928\n",
      "Epoch : 1577\n",
      "Train Loss : 0.01000714297422947\n",
      "Epoch : 1577\n",
      "Val Loss : 0.010357967659831047\n",
      "Epoch : 1578\n",
      "Train Loss : 0.010005102993330498\n",
      "Epoch : 1578\n",
      "Val Loss : 0.01034967266023159\n",
      "Epoch : 1579\n",
      "Train Loss : 0.010003159162372988\n",
      "Epoch : 1579\n",
      "Val Loss : 0.010340677842497826\n",
      "Epoch : 1580\n",
      "Train Loss : 0.010001270176703608\n",
      "Epoch : 1580\n",
      "Val Loss : 0.01033118960261345\n",
      "Epoch : 1581\n",
      "Train Loss : 0.009999409102539786\n",
      "Epoch : 1581\n",
      "Val Loss : 0.010321195170283317\n",
      "Epoch : 1582\n",
      "Train Loss : 0.009997567896038506\n",
      "Epoch : 1582\n",
      "Val Loss : 0.010310777768492698\n",
      "Epoch : 1583\n",
      "Train Loss : 0.009995736538024878\n",
      "Epoch : 1583\n",
      "Val Loss : 0.010299986481666565\n",
      "Epoch : 1584\n",
      "Train Loss : 0.009993891132286223\n",
      "Epoch : 1584\n",
      "Val Loss : 0.010288763135671616\n",
      "Epoch : 1585\n",
      "Train Loss : 0.00999205320861416\n",
      "Epoch : 1585\n",
      "Val Loss : 0.010277338430285454\n",
      "Epoch : 1586\n",
      "Train Loss : 0.009990203338492914\n",
      "Epoch : 1586\n",
      "Val Loss : 0.010265558212995528\n",
      "Epoch : 1587\n",
      "Train Loss : 0.009988329926790716\n",
      "Epoch : 1587\n",
      "Val Loss : 0.01025340948998928\n",
      "Epoch : 1588\n",
      "Train Loss : 0.009986436821427528\n",
      "Epoch : 1588\n",
      "Val Loss : 0.010241057127714157\n",
      "Epoch : 1589\n",
      "Train Loss : 0.00998452580716209\n",
      "Epoch : 1589\n",
      "Val Loss : 0.010228370815515518\n",
      "Epoch : 1590\n",
      "Train Loss : 0.009982586791904583\n",
      "Epoch : 1590\n",
      "Val Loss : 0.010215443804860115\n",
      "Epoch : 1591\n",
      "Train Loss : 0.009980616370196127\n",
      "Epoch : 1591\n",
      "Val Loss : 0.01020217265188694\n",
      "Epoch : 1592\n",
      "Train Loss : 0.009978623590117247\n",
      "Epoch : 1592\n",
      "Val Loss : 0.010188647925853729\n",
      "Epoch : 1593\n",
      "Train Loss : 0.009976600923200943\n",
      "Epoch : 1593\n",
      "Val Loss : 0.01017486909031868\n",
      "Epoch : 1594\n",
      "Train Loss : 0.009974530743918915\n",
      "Epoch : 1594\n",
      "Val Loss : 0.010160739734768868\n",
      "Epoch : 1595\n",
      "Train Loss : 0.009972430283395952\n",
      "Epoch : 1595\n",
      "Val Loss : 0.010146321952342987\n",
      "Epoch : 1596\n",
      "Train Loss : 0.009970292626313361\n",
      "Epoch : 1596\n",
      "Val Loss : 0.010131622910499573\n",
      "Epoch : 1597\n",
      "Train Loss : 0.009968095034811632\n",
      "Epoch : 1597\n",
      "Val Loss : 0.01011654083430767\n",
      "Epoch : 1598\n",
      "Train Loss : 0.009965862493855992\n",
      "Epoch : 1598\n",
      "Val Loss : 0.010101211443543433\n",
      "Epoch : 1599\n",
      "Train Loss : 0.00996357262187581\n",
      "Epoch : 1599\n",
      "Val Loss : 0.010085429534316064\n",
      "Epoch : 1600\n",
      "Train Loss : 0.009961210164735326\n",
      "Epoch : 1600\n",
      "Val Loss : 0.010069195345044137\n",
      "Epoch : 1601\n",
      "Train Loss : 0.009958788933186026\n",
      "Epoch : 1601\n",
      "Val Loss : 0.010052615478634834\n",
      "Epoch : 1602\n",
      "Train Loss : 0.009956295048532961\n",
      "Epoch : 1602\n",
      "Val Loss : 0.01003551758825779\n",
      "Epoch : 1603\n",
      "Train Loss : 0.009953736030957347\n",
      "Epoch : 1603\n",
      "Val Loss : 0.010018044516444207\n",
      "Epoch : 1604\n",
      "Train Loss : 0.00995108279568604\n",
      "Epoch : 1604\n",
      "Val Loss : 0.009999974966049194\n",
      "Epoch : 1605\n",
      "Train Loss : 0.009948336554101248\n",
      "Epoch : 1605\n",
      "Val Loss : 0.009981398195028305\n",
      "Epoch : 1606\n",
      "Train Loss : 0.009945492336387312\n",
      "Epoch : 1606\n",
      "Val Loss : 0.009962123110890389\n",
      "Epoch : 1607\n",
      "Train Loss : 0.009942529232529558\n",
      "Epoch : 1607\n",
      "Val Loss : 0.00994225363433361\n",
      "Epoch : 1608\n",
      "Train Loss : 0.00993944426461577\n",
      "Epoch : 1608\n",
      "Val Loss : 0.009921632796525956\n",
      "Epoch : 1609\n",
      "Train Loss : 0.00993621738269607\n",
      "Epoch : 1609\n",
      "Val Loss : 0.009900218844413756\n",
      "Epoch : 1610\n",
      "Train Loss : 0.009932834298757577\n",
      "Epoch : 1610\n",
      "Val Loss : 0.009877962276339532\n",
      "Epoch : 1611\n",
      "Train Loss : 0.009929301338170876\n",
      "Epoch : 1611\n",
      "Val Loss : 0.00985479000210762\n",
      "Epoch : 1612\n",
      "Train Loss : 0.009925591492581303\n",
      "Epoch : 1612\n",
      "Val Loss : 0.009830695509910583\n",
      "Epoch : 1613\n",
      "Train Loss : 0.009921691700272607\n",
      "Epoch : 1613\n",
      "Val Loss : 0.009805589437484741\n",
      "Epoch : 1614\n",
      "Train Loss : 0.009917584110343053\n",
      "Epoch : 1614\n",
      "Val Loss : 0.009779401957988739\n",
      "Epoch : 1615\n",
      "Train Loss : 0.009913273859981222\n",
      "Epoch : 1615\n",
      "Val Loss : 0.009752114742994308\n",
      "Epoch : 1616\n",
      "Train Loss : 0.009908767224842968\n",
      "Epoch : 1616\n",
      "Val Loss : 0.009723859548568725\n",
      "Epoch : 1617\n",
      "Train Loss : 0.009904051400071468\n",
      "Epoch : 1617\n",
      "Val Loss : 0.009694533213973046\n",
      "Epoch : 1618\n",
      "Train Loss : 0.00989917453769528\n",
      "Epoch : 1618\n",
      "Val Loss : 0.009664399549365044\n",
      "Epoch : 1619\n",
      "Train Loss : 0.009894176918244341\n",
      "Epoch : 1619\n",
      "Val Loss : 0.009633659303188324\n",
      "Epoch : 1620\n",
      "Train Loss : 0.00988910954902976\n",
      "Epoch : 1620\n",
      "Val Loss : 0.009602558806538581\n",
      "Epoch : 1621\n",
      "Train Loss : 0.009884061653074209\n",
      "Epoch : 1621\n",
      "Val Loss : 0.009571542754769326\n",
      "Epoch : 1622\n",
      "Train Loss : 0.009879143517530685\n",
      "Epoch : 1622\n",
      "Val Loss : 0.009541045963764191\n",
      "Epoch : 1623\n",
      "Train Loss : 0.009874439128803826\n",
      "Epoch : 1623\n",
      "Val Loss : 0.009511498987674713\n",
      "Epoch : 1624\n",
      "Train Loss : 0.009870027500168285\n",
      "Epoch : 1624\n",
      "Val Loss : 0.009483365997672081\n",
      "Epoch : 1625\n",
      "Train Loss : 0.009865986668832892\n",
      "Epoch : 1625\n",
      "Val Loss : 0.009456984087824822\n",
      "Epoch : 1626\n",
      "Train Loss : 0.009862312137271904\n",
      "Epoch : 1626\n",
      "Val Loss : 0.009432613924145699\n",
      "Epoch : 1627\n",
      "Train Loss : 0.009859020652720088\n",
      "Epoch : 1627\n",
      "Val Loss : 0.009410396933555602\n",
      "Epoch : 1628\n",
      "Train Loss : 0.00985605879835917\n",
      "Epoch : 1628\n",
      "Val Loss : 0.009390339896082877\n",
      "Epoch : 1629\n",
      "Train Loss : 0.009853371753758383\n",
      "Epoch : 1629\n",
      "Val Loss : 0.009372340619564056\n",
      "Epoch : 1630\n",
      "Train Loss : 0.009850915607555482\n",
      "Epoch : 1630\n",
      "Val Loss : 0.00935613751411438\n",
      "Epoch : 1631\n",
      "Train Loss : 0.009848637431793685\n",
      "Epoch : 1631\n",
      "Val Loss : 0.009341592445969582\n",
      "Epoch : 1632\n",
      "Train Loss : 0.009846493948144951\n",
      "Epoch : 1632\n",
      "Val Loss : 0.009328458964824676\n",
      "Epoch : 1633\n",
      "Train Loss : 0.009844462143661607\n",
      "Epoch : 1633\n",
      "Val Loss : 0.009316529765725137\n",
      "Epoch : 1634\n",
      "Train Loss : 0.009842509118793698\n",
      "Epoch : 1634\n",
      "Val Loss : 0.009305600211024284\n",
      "Epoch : 1635\n",
      "Train Loss : 0.009840621362735263\n",
      "Epoch : 1635\n",
      "Val Loss : 0.009295509561896325\n",
      "Epoch : 1636\n",
      "Train Loss : 0.00983877623208477\n",
      "Epoch : 1636\n",
      "Val Loss : 0.009286113068461417\n",
      "Epoch : 1637\n",
      "Train Loss : 0.00983697545028613\n",
      "Epoch : 1637\n",
      "Val Loss : 0.009277303576469421\n",
      "Epoch : 1638\n",
      "Train Loss : 0.009835216234971727\n",
      "Epoch : 1638\n",
      "Val Loss : 0.009269021451473236\n",
      "Epoch : 1639\n",
      "Train Loss : 0.009833470465834137\n",
      "Epoch : 1639\n",
      "Val Loss : 0.009261163592338561\n",
      "Epoch : 1640\n",
      "Train Loss : 0.009831745187450346\n",
      "Epoch : 1640\n",
      "Val Loss : 0.009253656178712846\n",
      "Epoch : 1641\n",
      "Train Loss : 0.00983003317627035\n",
      "Epoch : 1641\n",
      "Val Loss : 0.009246507912874222\n",
      "Epoch : 1642\n",
      "Train Loss : 0.00982835568202613\n",
      "Epoch : 1642\n",
      "Val Loss : 0.00923958683013916\n",
      "Epoch : 1643\n",
      "Train Loss : 0.009826680257571762\n",
      "Epoch : 1643\n",
      "Val Loss : 0.009232943207025527\n",
      "Epoch : 1644\n",
      "Train Loss : 0.009825014040285051\n",
      "Epoch : 1644\n",
      "Val Loss : 0.009226536124944687\n",
      "Epoch : 1645\n",
      "Train Loss : 0.009823366247276607\n",
      "Epoch : 1645\n",
      "Val Loss : 0.009220312222838402\n",
      "Epoch : 1646\n",
      "Train Loss : 0.009821721655896657\n",
      "Epoch : 1646\n",
      "Val Loss : 0.009214283168315887\n",
      "Epoch : 1647\n",
      "Train Loss : 0.009820093077630654\n",
      "Epoch : 1647\n",
      "Val Loss : 0.009208441272377969\n",
      "Epoch : 1648\n",
      "Train Loss : 0.009818464842396275\n",
      "Epoch : 1648\n",
      "Val Loss : 0.009202761217951775\n",
      "Epoch : 1649\n",
      "Train Loss : 0.009816843482708804\n",
      "Epoch : 1649\n",
      "Val Loss : 0.009197195589542389\n",
      "Epoch : 1650\n",
      "Train Loss : 0.009815236223775402\n",
      "Epoch : 1650\n",
      "Val Loss : 0.009191795140504838\n",
      "Epoch : 1651\n",
      "Train Loss : 0.009813639620365409\n",
      "Epoch : 1651\n",
      "Val Loss : 0.009186529517173768\n",
      "Epoch : 1652\n",
      "Train Loss : 0.009812048026542858\n",
      "Epoch : 1652\n",
      "Val Loss : 0.0091813887655735\n",
      "Epoch : 1653\n",
      "Train Loss : 0.00981046532668518\n",
      "Epoch : 1653\n",
      "Val Loss : 0.009176398262381553\n",
      "Epoch : 1654\n",
      "Train Loss : 0.009808876430715213\n",
      "Epoch : 1654\n",
      "Val Loss : 0.009171464547514916\n",
      "Epoch : 1655\n",
      "Train Loss : 0.009807310605693436\n",
      "Epoch : 1655\n",
      "Val Loss : 0.009166676104068756\n",
      "Epoch : 1656\n",
      "Train Loss : 0.009805733283312188\n",
      "Epoch : 1656\n",
      "Val Loss : 0.009161970138549805\n",
      "Epoch : 1657\n",
      "Train Loss : 0.009804174236065022\n",
      "Epoch : 1657\n",
      "Val Loss : 0.00915738633275032\n",
      "Epoch : 1658\n",
      "Train Loss : 0.009802610240545243\n",
      "Epoch : 1658\n",
      "Val Loss : 0.009152905121445655\n",
      "Epoch : 1659\n",
      "Train Loss : 0.00980105025534687\n",
      "Epoch : 1659\n",
      "Val Loss : 0.009148491948843002\n",
      "Epoch : 1660\n",
      "Train Loss : 0.009799505464626589\n",
      "Epoch : 1660\n",
      "Val Loss : 0.00914419201016426\n",
      "Epoch : 1661\n",
      "Train Loss : 0.009797959189093092\n",
      "Epoch : 1661\n",
      "Val Loss : 0.009139977678656578\n",
      "Epoch : 1662\n",
      "Train Loss : 0.00979641243298389\n",
      "Epoch : 1662\n",
      "Val Loss : 0.009135822042822837\n",
      "Epoch : 1663\n",
      "Train Loss : 0.009794889428914653\n",
      "Epoch : 1663\n",
      "Val Loss : 0.009131788805127144\n",
      "Epoch : 1664\n",
      "Train Loss : 0.009793350325559275\n",
      "Epoch : 1664\n",
      "Val Loss : 0.009127811789512634\n",
      "Epoch : 1665\n",
      "Train Loss : 0.009791820333256415\n",
      "Epoch : 1665\n",
      "Val Loss : 0.009123913317918777\n",
      "Epoch : 1666\n",
      "Train Loss : 0.009790297547931984\n",
      "Epoch : 1666\n",
      "Val Loss : 0.00912009808421135\n",
      "Epoch : 1667\n",
      "Train Loss : 0.009788765416238656\n",
      "Epoch : 1667\n",
      "Val Loss : 0.009116337850689889\n",
      "Epoch : 1668\n",
      "Train Loss : 0.009787234974846085\n",
      "Epoch : 1668\n",
      "Val Loss : 0.009112635150551796\n",
      "Epoch : 1669\n",
      "Train Loss : 0.009785717436082635\n",
      "Epoch : 1669\n",
      "Val Loss : 0.00910902164876461\n",
      "Epoch : 1670\n",
      "Train Loss : 0.009784199597373658\n",
      "Epoch : 1670\n",
      "Val Loss : 0.009105450704693794\n",
      "Epoch : 1671\n",
      "Train Loss : 0.009782673381732982\n",
      "Epoch : 1671\n",
      "Val Loss : 0.009101969182491303\n",
      "Epoch : 1672\n",
      "Train Loss : 0.009781150931154387\n",
      "Epoch : 1672\n",
      "Val Loss : 0.009098532557487488\n",
      "Epoch : 1673\n",
      "Train Loss : 0.009779632088207901\n",
      "Epoch : 1673\n",
      "Val Loss : 0.009095146164298058\n",
      "Epoch : 1674\n",
      "Train Loss : 0.00977811062529524\n",
      "Epoch : 1674\n",
      "Val Loss : 0.009091831743717194\n",
      "Epoch : 1675\n",
      "Train Loss : 0.009776580271732022\n",
      "Epoch : 1675\n",
      "Val Loss : 0.00908853928744793\n",
      "Epoch : 1676\n",
      "Train Loss : 0.009775041874325785\n",
      "Epoch : 1676\n",
      "Val Loss : 0.009085303992033006\n",
      "Epoch : 1677\n",
      "Train Loss : 0.00977351234271274\n",
      "Epoch : 1677\n",
      "Val Loss : 0.009082121819257737\n",
      "Epoch : 1678\n",
      "Train Loss : 0.009771976558644046\n",
      "Epoch : 1678\n",
      "Val Loss : 0.009078982934355736\n",
      "Epoch : 1679\n",
      "Train Loss : 0.009770428609381368\n",
      "Epoch : 1679\n",
      "Val Loss : 0.00907588566839695\n",
      "Epoch : 1680\n",
      "Train Loss : 0.009768872424045385\n",
      "Epoch : 1680\n",
      "Val Loss : 0.00907281258702278\n",
      "Epoch : 1681\n",
      "Train Loss : 0.00976731471246725\n",
      "Epoch : 1681\n",
      "Val Loss : 0.009069788366556168\n",
      "Epoch : 1682\n",
      "Train Loss : 0.009765737690031528\n",
      "Epoch : 1682\n",
      "Val Loss : 0.009066789209842682\n",
      "Epoch : 1683\n",
      "Train Loss : 0.009764159575528945\n",
      "Epoch : 1683\n",
      "Val Loss : 0.009063845425844192\n",
      "Epoch : 1684\n",
      "Train Loss : 0.009762562940632958\n",
      "Epoch : 1684\n",
      "Val Loss : 0.009060913041234017\n",
      "Epoch : 1685\n",
      "Train Loss : 0.009760955965073507\n",
      "Epoch : 1685\n",
      "Val Loss : 0.009058011069893837\n",
      "Epoch : 1686\n",
      "Train Loss : 0.009759334673329498\n",
      "Epoch : 1686\n",
      "Val Loss : 0.00905513896048069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\workspaces\\kaggle\\TPS\\2022-11\\run.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m run(net, criterion, optimizer, \u001b[39m2000\u001b[39;49m, train_dataloader, val_dataloader)\n",
      "\u001b[1;32mc:\\Users\\USER\\workspaces\\kaggle\\TPS\\2022-11\\run.ipynb 셀 22\u001b[0m in \u001b[0;36mrun\u001b[1;34m(model, criterion, optimizer, epochs, train_loader, val_loader, log)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m save_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs) :\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, criterion, optimizer, train_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m log :\n",
      "\u001b[1;32mc:\\Users\\USER\\workspaces\\kaggle\\TPS\\2022-11\\run.ipynb 셀 22\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, dataloader)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader) :\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/workspaces/kaggle/TPS/2022-11/run.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39msqueeze(), targets)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:206\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_zero_grad_profile_name\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_for_profile()\n\u001b[1;32m--> 206\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_zero_grad_profile_name):\n\u001b[0;32m    207\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    208\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:432\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[39m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run(net, criterion, optimizer, 2000, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5300650774925307"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad() :\n",
    "    outputs = net(torch.from_numpy(origin_df.loc[:19999].drop('label', axis=1).to_numpy()).float()).squeeze()\n",
    "\n",
    "log_loss(labels_df, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = origin_df[20000:].drop('label', axis=1)\n",
    "\n",
    "input_dataset = TensorDataset(to_tensor(input_df))\n",
    "input_dataloader = DataLoader(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('Linear_{}.pt'.format(input_size)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, input_loader) :\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad() :\n",
    "        for (inputs, ) in tqdm(input_loader) :\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            preds.append(outputs.numpy())\n",
    "\n",
    "    pred_df = pd.DataFrame({'pred' : preds})\n",
    "    pred_df.index = sub_ids\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:07<00:00, 2649.26it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = prediction(net, input_dataloader)\n",
    "pred_df.to_csv('submission_best{}.csv'.format(input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('automl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d20d86510def51f33a44b5a435d55c10102dd0bfb266b4794da87c9e55979b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
